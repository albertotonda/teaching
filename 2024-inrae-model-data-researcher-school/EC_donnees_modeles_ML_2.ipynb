{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training school \"Data and Models\" INRAE 2024\n",
        "\n",
        "## Exercise 2: Introduction to ML - Classification\n",
        "We are now going to see a few short examples of classification, to give you a global idea of the tasks. Differently from regression tasks, where the objective is to predict a continuous value for the target feature, in classification the objective is to associate a sample to a class label. A class label is a discrete value with an associated meaning (e.g. Good/Medium/Bad, Healthy/Unhealthy, Cat/Dog/Horse)."
      ],
      "metadata": {
        "id": "2ykCXb0ZinNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# don't worry about this code here, totally normal stuff and absolutely not a horrible hack\n",
        "# to make things work in Google colaboratory, just run it and don't think about it\n",
        "!pip uninstall scikit-learn --yes\n",
        "!pip uninstall imblearn --yes\n",
        "!pip install scikit-learn==1.2.2\n",
        "!pip install imblearn"
      ],
      "metadata": {
        "id": "HVRylI9dLppq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-AjYuEWikWd"
      },
      "outputs": [],
      "source": [
        "# we need (again) to install the openml library to download the data set we are going to work on\n",
        "!pip install openml\n",
        "\n",
        "# load and download the dataset\n",
        "import openml\n",
        "\n",
        "dataset = openml.datasets.get_dataset(188)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at the data.\n",
        "\n",
        "The dataset 188, \"eucalyptus\", describes the expected utility of a seed lot of eucalyptus, taking into account information such as location, altitude, initial height of the seedlings, etc. The class here is a set of discrete values, ranging from a terrible outcome (all seedlings died) to a great outcome, as evaluated by human experts.\n",
        "\n",
        "We can even check how many samples are available for each class, something that will be useful for later."
      ],
      "metadata": {
        "id": "A2Vrf0npj5bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the data into a dataframe object, which makes it easier to manipulate datasets\n",
        "df, *_ = dataset.get_data()\n",
        "#print(df)\n",
        "\n",
        "# print the different values of the target variable: as you will see, there are\n",
        "# just five values, qualitative assessment of the final quality of the eucalyptus\n",
        "# 'none' means that all seedlings died\n",
        "target_feature = dataset.default_target_attribute\n",
        "other_features = [c for c in df.columns if c != target_feature]\n",
        "class_labels = df[target_feature].sort_values(ascending=False).unique() # all this stuff is just to get them in order from best to worst\n",
        "print(\"Unique values of the class:\", class_labels) # print the unique values\n",
        "\n",
        "# before starting with the learning process, we need to tackle the categorical variables\n",
        "# and convert them to numerical values\n",
        "categorical_columns = df[other_features].select_dtypes(include=['category', 'object', 'string'])\n",
        "print(\"Categorical columns found:\", categorical_columns.columns)\n",
        "for c in categorical_columns :\n",
        "  df[c].replace({category : index for index, category in enumerate(df[c].astype('category').cat.categories)}, inplace=True)\n",
        "\n",
        "# also remove all rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# this block of code just prints how many samples are available for each class label;\n",
        "# in general, we would like to have a balanced data set, with an equal number of\n",
        "# samples for each class label\n",
        "for class_label in class_labels :\n",
        "  df_class_label = df[df[target_feature] == class_label]\n",
        "  print(\"For class label \\\"%s\\\", we have %d samples (%.2f%% of the total)\" %\n",
        "        (class_label, df_class_label.shape[0], df_class_label.shape[0] * 100/df.shape[0]))\n",
        "\n",
        "# get just the data without the column headers, as numerical matrices; for the target feature,\n",
        "# we have to replace the strings ('best', 'good', ...) with integers (0, 1, 2, ...); finally our data is ready\n",
        "X = df[other_features].values\n",
        "\n",
        "import numpy as np\n",
        "dictionary = {class_label : i for i, class_label in enumerate(class_labels)} # creates a dictionary to go from label name to an integer\n",
        "y = np.array([dictionary[y_i] for y_i in df[target_feature].values])"
      ],
      "metadata": {
        "id": "Xpo9bigNkEJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is interesting to notice that the data set is not perfectly balanced. One of the classes has over 30% of all samples, while the class labels that is least represented has around 15% of the samples.\n",
        "\n",
        "Just as was the case for regression, we are going to evaluate the performance of our classification algorithm resorting to a cross-validation. This time, we are going to use different metrics, to compare the information that we are able to obtain from each.\n",
        "\n",
        "Also, for classification problems we would like to have each of the _k_ splits in the _k-fold cross-validation_ to 'look like' the original data: we would not like to have a split that contains only samples from one class, for example. To attain this objective, we are going to use a cross-validation variant called _stratified cross-validation_, that will try to keep the proportion of samples of each class in each fold as close as possible to the original proportion in the whole dataset. So, for example, if the original dataset has 30% samples of class A, 40% samples of class B, and 30% samples of class C, the stratified cross-validation will attempt to keep the 30-40-30 proportion in each fold.\n",
        "\n",
        "The stratified cross-validation is already implemented in the _cross_validate_ and _cross_val_predict_ functions we used in the previous exercise, and it is automatically used if the values of the target _y_ are class labels."
      ],
      "metadata": {
        "id": "f0SMzhBckE0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a classifier, we are going to pick Random Forest again, because it's\n",
        "# fast to train and it is able to natively manage integer variables without further preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "# since we are going to use several metrics, it's faster to just obtain the predictions\n",
        "# of the test set for each fold, and then compute the metrics afterwards\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "y_pred = cross_val_predict(classifier, X, y, cv=5)\n",
        "\n",
        "# let's see if the final result was good, using different metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
        "print(\"Accuracy: %.4f\" % accuracy_score(y, y_pred))\n",
        "print(\"F1 score: %.4f\" % f1_score(y, y_pred, average='weighted'))\n",
        "print(\"Matthew's Correlation Coefficient: %.4f\" % matthews_corrcoef(y, y_pred))"
      ],
      "metadata": {
        "id": "X7REvv_K1bjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful visualization for the results of classification problems is the _confusion matrix_, a plot showing the percentage of samples that were correctly and uncorrectly classified, and more specifically _where_ are the mistakes (for example: when the algorithm misclassifies a sample belonging to class A, does it have more the tendency to attribute it to class B or C?)."
      ],
      "metadata": {
        "id": "xEJSr3FB1cl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a figure, and display the confusion matrix\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "cmd = ConfusionMatrixDisplay.from_predictions(\n",
        "    y_true=y,\n",
        "    y_pred=y_pred,\n",
        "    display_labels=class_labels,\n",
        "    ax=ax,\n",
        ")"
      ],
      "metadata": {
        "id": "8bdFlxZH2C7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the classification performance is not great. However, from the confusion matrix, we can easily observe that most of the mistakes happen between adjacent classes, especially among the best two ('best' and 'good') and worst two ('low' and 'none'). We can use the option 'normalize' to have a more precise idea of the ratio of samples from a class that end up with the correct label or another."
      ],
      "metadata": {
        "id": "IS2PCVAZ7NX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "cmd = ConfusionMatrixDisplay.from_predictions(\n",
        "    y_true=y,\n",
        "    y_pred=y_pred,\n",
        "    display_labels=class_labels,\n",
        "    normalize='true',\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "ax.set_title(\"Confusion matrix for Random Forest\")"
      ],
      "metadata": {
        "id": "BKCvDjPB_rWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now check if we can do better! Random Forest is nice, but we will try a few state-of-the-art classifiers: \"eXtreme Gradient Boosting\", or XGBoost; and \"Category Boosting\", or CatBoost. The two are commonly considered to be the best by industry and practitioners."
      ],
      "metadata": {
        "id": "NSF-Pj2CoZSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "Cf9VS4r0oMZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "classifier_xgboost = XGBClassifier(verbosity=0)\n",
        "classifier_catboost = CatBoostClassifier(verbose=0)\n",
        "\n",
        "print(\"Running the cross-validation (this might take some) time...\")\n",
        "y_pred_xgboost = cross_val_predict(classifier_xgboost, X, y, cv=5)\n",
        "y_pred_catboost = cross_val_predict(classifier_catboost, X, y, cv=5)\n",
        "\n",
        "print(\"F1 score for XGBoost: %.4f\" % f1_score(y, y_pred_xgboost, average='weighted'))\n",
        "print(\"F1 score for CatBoost: %.4f\" % f1_score(y, y_pred_catboost, average='weighted'))"
      ],
      "metadata": {
        "id": "WImF56ehrZ0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the confusion matrix of the most performing of the two new algorithms."
      ],
      "metadata": {
        "id": "MXTgJ1LfsiI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "cmd = ConfusionMatrixDisplay.from_predictions(\n",
        "    y_true=y,\n",
        "    y_pred=y_pred_xgboost,\n",
        "    display_labels=class_labels,\n",
        "    normalize='true',\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "ax.set_title(\"Confusion matrix for XGBoost\")"
      ],
      "metadata": {
        "id": "RxC6CjFlsgIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are better, but not by a lot. This is a common trend, problems that are difficult for one ML algorithm are often difficult for most algorithms designed to deal with the same type of problems (classification, regression, ...). Let's now try to see what happens with a simpler classification problem.\n",
        "\n",
        "Dataset 24, \"mushroom\", is a classification of mushrooms between poisonous and edible. As you will see, it's pretty balanced, with around 4,000 samples for each class."
      ],
      "metadata": {
        "id": "jzv1rb9xuCFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load another data set\n",
        "dataset = openml.datasets.get_dataset(24)\n",
        "\n",
        "# obtain the data set as a dataframe\n",
        "df, *_ = dataset.get_data()\n",
        "\n",
        "# preprocessing to obtain just matrices of values\n",
        "target_feature = dataset.default_target_attribute\n",
        "other_features = [c for c in df.columns if c != target_feature]\n",
        "\n",
        "# convert categorical features to numerical values\n",
        "categorical_columns = df[other_features].select_dtypes(include=['category', 'object', 'string'])\n",
        "print(\"Categorical columns found:\", categorical_columns.columns)\n",
        "for c in categorical_columns :\n",
        "  df[c].replace({category : index for index, category in enumerate(df[c].astype('category').cat.categories)}, inplace=True)\n",
        "\n",
        "class_labels = df[target_feature].unique()\n",
        "dictionary = {class_label : i for i, class_label in enumerate(class_labels)} # creates a dictionary to go from label name to an integer\n",
        "for class_label in class_labels :\n",
        "  print(\"For class label \\\"%s\\\", there are %d samples\" % (class_label, df[df[target_feature] == class_label].shape[0]))\n",
        "\n",
        "# finally, obtain matrices of numerical values\n",
        "X = df[other_features].values\n",
        "y = np.array([dictionary[y_i] for y_i in df[target_feature].values])\n",
        "\n",
        "print(\"Training classifiers, this might take some time...\")\n",
        "classifier_rf = RandomForestClassifier()\n",
        "y_pred_rf = cross_val_predict(classifier_rf, X, y, cv=5)\n",
        "\n",
        "classifier_xgboost = XGBClassifier(verbosity=0)\n",
        "y_pred_xgboost = cross_val_predict(classifier_xgboost, X, y, cv=5)\n",
        "\n",
        "print(\"\\nAccuracy for Random Forest: %.4f\" % accuracy_score(y, y_pred_rf))\n",
        "print(\"F1 score for Random Forest: %.4f\" % f1_score(y, y_pred_rf))\n",
        "print(\"\\nAccuracy for XGBoost: %.4f\" % accuracy_score(y, y_pred_xgboost))\n",
        "print(\"F1 score for XGBoost: %.4f\" % f1_score(y, y_pred_xgboost))"
      ],
      "metadata": {
        "id": "TgaDTXBLut-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, there is little difference between the two algorithms, and the values of accuracy and F1 are extremely close, as the two class labels have almost equal numerosity. Let's take a look at the confusion matrix for Random Forest, this time there are only two classes."
      ],
      "metadata": {
        "id": "KuNYSi6T14wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "cmd = ConfusionMatrixDisplay.from_predictions(\n",
        "    y_true=y,\n",
        "    y_pred=y_pred_rf,\n",
        "    display_labels=class_labels,\n",
        "    #normalize='true',\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "ax.set_title(\"Confusion matrix for Random Forest\")"
      ],
      "metadata": {
        "id": "6B_xSqmF14NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This problem was clearly simpler, and all algorithms performed well. Now, let's try with a more unbalanced dataset. We need a specific library that was designed to explore and deal with heavily imbalanced problems."
      ],
      "metadata": {
        "id": "tq9RhtGRInI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the imbalanced dataset\n",
        "from imblearn.datasets import fetch_datasets\n",
        "ecoli = fetch_datasets()['ecoli']\n",
        "\n",
        "# how imbalanced is this, exactly? let's count\n",
        "import numpy as np\n",
        "class_labels, counts = np.unique(ecoli[\"target\"], return_counts=True)\n",
        "for i in range(0, class_labels.shape[0]) :\n",
        "  print(\"For class label \\\"%d\\\", found %d samples\" % (class_labels[i], counts[i]))\n",
        "\n",
        "# preprocessing to get our nice matrices of numbers\n",
        "dictionary = {-1 : 0, 1 : 1}\n",
        "\n",
        "X = ecoli[\"data\"]\n",
        "y = np.array([dictionary[l] for l in ecoli[\"target\"]])\n",
        "\n",
        "# a first classification in cross-validation using Random Forest\n",
        "classifier_rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "y_pred_rf = cross_val_predict(classifier_rf, X, y, cv=5)\n",
        "\n",
        "print(\"Accuracy for Random Forest: %.4f\" % accuracy_score(y, y_pred_rf))\n",
        "print(\"F1 score for Random Forest: %.4f\" % f1_score(y, y_pred_rf))"
      ],
      "metadata": {
        "id": "LaxXhLl9Pbo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we can observe a huge difference between accuracy and F1! Before we take a look at the confusion matrix, try to guess: what do you expect to see here?"
      ],
      "metadata": {
        "id": "OiZ0KqOgQj7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "cmd = ConfusionMatrixDisplay.from_predictions(\n",
        "    y_true=y,\n",
        "    y_pred=y_pred_rf,\n",
        "    display_labels=class_labels,\n",
        "    #normalize='true',\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "ax.set_title(\"Confusion matrix for Random Forest\")"
      ],
      "metadata": {
        "id": "Dtyn7HjBQ6lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm has the tendency of putting most samples into class \"-1\", which is the more numerous (301 samples vs 35). Let's try to improve Random Forest's behavior, by using an option that takes into account the class imbalance during training, assigning a different weight to samples from each class. The weights are used to change the function that Random Forest internally optimizes, so that misclassifying a sample with a higher weight is considered much worse than misclassifying a sample with a lower weight. The weights are automatically computed to be inversely proportional to class frequencies, with less frequent classes given more importance (trying to compensate for the fact that they have less samples).\n",
        "\n",
        "In the plot for the confusion matrix below, you can comment/uncomment the line\n",
        "```\n",
        "#normalize='true',\n",
        "```\n",
        "to display the values in the matrix as ratios rather than number of samples."
      ],
      "metadata": {
        "id": "b7WhkDpNS7Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_rf_balanced = RandomForestClassifier(class_weight=\"balanced\")\n",
        "y_pred_rf_balanced = cross_val_predict(classifier_rf_balanced, X, y, cv=5)\n",
        "\n",
        "print(\"Accuracy for Random Forest: %.4f\" % accuracy_score(y, y_pred_rf_balanced))\n",
        "print(\"F1 score for Random Forest: %.4f\" % f1_score(y, y_pred_rf_balanced))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "cmd = ConfusionMatrixDisplay.from_predictions(\n",
        "    y_true=y,\n",
        "    y_pred=y_pred_rf_balanced,\n",
        "    display_labels=class_labels,\n",
        "    #normalize='true',\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "ax.set_title(\"Confusion matrix for Random Forest\")"
      ],
      "metadata": {
        "id": "BO25cbdVS1bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are only marginally better! What about XGBoost? In the case of XGBoost, the algorithm has a similar option to take into account class imbalance, called 'scale_pos_weight'. The details of how this is computed are not very important, it's a different way of giving more importance to samples from the less frequent class label."
      ],
      "metadata": {
        "id": "po_HhZNbUEm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_xgboost_balanced = XGBClassifier(scale_pos_weight=counts[0]/counts[1], verbosity=0)\n",
        "y_pred_xgboost_balanced = cross_val_predict(classifier_xgboost_balanced, X, y, cv=5)\n",
        "\n",
        "print(\"Accuracy for XGBoost: %.4f\" % accuracy_score(y, y_pred_xgboost_balanced))\n",
        "print(\"F1 score for XGBoost: %.4f\" % f1_score(y, y_pred_xgboost_balanced))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "cmd = ConfusionMatrixDisplay.from_predictions(\n",
        "    y_true=y,\n",
        "    y_pred=y_pred_xgboost_balanced,\n",
        "    display_labels=class_labels,\n",
        "    #normalize='true',\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "ax.set_title(\"Confusion matrix for XGBoost\")"
      ],
      "metadata": {
        "id": "g9LDOcdwUWMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bit better, but it's still not great: if we have a sample of class '1', we only have 60% probability that it will be correctly classified. This imbalanced data set is a difficult classification problem, in practical cases it would be best to try to have balanced class labels."
      ],
      "metadata": {
        "id": "21hjY6-SqDvW"
      }
    }
  ]
}