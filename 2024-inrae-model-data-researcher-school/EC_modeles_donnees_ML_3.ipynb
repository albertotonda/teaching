{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training school \"Data and Models\" INRAE 2024\n",
        "\n",
        "## Exercise 3: Introduction to ML - Explanations/Interpretation\n",
        "In this part, we are going to check a few options for obtaining \"explanations\" of the behavior of ML models, that are normally \"black boxes\". Let's start (again) by installing the openml library and downloading a data set."
      ],
      "metadata": {
        "id": "u6XR5x7FjtQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrDKOihei1k1"
      },
      "outputs": [],
      "source": [
        "!pip install openml\n",
        "import openml\n",
        "\n",
        "# load another data set\n",
        "dataset = openml.datasets.get_dataset(24)\n",
        "\n",
        "# obtain the data set as a dataframe\n",
        "df, *_ = dataset.get_data()\n",
        "\n",
        "# preprocessing to obtain just matrices of values\n",
        "target_feature = dataset.default_target_attribute\n",
        "other_features = [c for c in df.columns if c != target_feature]\n",
        "print(\"The dataset has %d samples and %d features\" % (df.shape[0], len(other_features)))\n",
        "\n",
        "# convert categorical features to numerical values\n",
        "categorical_columns = df[other_features].select_dtypes(include=['category', 'object', 'string'])\n",
        "print(\"Categorical columns found:\", categorical_columns.columns)\n",
        "for c in categorical_columns :\n",
        "  df[c].replace({category : index for index, category in enumerate(df[c].astype('category').cat.categories)}, inplace=True)\n",
        "\n",
        "class_labels = df[target_feature].unique()\n",
        "dictionary = {class_label : i for i, class_label in enumerate(class_labels)} # creates a dictionary to go from label name to an integer\n",
        "for class_label in class_labels :\n",
        "  print(\"For class label \\\"%s\\\", there are %d samples\" % (class_label, df[df[target_feature] == class_label].shape[0]))\n",
        "\n",
        "# finally, obtain matrices of numerical\n",
        "import numpy as np\n",
        "X = df[other_features].values\n",
        "y = np.array([dictionary[y_i] for y_i in df[target_feature].values])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, this time we are mostly interested in observing the explanations, so we will skip the k-fold cross-validation and just use a single train/test split of the data. We are going to train the model on the training set, and test it on the test set. Let's use a few different ML algorithms."
      ],
      "metadata": {
        "id": "q9nNfSzQkXmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and test; since this is a classification problem, we\n",
        "# are going to use a stratified split, so that each split preserve the same proportion\n",
        "# of class labels\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True)\n",
        "\n",
        "# set up a few different classifiers\n",
        "# our good old friend, Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# logistic regression is using a linear model for the decision boundaries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# support vector machines is another famous ML technique, creating a higher-dimensional\n",
        "# space by combining the data set features, and then fitting a linear model in\n",
        "# the higher-dimensional space\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "\n",
        "# create a dictionary of classifiers, so that we can iterate over them\n",
        "classifiers = {\"Logistic Regression\" : lr, \"Random Forest\" : rf, \"Support Vector Machines\" : svc}"
      ],
      "metadata": {
        "id": "7oMQHeo1k0yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, logistic regression and support vector machines require data normalization, or they crash horribly\n",
        "# in order to properly perform the needed pre-processing, we need to add a normalization step,\n",
        "# using another algorithm from scikit-learn, that rescales each feature to 0 mean and unit variance\n",
        "# so that its values will be in [-1, 1]\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# normalization is LEARNED ON THE TRAINING SET and APPLIED TO THE TEST SET\n",
        "# this is important, applying normalization to the whole data set can lead to\n",
        "# the ML algorithm having access to information it should not have (for example,\n",
        "# if the highest value of a feature falls into the test set, there will be no\n",
        "# values 1.0 in the training set for that feature)\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# iterate over the classifiers, train them and test them\n",
        "from sklearn.metrics import f1_score\n",
        "for classifier_name, classifier in classifiers.items() :\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  f1_value = f1_score(y_test, y_pred)\n",
        "\n",
        "  print(\"For classifier \\\"%s\\\", F1=%.4f\" % (classifier_name, f1_value))"
      ],
      "metadata": {
        "id": "G7Xmu-_ZmAAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global explanations\n",
        "Now that we have three trained classifier models, we can check what is the relative importance of the features they used to get to the decision; since the data set has 22 features, which is quite a lot to visualize, we are only going to look at the top 10 (most important), visualized as a histogram."
      ],
      "metadata": {
        "id": "sMzEUV2qod6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's first take a look at Random Forest; we create a list with the name of the feature, and a value describing its relative importance\n",
        "feature_importance_rf = [ [other_features[i], rf.feature_importances_[i]] for i in range(0, len(other_features))]\n",
        "# sort the list from most important to least important, cut it at the first 10\n",
        "feature_importance_rf = sorted(feature_importance_rf, reverse=True, key=lambda x : x[1])[:10]\n",
        "\n",
        "# create the histogram\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "fig, ax = plt.subplots()\n",
        "x = [x for x in range(0, len(feature_importance_rf))]\n",
        "ax.bar(x, [y[1] for y in feature_importance_rf])\n",
        "ax.set_xticks(x, [y[0] for y in feature_importance_rf], rotation=90)\n",
        "ax.set_title(\"Relative feature importance according to Random Forest\")"
      ],
      "metadata": {
        "id": "3OnBQpTYodLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, for Random Forest, what is most important in discriminating a mushroom between poisonous or edible is its smell. Let's see if Logistic Regression agrees."
      ],
      "metadata": {
        "id": "pAPmaadlsV4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression just uses the absolute values of the coefficients as relative importance\n",
        "feature_importance_lr = [ [other_features[i], abs(lr.coef_[0,i])] for i in range(0, len(other_features))]\n",
        "# sort the list from most important to least important, cut it at the first 10\n",
        "feature_importance_lr = sorted(feature_importance_lr, reverse=True, key=lambda x : x[1])[:10]\n",
        "\n",
        "# create the histogram\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "fig, ax = plt.subplots()\n",
        "x = [x for x in range(0, len(feature_importance_lr))]\n",
        "ax.bar(x, [y[1] for y in feature_importance_lr])\n",
        "ax.set_xticks(x, [y[0] for y in feature_importance_lr], rotation=90)\n",
        "ax.set_title(\"Relative feature importance according to Logistic Regression\")"
      ],
      "metadata": {
        "id": "CxLgABtLsgDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They actually seem to disagree! Notice that the values on the y-axis make sense only for *the same algorithm*, and cannot be easily compared between different algorithms: what matters the most is the relative ranking of the features.\n",
        "\n",
        "Well, let's see what Support Vector Machines thinks of the feature importance. SVM does not have a native way of returning the feature importance, because it is using \"artificial features\", created as combinations of the original features.\n",
        "\n",
        "However, there is a way out: one of the utils in sklearn evaluates the relative importance of the features for _no matter what classifier_, by simply removing the features one by one, and checking when the performance of the classifier drops, and by how much. It's slower, because it requires several iterations, but it can be applied to any kind of trained model."
      ],
      "metadata": {
        "id": "DuBC0NlPtMXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "result = permutation_importance(svc, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)"
      ],
      "metadata": {
        "id": "fGkDA5X1tL3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_svc = [ [other_features[i], result[\"importances_mean\"][i]] for i in range(0, len(other_features))]\n",
        "# sort the list from most important to least important, cut it at the first 10\n",
        "feature_importance_svc = sorted(feature_importance_svc, reverse=True, key=lambda x : x[1])[:10]\n",
        "\n",
        "# create the histogram\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "fig, ax = plt.subplots()\n",
        "x = [x for x in range(0, len(feature_importance_svc))]\n",
        "ax.bar(x, [y[1] for y in feature_importance_svc])\n",
        "ax.set_xticks(x, [y[0] for y in feature_importance_svc], rotation=90)\n",
        "ax.set_title(\"Relative feature importance according to Support Vector Machines\")"
      ],
      "metadata": {
        "id": "4oNR-GOtuSCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, a disagreement, but we might notice that some of the features appear often in the top 10 of each algorithm.\n",
        "\n",
        "What happens if we believe the classifiers, and retrain them just using the most common features that appear in the top 10? We expect having a small-to-negligable drop in performance. Let's see if it is true!"
      ],
      "metadata": {
        "id": "7QRCip9bulfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_important_features = [f for f, v in feature_importance_svc if f in [y[0] for y in feature_importance_rf] and f in [y[0] for y in feature_importance_lr]]\n",
        "print(\"Features in the top 10 of all classifiers:\", most_important_features)\n",
        "\n",
        "# get the matrix with the values for just those features\n",
        "X_reduced = df[most_important_features].values\n",
        "\n",
        "# let's try the classification again\n",
        "X_train_reduced, X_test_reduced, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42, stratify=y, shuffle=True)\n",
        "scaler.fit(X_train_reduced)\n",
        "X_train_reduced = scaler.transform(X_train_reduced)\n",
        "X_test_reduced = scaler.transform(X_test_reduced)\n",
        "\n",
        "for classifier_name, classifier in classifiers.items() :\n",
        "  classifier.fit(X_train_reduced, y_train)\n",
        "  y_pred = classifier.predict(X_test_reduced)\n",
        "  f1_value = f1_score(y_test, y_pred)\n",
        "\n",
        "  print(\"For classifier \\\"%s\\\", F1=%.4f\" % (classifier_name, f1_value))"
      ],
      "metadata": {
        "id": "Lmj1bdwgu2Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the results of this notebook are repeatable, we went from 22 to just 4 features, with only a small drop in performance! 4 features make a model way easier to understand for a human. The process we just went through is sometimes called **feature selection**, and can be performed in a number of different ways, for either (i) ease human interpretation or (ii) improve model performance, by removing useless or deceptive features."
      ],
      "metadata": {
        "id": "kpJ9NazhwXnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local explanations\n",
        "But what if we were interested in knowing the relative importance of the features for the classification of ONE sample in particular? Well, we could build a linear piece-wise approximation of the decision boundary around a sample, and look at the weights for that linear function. *Easy*.\n",
        "\n",
        "Luckily, someone already did it for us, in the LIME library."
      ],
      "metadata": {
        "id": "Gz0w_d2LwXmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "vXX7TQXwzb_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an 'explainer' object\n",
        "from lime import lime_tabular\n",
        "explainer = lime_tabular.LimeTabularExplainer(X_train, mode='classification', training_labels=y_train, feature_names=other_features)\n",
        "\n",
        "# re-train the algorithms on the original (non-reduced) data\n",
        "lr = LogisticRegression()\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LJSt9_8fzpAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain an explanation for a test sample from logistic regression; we use a function\n",
        "# of logistic regression called \"predict_proba\" that shows pseudo-probabilities of a\n",
        "# sample to belong to each class; don't worry too much about it\n",
        "test_sample = X_test[100]\n",
        "exp = explainer.explain_instance(test_sample, lr.predict_proba, num_features=5)\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "myvLHPLp4oh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting! Does Random Forest agree with Linear Regression on where this sample should be classified, and why? Let's check."
      ],
      "metadata": {
        "id": "Th0nrrwF2yYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp = explainer.explain_instance(test_sample, rf.predict_proba, num_features=5)\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "5AuOCfPJ26fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, go back to the cell with the line\n",
        "```\n",
        "test_sample = X_test[0]\n",
        "```\n",
        "change it to evaluate another test sample, for example:\n",
        "```\n",
        "test_sample = X_test[100]\n",
        "```\n",
        "and re-run the last two cells. You can compare the evaluations of Random Forest and Support Vector Machines on several samples, observing how they agree or disagree on classification, and why."
      ],
      "metadata": {
        "id": "q8A44gTj3GnV"
      }
    }
  ]
}