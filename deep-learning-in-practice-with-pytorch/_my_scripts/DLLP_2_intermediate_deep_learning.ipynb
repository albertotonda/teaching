{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eFWpa3nS9zq"
   },
   "source": [
    "# Intermediate Deep Learning concepts\n",
    "\n",
    "In this notebook, we are going to see some slightly more advanced concepts of Deep Learning, and how they are practically implemented in pytorch.\n",
    "\n",
    "Let's start again by importing all libraries we will need in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "48wMZK0zXjCJ"
   },
   "outputs": [],
   "source": [
    "!pip install openml --quiet\n",
    "\n",
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pEwtBrewS886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.5.1\n",
      "Is GPU computation available? True\n",
      "(Mac M1 only) Is GPU computation available? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"pytorch version:\",torch.__version__)\n",
    "print(\"Is GPU computation available?\", torch.cuda.is_available())\n",
    "\n",
    "#Â this is only relevant for people with a MacBook M1 and pytorch >= 2.0\n",
    "print(\"(Mac M1 only) Is GPU computation available?\", torch.backends.mps.is_available())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ovnn5qwdUNpA"
   },
   "source": [
    "## Monitoring the training process with training loss and validation loss\n",
    "\n",
    "As we have seen in the previous example, it can be difficult to exactly assess when to stop the training process for a DL model; or, in other words, how to set the proper number of iterations in the gradient descent algorithm. In practice, we would like to know exactly when our model starts overfitting the training data, and stop right before overfitting becomes a problem. However, it is impossible to assess overfitting just checking the performance on the training data: two different models might show the same performance on training data; but one could be able to generalize perfectly to unseen data, and the other could be completely overfitted and performing poorly on unseen data.\n",
    "\n",
    "One way of tackling this problem is to further divide our training data into two parts: one training set proper and one *validation set*, that will not be used for training, but just to check performance during the training process, and identify possible signs of overfitting.\n",
    "\n",
    "Let's load and preprocess the data, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HCNuih3EVssJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        theta1    theta2    theta3    theta4    theta5    theta6    theta7  \\\n",
      "0    -0.015119  0.360741  0.469398  1.309675  0.988024 -0.025493  0.664071   \n",
      "1     0.360478 -0.301395  0.629183 -1.440146 -0.741637 -1.196749 -1.038444   \n",
      "2     1.563238 -1.294753  0.078987  1.432937  1.149136 -1.292140  1.562988   \n",
      "3     0.199485  0.901157 -1.356304 -0.080525 -0.976628  0.829894 -0.855649   \n",
      "4     0.659737  0.120552 -0.008756  0.648839  0.626832 -0.646539  1.318074   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "8187  1.459523 -0.201551 -0.610721  1.424181  0.269273  1.172781 -0.509818   \n",
      "8188  0.654980 -0.823516 -0.809246  0.408441 -1.368838  0.582222 -1.394697   \n",
      "8189  0.103862  1.024329 -1.047221  0.280905 -0.181155 -0.651934 -1.241842   \n",
      "8190 -1.109311 -0.027690 -1.448736 -0.648566 -0.462142  0.814971  1.100615   \n",
      "8191  1.155010 -0.429331 -1.567260  0.883077 -1.203777 -0.126506  0.331042   \n",
      "\n",
      "        theta8         y  \n",
      "0     0.062763  0.536524  \n",
      "1    -0.717461  0.308014  \n",
      "2    -0.937731  0.518900  \n",
      "3     0.930630  0.494151  \n",
      "4    -0.899172  0.470218  \n",
      "...        ...       ...  \n",
      "8187 -0.686006  0.486093  \n",
      "8188 -0.057294  0.696752  \n",
      "8189  1.530220  0.803888  \n",
      "8190  1.337159  0.887959  \n",
      "8191 -1.327384  0.496853  \n",
      "\n",
      "[8192 rows x 9 columns]\n",
      "I found a total of 0 categorical features.\n",
      "X= [[-0.01511921  0.36074091  0.46939777 ... -0.02549255  0.66407094\n",
      "   0.062763  ]\n",
      " [ 0.36047801 -0.30139478  0.62918307 ... -1.1967495  -1.0384439\n",
      "  -0.7174612 ]\n",
      " [ 1.5632379  -1.2947529   0.07898717 ... -1.2921402   1.5629882\n",
      "  -0.93773069]\n",
      " ...\n",
      " [ 0.10386245  1.0243292  -1.0472214  ... -0.65193433 -1.2418421\n",
      "   1.5302204 ]\n",
      " [-1.1093114  -0.02768995 -1.4487363  ...  0.81497099  1.1006145\n",
      "   1.3371587 ]\n",
      " [ 1.1550105  -0.42933117 -1.5672604  ... -0.12650646  0.33104203\n",
      "  -1.3273843 ]]\n",
      "y= [0.53652416 0.3080143  0.51889978 ... 0.80388768 0.88795855 0.49685261]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alberto\\anaconda3\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = openml.datasets.get_dataset(189)\n",
    "\n",
    "df, *_ = dataset.get_data()\n",
    "print(df)\n",
    "\n",
    "# as you noticed, some of the columns contain strings instead of numbers; we call these\n",
    "# \"categorical\" columns or features. We need to change that, as most ML algorithms\n",
    "# only process numbers. Don't worry too much about this part, it's just replacing strings\n",
    "# with numbers\n",
    "categorical_columns = df.select_dtypes(include=['category', 'object', 'string']).columns\n",
    "print(\"I found a total of %d categorical features.\" % len(categorical_columns))\n",
    "for c in categorical_columns :\n",
    "  df[c].replace({category : index for index, category in enumerate(df[c].astype('category').cat.categories)}, inplace=True)\n",
    "\n",
    "# also, remove all rows that contain invalid numerical values (for example, missing values)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# name of the target column\n",
    "target_feature = dataset.default_target_attribute\n",
    "other_features = [c for c in df.columns if c != target_feature]\n",
    "\n",
    "# get just the data without the column headers, as numerical matrices\n",
    "X = df[other_features].values\n",
    "y = df[target_feature].values\n",
    "\n",
    "print(\"X=\", X)\n",
    "print(\"y=\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyuolakxXy-v"
   },
   "source": [
    "This time, however, we will split the data into three parts: a training set, a validation set, and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1c93YEsrZBwu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole dataset: 8192 samples; training set: 5897 samples; validation set: 1475 samples; test set: 820 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# we use 10% of the data for the test, the rest for training and validation\n",
    "X_training, X_test, y_training, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=42)\n",
    "# we use 20% of the remaining data for validation, the rest for the training\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Whole dataset: %d samples; training set: %d samples; validation set: %d samples; test set: %d samples\"\n",
    "% (X.shape[0], X_train.shape[0], X_val.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqCTeBh9aNUA"
   },
   "source": [
    "Normalize the data, as usual by learning the normalization on the training set and applying it to the other two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Uk9XvF4HaWpX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_tensor has shape: torch.Size([5897])\n",
      "y_val_tensor has shape: torch.Size([1475])\n",
      "y_test_tensor has shape: torch.Size([820])\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler is an object that is able to learn and apply a normalization\n",
    "# that reduces the values of a feature to zero mean and unitary variance\n",
    "# so that most of the data values of a feature will fall into the interval (-1,1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_X = StandardScaler() # we need a separate instance of the StandardScaler object for X and y\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "scaler_X.fit(X_train)\n",
    "scaler_y.fit(y_train.reshape(-1,1))\n",
    "\n",
    "X_train_scaled = scaler_X.transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# don't worry too much about all this reshaping going on here, these function like\n",
    "# to have their inputs in a particular way, but the functions later like another\n",
    "# type of input, so we are forced to reshape and reshape again\n",
    "y_train_scaled = scaler_y.transform(y_train.reshape(-1,1)).reshape(-1,)\n",
    "y_val_scaled = scaler_y.transform(y_val.reshape(-1,1)).reshape(-1,)\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1,1)).reshape(-1,)\n",
    "\n",
    "# convert all the arrays to pytorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float)\n",
    "\n",
    "print(\"y_train_tensor has shape:\", y_train_tensor.shape)\n",
    "print(\"y_val_tensor has shape:\", y_val_tensor.shape)\n",
    "print(\"y_test_tensor has shape:\", y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kqj25aSaobQ"
   },
   "source": [
    "Now, let's go back to our first neural network for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_bcwT8DQazG1"
   },
   "outputs": [],
   "source": [
    "# we create a new class, that inherits from torch.nn.Module, the basic pytorch module\n",
    "class TwoLayerNeuralNetworkRegressor(torch.nn.Module) :\n",
    "\n",
    "  # first, the builder __init__ that is called every time the class is instantiated\n",
    "  # note that we added an additional argument, input_features_size, that we can modify\n",
    "  # to adapt this to problems with a different number of features\n",
    "  def __init__(self, input_features_size=8) :\n",
    "    super(TwoLayerNeuralNetworkRegressor, self).__init__() # call the __init__ of the parent class Module\n",
    "    self.linear_1 = torch.nn.Linear(input_features_size, 5) # linear layer, input_features_size inputs and 5 outputs\n",
    "    self.activation_function_1 = torch.nn.Sigmoid() # activation function\n",
    "    self.linear_2 = torch.nn.Linear(5, 1) # another linear layer, 5 inputs, 1 output (that will be intepreted as the prediction)\n",
    "\n",
    "  # the method 'forward' describes what happens during a forward pass\n",
    "  def forward(self, x) :\n",
    "    z_1 = self.linear_1(x) # pass inputs through first linear module\n",
    "    z_2 = self.activation_function_1(z_1) # pass output of linear layer through activation function\n",
    "    y_hat = self.linear_2(z_2) # pass output of activation function through last linear TwoLayerNeuralNetwork\n",
    "\n",
    "    # return the tensor in output of the last module as a prediction\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3KLiE3ja9M_"
   },
   "source": [
    "This time, your task will be to record the MSE on the validation set during the training process, and plot it at the end. What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HHdCgklFbNw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimization...\n",
      "Epoch 0: training loss=1.1617e+00, validation loss=1.2191e+00\n",
      "Epoch 100: training loss=9.7343e-01, validation loss=1.0228e+00\n",
      "Epoch 200: training loss=9.3795e-01, validation loss=9.8882e-01\n",
      "Epoch 300: training loss=8.9437e-01, validation loss=9.4711e-01\n",
      "Epoch 400: training loss=8.4072e-01, validation loss=8.9548e-01\n",
      "Epoch 500: training loss=7.8002e-01, validation loss=8.3669e-01\n",
      "Epoch 600: training loss=7.1987e-01, validation loss=7.7802e-01\n",
      "Epoch 700: training loss=6.6867e-01, validation loss=7.2771e-01\n",
      "Epoch 800: training loss=6.3109e-01, validation loss=6.9049e-01\n",
      "Epoch 900: training loss=6.0683e-01, validation loss=6.6622e-01\n",
      "Epoch 1000: training loss=5.9263e-01, validation loss=6.5185e-01\n",
      "Epoch 1100: training loss=5.8485e-01, validation loss=6.4386e-01\n",
      "Epoch 1200: training loss=5.8074e-01, validation loss=6.3957e-01\n",
      "Epoch 1300: training loss=5.7858e-01, validation loss=6.3726e-01\n",
      "Epoch 1400: training loss=5.7741e-01, validation loss=6.3598e-01\n",
      "Epoch 1500: training loss=5.7674e-01, validation loss=6.3522e-01\n",
      "Epoch 1600: training loss=5.7630e-01, validation loss=6.3473e-01\n",
      "Epoch 1700: training loss=5.7599e-01, validation loss=6.3437e-01\n",
      "Epoch 1800: training loss=5.7574e-01, validation loss=6.3408e-01\n",
      "Epoch 1900: training loss=5.7551e-01, validation loss=6.3383e-01\n",
      "Epoch 2000: training loss=5.7529e-01, validation loss=6.3359e-01\n",
      "Epoch 2100: training loss=5.7507e-01, validation loss=6.3336e-01\n",
      "Epoch 2200: training loss=5.7486e-01, validation loss=6.3313e-01\n",
      "Epoch 2300: training loss=5.7463e-01, validation loss=6.3289e-01\n",
      "Epoch 2400: training loss=5.7440e-01, validation loss=6.3265e-01\n",
      "Epoch 2500: training loss=5.7416e-01, validation loss=6.3240e-01\n",
      "Epoch 2600: training loss=5.7391e-01, validation loss=6.3214e-01\n",
      "Epoch 2700: training loss=5.7364e-01, validation loss=6.3186e-01\n",
      "Epoch 2800: training loss=5.7335e-01, validation loss=6.3157e-01\n",
      "Epoch 2900: training loss=5.7305e-01, validation loss=6.3127e-01\n",
      "Epoch 3000: training loss=5.7273e-01, validation loss=6.3094e-01\n",
      "Epoch 3100: training loss=5.7238e-01, validation loss=6.3060e-01\n",
      "Epoch 3200: training loss=5.7202e-01, validation loss=6.3023e-01\n",
      "Epoch 3300: training loss=5.7162e-01, validation loss=6.2983e-01\n",
      "Epoch 3400: training loss=5.7119e-01, validation loss=6.2941e-01\n",
      "Epoch 3500: training loss=5.7073e-01, validation loss=6.2895e-01\n",
      "Epoch 3600: training loss=5.7024e-01, validation loss=6.2846e-01\n",
      "Epoch 3700: training loss=5.6970e-01, validation loss=6.2793e-01\n",
      "Epoch 3800: training loss=5.6912e-01, validation loss=6.2735e-01\n",
      "Epoch 3900: training loss=5.6849e-01, validation loss=6.2673e-01\n",
      "Epoch 4000: training loss=5.6781e-01, validation loss=6.2606e-01\n",
      "Epoch 4100: training loss=5.6708e-01, validation loss=6.2533e-01\n",
      "Epoch 4200: training loss=5.6628e-01, validation loss=6.2454e-01\n",
      "Epoch 4300: training loss=5.6542e-01, validation loss=6.2369e-01\n",
      "Epoch 4400: training loss=5.6448e-01, validation loss=6.2276e-01\n",
      "Epoch 4500: training loss=5.6347e-01, validation loss=6.2176e-01\n",
      "Epoch 4600: training loss=5.6238e-01, validation loss=6.2067e-01\n",
      "Epoch 4700: training loss=5.6120e-01, validation loss=6.1950e-01\n",
      "Epoch 4800: training loss=5.5992e-01, validation loss=6.1823e-01\n",
      "Epoch 4900: training loss=5.5855e-01, validation loss=6.1686e-01\n",
      "Epoch 5000: training loss=5.5708e-01, validation loss=6.1539e-01\n",
      "Epoch 5100: training loss=5.5551e-01, validation loss=6.1381e-01\n",
      "Epoch 5200: training loss=5.5382e-01, validation loss=6.1211e-01\n",
      "Epoch 5300: training loss=5.5202e-01, validation loss=6.1030e-01\n",
      "Epoch 5400: training loss=5.5010e-01, validation loss=6.0836e-01\n",
      "Epoch 5500: training loss=5.4806e-01, validation loss=6.0630e-01\n",
      "Epoch 5600: training loss=5.4591e-01, validation loss=6.0411e-01\n",
      "Epoch 5700: training loss=5.4363e-01, validation loss=6.0179e-01\n",
      "Epoch 5800: training loss=5.4123e-01, validation loss=5.9935e-01\n",
      "Epoch 5900: training loss=5.3870e-01, validation loss=5.9677e-01\n",
      "Epoch 6000: training loss=5.3606e-01, validation loss=5.9407e-01\n",
      "Epoch 6100: training loss=5.3331e-01, validation loss=5.9123e-01\n",
      "Epoch 6200: training loss=5.3044e-01, validation loss=5.8828e-01\n",
      "Epoch 6300: training loss=5.2746e-01, validation loss=5.8521e-01\n",
      "Epoch 6400: training loss=5.2438e-01, validation loss=5.8202e-01\n",
      "Epoch 6500: training loss=5.2121e-01, validation loss=5.7873e-01\n",
      "Epoch 6600: training loss=5.1795e-01, validation loss=5.7533e-01\n",
      "Epoch 6700: training loss=5.1460e-01, validation loss=5.7184e-01\n",
      "Epoch 6800: training loss=5.1118e-01, validation loss=5.6826e-01\n",
      "Epoch 6900: training loss=5.0769e-01, validation loss=5.6460e-01\n",
      "Epoch 7000: training loss=5.0414e-01, validation loss=5.6087e-01\n",
      "Epoch 7100: training loss=5.0054e-01, validation loss=5.5707e-01\n",
      "Epoch 7200: training loss=4.9691e-01, validation loss=5.5322e-01\n",
      "Epoch 7300: training loss=4.9324e-01, validation loss=5.4933e-01\n",
      "Epoch 7400: training loss=4.8954e-01, validation loss=5.4540e-01\n",
      "Epoch 7500: training loss=4.8583e-01, validation loss=5.4144e-01\n",
      "Epoch 7600: training loss=4.8212e-01, validation loss=5.3746e-01\n",
      "Epoch 7700: training loss=4.7840e-01, validation loss=5.3348e-01\n",
      "Epoch 7800: training loss=4.7470e-01, validation loss=5.2949e-01\n",
      "Epoch 7900: training loss=4.7101e-01, validation loss=5.2551e-01\n",
      "Epoch 8000: training loss=4.6734e-01, validation loss=5.2154e-01\n",
      "Epoch 8100: training loss=4.6370e-01, validation loss=5.1759e-01\n",
      "Epoch 8200: training loss=4.6011e-01, validation loss=5.1367e-01\n",
      "Epoch 8300: training loss=4.5655e-01, validation loss=5.0979e-01\n",
      "Epoch 8400: training loss=4.5304e-01, validation loss=5.0594e-01\n",
      "Epoch 8500: training loss=4.4959e-01, validation loss=5.0214e-01\n",
      "Epoch 8600: training loss=4.4619e-01, validation loss=4.9840e-01\n",
      "Epoch 8700: training loss=4.4286e-01, validation loss=4.9471e-01\n",
      "Epoch 8800: training loss=4.3959e-01, validation loss=4.9108e-01\n",
      "Epoch 8900: training loss=4.3640e-01, validation loss=4.8752e-01\n",
      "Epoch 9000: training loss=4.3327e-01, validation loss=4.8403e-01\n",
      "Epoch 9100: training loss=4.3023e-01, validation loss=4.8061e-01\n",
      "Epoch 9200: training loss=4.2726e-01, validation loss=4.7726e-01\n",
      "Epoch 9300: training loss=4.2436e-01, validation loss=4.7399e-01\n",
      "Epoch 9400: training loss=4.2155e-01, validation loss=4.7080e-01\n",
      "Epoch 9500: training loss=4.1882e-01, validation loss=4.6768e-01\n",
      "Epoch 9600: training loss=4.1618e-01, validation loss=4.6465e-01\n",
      "Epoch 9700: training loss=4.1361e-01, validation loss=4.6170e-01\n",
      "Epoch 9800: training loss=4.1113e-01, validation loss=4.5883e-01\n",
      "Epoch 9900: training loss=4.0873e-01, validation loss=4.5604e-01\n",
      "Epoch 10000: training loss=4.0641e-01, validation loss=4.5334e-01\n",
      "Epoch 10100: training loss=4.0417e-01, validation loss=4.5071e-01\n",
      "Epoch 10200: training loss=4.0201e-01, validation loss=4.4817e-01\n",
      "Epoch 10300: training loss=3.9993e-01, validation loss=4.4570e-01\n",
      "Epoch 10400: training loss=3.9792e-01, validation loss=4.4331e-01\n",
      "Epoch 10500: training loss=3.9599e-01, validation loss=4.4099e-01\n",
      "Epoch 10600: training loss=3.9412e-01, validation loss=4.3875e-01\n",
      "Epoch 10700: training loss=3.9233e-01, validation loss=4.3658e-01\n",
      "Epoch 10800: training loss=3.9060e-01, validation loss=4.3447e-01\n",
      "Epoch 10900: training loss=3.8893e-01, validation loss=4.3244e-01\n",
      "Epoch 11000: training loss=3.8733e-01, validation loss=4.3047e-01\n",
      "Epoch 11100: training loss=3.8579e-01, validation loss=4.2855e-01\n",
      "Epoch 11200: training loss=3.8430e-01, validation loss=4.2670e-01\n",
      "Epoch 11300: training loss=3.8286e-01, validation loss=4.2491e-01\n",
      "Epoch 11400: training loss=3.8147e-01, validation loss=4.2316e-01\n",
      "Epoch 11500: training loss=3.8013e-01, validation loss=4.2147e-01\n",
      "Epoch 11600: training loss=3.7883e-01, validation loss=4.1982e-01\n",
      "Epoch 11700: training loss=3.7757e-01, validation loss=4.1822e-01\n",
      "Epoch 11800: training loss=3.7635e-01, validation loss=4.1666e-01\n",
      "Epoch 11900: training loss=3.7517e-01, validation loss=4.1514e-01\n",
      "Epoch 12000: training loss=3.7401e-01, validation loss=4.1365e-01\n",
      "Epoch 12100: training loss=3.7289e-01, validation loss=4.1220e-01\n",
      "Epoch 12200: training loss=3.7179e-01, validation loss=4.1078e-01\n",
      "Epoch 12300: training loss=3.7072e-01, validation loss=4.0938e-01\n",
      "Epoch 12400: training loss=3.6966e-01, validation loss=4.0801e-01\n",
      "Epoch 12500: training loss=3.6863e-01, validation loss=4.0666e-01\n",
      "Epoch 12600: training loss=3.6761e-01, validation loss=4.0533e-01\n",
      "Epoch 12700: training loss=3.6660e-01, validation loss=4.0402e-01\n",
      "Epoch 12800: training loss=3.6561e-01, validation loss=4.0272e-01\n",
      "Epoch 12900: training loss=3.6462e-01, validation loss=4.0143e-01\n",
      "Epoch 13000: training loss=3.6364e-01, validation loss=4.0016e-01\n",
      "Epoch 13100: training loss=3.6267e-01, validation loss=3.9889e-01\n",
      "Epoch 13200: training loss=3.6170e-01, validation loss=3.9762e-01\n",
      "Epoch 13300: training loss=3.6073e-01, validation loss=3.9636e-01\n",
      "Epoch 13400: training loss=3.5976e-01, validation loss=3.9510e-01\n",
      "Epoch 13500: training loss=3.5879e-01, validation loss=3.9384e-01\n",
      "Epoch 13600: training loss=3.5781e-01, validation loss=3.9258e-01\n",
      "Epoch 13700: training loss=3.5683e-01, validation loss=3.9132e-01\n",
      "Epoch 13800: training loss=3.5585e-01, validation loss=3.9005e-01\n",
      "Epoch 13900: training loss=3.5485e-01, validation loss=3.8878e-01\n",
      "Epoch 14000: training loss=3.5385e-01, validation loss=3.8750e-01\n",
      "Epoch 14100: training loss=3.5284e-01, validation loss=3.8621e-01\n",
      "Epoch 14200: training loss=3.5181e-01, validation loss=3.8491e-01\n",
      "Epoch 14300: training loss=3.5078e-01, validation loss=3.8360e-01\n",
      "Epoch 14400: training loss=3.4973e-01, validation loss=3.8228e-01\n",
      "Epoch 14500: training loss=3.4867e-01, validation loss=3.8095e-01\n",
      "Epoch 14600: training loss=3.4760e-01, validation loss=3.7960e-01\n",
      "Epoch 14700: training loss=3.4652e-01, validation loss=3.7825e-01\n",
      "Epoch 14800: training loss=3.4542e-01, validation loss=3.7688e-01\n",
      "Epoch 14900: training loss=3.4431e-01, validation loss=3.7550e-01\n",
      "Epoch 15000: training loss=3.4318e-01, validation loss=3.7411e-01\n",
      "Epoch 15100: training loss=3.4204e-01, validation loss=3.7270e-01\n",
      "Epoch 15200: training loss=3.4089e-01, validation loss=3.7128e-01\n",
      "Epoch 15300: training loss=3.3972e-01, validation loss=3.6986e-01\n",
      "Epoch 15400: training loss=3.3854e-01, validation loss=3.6842e-01\n",
      "Epoch 15500: training loss=3.3735e-01, validation loss=3.6696e-01\n",
      "Epoch 15600: training loss=3.3615e-01, validation loss=3.6550e-01\n",
      "Epoch 15700: training loss=3.3493e-01, validation loss=3.6403e-01\n",
      "Epoch 15800: training loss=3.3370e-01, validation loss=3.6255e-01\n",
      "Epoch 15900: training loss=3.3247e-01, validation loss=3.6106e-01\n",
      "Epoch 16000: training loss=3.3122e-01, validation loss=3.5957e-01\n",
      "Epoch 16100: training loss=3.2997e-01, validation loss=3.5807e-01\n",
      "Epoch 16200: training loss=3.2870e-01, validation loss=3.5656e-01\n",
      "Epoch 16300: training loss=3.2743e-01, validation loss=3.5505e-01\n",
      "Epoch 16400: training loss=3.2616e-01, validation loss=3.5354e-01\n",
      "Epoch 16500: training loss=3.2488e-01, validation loss=3.5202e-01\n",
      "Epoch 16600: training loss=3.2359e-01, validation loss=3.5051e-01\n",
      "Epoch 16700: training loss=3.2230e-01, validation loss=3.4899e-01\n",
      "Epoch 16800: training loss=3.2101e-01, validation loss=3.4748e-01\n",
      "Epoch 16900: training loss=3.1972e-01, validation loss=3.4597e-01\n",
      "Epoch 17000: training loss=3.1843e-01, validation loss=3.4447e-01\n",
      "Epoch 17100: training loss=3.1714e-01, validation loss=3.4297e-01\n",
      "Epoch 17200: training loss=3.1585e-01, validation loss=3.4148e-01\n",
      "Epoch 17300: training loss=3.1457e-01, validation loss=3.3999e-01\n",
      "Epoch 17400: training loss=3.1329e-01, validation loss=3.3852e-01\n",
      "Epoch 17500: training loss=3.1201e-01, validation loss=3.3706e-01\n",
      "Epoch 17600: training loss=3.1075e-01, validation loss=3.3561e-01\n",
      "Epoch 17700: training loss=3.0949e-01, validation loss=3.3418e-01\n",
      "Epoch 17800: training loss=3.0824e-01, validation loss=3.3276e-01\n",
      "Epoch 17900: training loss=3.0700e-01, validation loss=3.3136e-01\n",
      "Epoch 18000: training loss=3.0577e-01, validation loss=3.2997e-01\n",
      "Epoch 18100: training loss=3.0456e-01, validation loss=3.2860e-01\n",
      "Epoch 18200: training loss=3.0335e-01, validation loss=3.2725e-01\n",
      "Epoch 18300: training loss=3.0217e-01, validation loss=3.2592e-01\n",
      "Epoch 18400: training loss=3.0099e-01, validation loss=3.2461e-01\n",
      "Epoch 18500: training loss=2.9983e-01, validation loss=3.2333e-01\n",
      "Epoch 18600: training loss=2.9869e-01, validation loss=3.2206e-01\n",
      "Epoch 18700: training loss=2.9756e-01, validation loss=3.2082e-01\n",
      "Epoch 18800: training loss=2.9645e-01, validation loss=3.1960e-01\n",
      "Epoch 18900: training loss=2.9536e-01, validation loss=3.1840e-01\n",
      "Epoch 19000: training loss=2.9429e-01, validation loss=3.1723e-01\n",
      "Epoch 19100: training loss=2.9323e-01, validation loss=3.1609e-01\n",
      "Epoch 19200: training loss=2.9220e-01, validation loss=3.1496e-01\n",
      "Epoch 19300: training loss=2.9118e-01, validation loss=3.1386e-01\n",
      "Epoch 19400: training loss=2.9018e-01, validation loss=3.1279e-01\n",
      "Epoch 19500: training loss=2.8920e-01, validation loss=3.1174e-01\n",
      "Epoch 19600: training loss=2.8824e-01, validation loss=3.1072e-01\n",
      "Epoch 19700: training loss=2.8730e-01, validation loss=3.0972e-01\n",
      "Epoch 19800: training loss=2.8637e-01, validation loss=3.0874e-01\n",
      "Epoch 19900: training loss=2.8547e-01, validation loss=3.0779e-01\n",
      "Epoch 20000: training loss=2.8458e-01, validation loss=3.0686e-01\n",
      "Epoch 20100: training loss=2.8372e-01, validation loss=3.0595e-01\n",
      "Epoch 20200: training loss=2.8287e-01, validation loss=3.0507e-01\n",
      "Epoch 20300: training loss=2.8204e-01, validation loss=3.0421e-01\n",
      "Epoch 20400: training loss=2.8123e-01, validation loss=3.0337e-01\n",
      "Epoch 20500: training loss=2.8043e-01, validation loss=3.0255e-01\n",
      "Epoch 20600: training loss=2.7966e-01, validation loss=3.0176e-01\n",
      "Epoch 20700: training loss=2.7889e-01, validation loss=3.0098e-01\n",
      "Epoch 20800: training loss=2.7815e-01, validation loss=3.0023e-01\n",
      "Epoch 20900: training loss=2.7742e-01, validation loss=2.9949e-01\n",
      "Epoch 21000: training loss=2.7671e-01, validation loss=2.9878e-01\n",
      "Epoch 21100: training loss=2.7601e-01, validation loss=2.9808e-01\n",
      "Epoch 21200: training loss=2.7533e-01, validation loss=2.9740e-01\n",
      "Epoch 21300: training loss=2.7467e-01, validation loss=2.9674e-01\n",
      "Epoch 21400: training loss=2.7401e-01, validation loss=2.9610e-01\n",
      "Epoch 21500: training loss=2.7338e-01, validation loss=2.9547e-01\n",
      "Epoch 21600: training loss=2.7275e-01, validation loss=2.9486e-01\n",
      "Epoch 21700: training loss=2.7214e-01, validation loss=2.9426e-01\n",
      "Epoch 21800: training loss=2.7154e-01, validation loss=2.9368e-01\n",
      "Epoch 21900: training loss=2.7095e-01, validation loss=2.9312e-01\n",
      "Epoch 22000: training loss=2.7038e-01, validation loss=2.9257e-01\n",
      "Epoch 22100: training loss=2.6982e-01, validation loss=2.9203e-01\n",
      "Epoch 22200: training loss=2.6927e-01, validation loss=2.9150e-01\n",
      "Epoch 22300: training loss=2.6873e-01, validation loss=2.9099e-01\n",
      "Epoch 22400: training loss=2.6820e-01, validation loss=2.9049e-01\n",
      "Epoch 22500: training loss=2.6768e-01, validation loss=2.9001e-01\n",
      "Epoch 22600: training loss=2.6718e-01, validation loss=2.8953e-01\n",
      "Epoch 22700: training loss=2.6668e-01, validation loss=2.8907e-01\n",
      "Epoch 22800: training loss=2.6619e-01, validation loss=2.8862e-01\n",
      "Epoch 22900: training loss=2.6571e-01, validation loss=2.8817e-01\n",
      "Epoch 23000: training loss=2.6524e-01, validation loss=2.8774e-01\n",
      "Epoch 23100: training loss=2.6478e-01, validation loss=2.8732e-01\n",
      "Epoch 23200: training loss=2.6433e-01, validation loss=2.8691e-01\n",
      "Epoch 23300: training loss=2.6389e-01, validation loss=2.8650e-01\n",
      "Epoch 23400: training loss=2.6345e-01, validation loss=2.8611e-01\n",
      "Epoch 23500: training loss=2.6302e-01, validation loss=2.8572e-01\n",
      "Epoch 23600: training loss=2.6261e-01, validation loss=2.8535e-01\n",
      "Epoch 23700: training loss=2.6219e-01, validation loss=2.8498e-01\n",
      "Epoch 23800: training loss=2.6179e-01, validation loss=2.8462e-01\n",
      "Epoch 23900: training loss=2.6139e-01, validation loss=2.8426e-01\n",
      "Epoch 24000: training loss=2.6100e-01, validation loss=2.8392e-01\n",
      "Epoch 24100: training loss=2.6062e-01, validation loss=2.8358e-01\n",
      "Epoch 24200: training loss=2.6024e-01, validation loss=2.8325e-01\n",
      "Epoch 24300: training loss=2.5987e-01, validation loss=2.8293e-01\n",
      "Epoch 24400: training loss=2.5951e-01, validation loss=2.8261e-01\n",
      "Epoch 24500: training loss=2.5915e-01, validation loss=2.8230e-01\n",
      "Epoch 24600: training loss=2.5880e-01, validation loss=2.8199e-01\n",
      "Epoch 24700: training loss=2.5845e-01, validation loss=2.8170e-01\n",
      "Epoch 24800: training loss=2.5811e-01, validation loss=2.8140e-01\n",
      "Epoch 24900: training loss=2.5777e-01, validation loss=2.8112e-01\n",
      "Epoch 25000: training loss=2.5744e-01, validation loss=2.8084e-01\n",
      "Epoch 25100: training loss=2.5712e-01, validation loss=2.8056e-01\n",
      "Epoch 25200: training loss=2.5680e-01, validation loss=2.8029e-01\n",
      "Epoch 25300: training loss=2.5648e-01, validation loss=2.8003e-01\n",
      "Epoch 25400: training loss=2.5617e-01, validation loss=2.7977e-01\n",
      "Epoch 25500: training loss=2.5587e-01, validation loss=2.7951e-01\n",
      "Epoch 25600: training loss=2.5557e-01, validation loss=2.7926e-01\n",
      "Epoch 25700: training loss=2.5527e-01, validation loss=2.7902e-01\n",
      "Epoch 25800: training loss=2.5498e-01, validation loss=2.7878e-01\n",
      "Epoch 25900: training loss=2.5469e-01, validation loss=2.7854e-01\n",
      "Epoch 26000: training loss=2.5441e-01, validation loss=2.7831e-01\n",
      "Epoch 26100: training loss=2.5413e-01, validation loss=2.7808e-01\n",
      "Epoch 26200: training loss=2.5386e-01, validation loss=2.7785e-01\n",
      "Epoch 26300: training loss=2.5358e-01, validation loss=2.7763e-01\n",
      "Epoch 26400: training loss=2.5332e-01, validation loss=2.7742e-01\n",
      "Epoch 26500: training loss=2.5305e-01, validation loss=2.7720e-01\n",
      "Epoch 26600: training loss=2.5279e-01, validation loss=2.7700e-01\n",
      "Epoch 26700: training loss=2.5253e-01, validation loss=2.7679e-01\n",
      "Epoch 26800: training loss=2.5228e-01, validation loss=2.7659e-01\n",
      "Epoch 26900: training loss=2.5203e-01, validation loss=2.7639e-01\n",
      "Epoch 27000: training loss=2.5178e-01, validation loss=2.7619e-01\n",
      "Epoch 27100: training loss=2.5154e-01, validation loss=2.7600e-01\n",
      "Epoch 27200: training loss=2.5130e-01, validation loss=2.7581e-01\n",
      "Epoch 27300: training loss=2.5106e-01, validation loss=2.7562e-01\n",
      "Epoch 27400: training loss=2.5082e-01, validation loss=2.7544e-01\n",
      "Epoch 27500: training loss=2.5059e-01, validation loss=2.7526e-01\n",
      "Epoch 27600: training loss=2.5036e-01, validation loss=2.7508e-01\n",
      "Epoch 27700: training loss=2.5013e-01, validation loss=2.7490e-01\n",
      "Epoch 27800: training loss=2.4991e-01, validation loss=2.7473e-01\n",
      "Epoch 27900: training loss=2.4968e-01, validation loss=2.7456e-01\n",
      "Epoch 28000: training loss=2.4946e-01, validation loss=2.7439e-01\n",
      "Epoch 28100: training loss=2.4924e-01, validation loss=2.7422e-01\n",
      "Epoch 28200: training loss=2.4903e-01, validation loss=2.7406e-01\n",
      "Epoch 28300: training loss=2.4882e-01, validation loss=2.7390e-01\n",
      "Epoch 28400: training loss=2.4860e-01, validation loss=2.7374e-01\n",
      "Epoch 28500: training loss=2.4839e-01, validation loss=2.7358e-01\n",
      "Epoch 28600: training loss=2.4819e-01, validation loss=2.7342e-01\n",
      "Epoch 28700: training loss=2.4798e-01, validation loss=2.7327e-01\n",
      "Epoch 28800: training loss=2.4778e-01, validation loss=2.7311e-01\n",
      "Epoch 28900: training loss=2.4757e-01, validation loss=2.7296e-01\n",
      "Epoch 29000: training loss=2.4737e-01, validation loss=2.7281e-01\n",
      "Epoch 29100: training loss=2.4717e-01, validation loss=2.7266e-01\n",
      "Epoch 29200: training loss=2.4697e-01, validation loss=2.7252e-01\n",
      "Epoch 29300: training loss=2.4678e-01, validation loss=2.7237e-01\n",
      "Epoch 29400: training loss=2.4658e-01, validation loss=2.7223e-01\n",
      "Epoch 29500: training loss=2.4639e-01, validation loss=2.7208e-01\n",
      "Epoch 29600: training loss=2.4620e-01, validation loss=2.7194e-01\n",
      "Epoch 29700: training loss=2.4601e-01, validation loss=2.7180e-01\n",
      "Epoch 29800: training loss=2.4582e-01, validation loss=2.7166e-01\n",
      "Epoch 29900: training loss=2.4563e-01, validation loss=2.7152e-01\n",
      "Epoch 30000: training loss=2.4544e-01, validation loss=2.7138e-01\n",
      "Epoch 30100: training loss=2.4525e-01, validation loss=2.7125e-01\n",
      "Epoch 30200: training loss=2.4507e-01, validation loss=2.7111e-01\n",
      "Epoch 30300: training loss=2.4488e-01, validation loss=2.7097e-01\n",
      "Epoch 30400: training loss=2.4470e-01, validation loss=2.7084e-01\n",
      "Epoch 30500: training loss=2.4452e-01, validation loss=2.7070e-01\n",
      "Epoch 30600: training loss=2.4434e-01, validation loss=2.7057e-01\n",
      "Epoch 30700: training loss=2.4416e-01, validation loss=2.7044e-01\n",
      "Epoch 30800: training loss=2.4398e-01, validation loss=2.7030e-01\n",
      "Epoch 30900: training loss=2.4380e-01, validation loss=2.7017e-01\n",
      "Epoch 31000: training loss=2.4362e-01, validation loss=2.7004e-01\n",
      "Epoch 31100: training loss=2.4344e-01, validation loss=2.6991e-01\n",
      "Epoch 31200: training loss=2.4326e-01, validation loss=2.6978e-01\n",
      "Epoch 31300: training loss=2.4308e-01, validation loss=2.6964e-01\n",
      "Epoch 31400: training loss=2.4291e-01, validation loss=2.6951e-01\n",
      "Epoch 31500: training loss=2.4273e-01, validation loss=2.6938e-01\n",
      "Epoch 31600: training loss=2.4256e-01, validation loss=2.6925e-01\n",
      "Epoch 31700: training loss=2.4238e-01, validation loss=2.6912e-01\n",
      "Epoch 31800: training loss=2.4221e-01, validation loss=2.6899e-01\n",
      "Epoch 31900: training loss=2.4203e-01, validation loss=2.6886e-01\n",
      "Epoch 32000: training loss=2.4186e-01, validation loss=2.6873e-01\n",
      "Epoch 32100: training loss=2.4169e-01, validation loss=2.6860e-01\n",
      "Epoch 32200: training loss=2.4151e-01, validation loss=2.6846e-01\n",
      "Epoch 32300: training loss=2.4134e-01, validation loss=2.6833e-01\n",
      "Epoch 32400: training loss=2.4117e-01, validation loss=2.6820e-01\n",
      "Epoch 32500: training loss=2.4099e-01, validation loss=2.6807e-01\n",
      "Epoch 32600: training loss=2.4082e-01, validation loss=2.6794e-01\n",
      "Epoch 32700: training loss=2.4065e-01, validation loss=2.6780e-01\n",
      "Epoch 32800: training loss=2.4048e-01, validation loss=2.6767e-01\n",
      "Epoch 32900: training loss=2.4031e-01, validation loss=2.6754e-01\n",
      "Epoch 33000: training loss=2.4014e-01, validation loss=2.6740e-01\n",
      "Epoch 33100: training loss=2.3997e-01, validation loss=2.6727e-01\n",
      "Epoch 33200: training loss=2.3980e-01, validation loss=2.6713e-01\n",
      "Epoch 33300: training loss=2.3963e-01, validation loss=2.6700e-01\n",
      "Epoch 33400: training loss=2.3946e-01, validation loss=2.6686e-01\n",
      "Epoch 33500: training loss=2.3929e-01, validation loss=2.6673e-01\n",
      "Epoch 33600: training loss=2.3912e-01, validation loss=2.6659e-01\n",
      "Epoch 33700: training loss=2.3895e-01, validation loss=2.6645e-01\n",
      "Epoch 33800: training loss=2.3878e-01, validation loss=2.6632e-01\n",
      "Epoch 33900: training loss=2.3861e-01, validation loss=2.6618e-01\n",
      "Epoch 34000: training loss=2.3844e-01, validation loss=2.6604e-01\n",
      "Epoch 34100: training loss=2.3827e-01, validation loss=2.6590e-01\n",
      "Epoch 34200: training loss=2.3810e-01, validation loss=2.6576e-01\n",
      "Epoch 34300: training loss=2.3793e-01, validation loss=2.6562e-01\n",
      "Epoch 34400: training loss=2.3776e-01, validation loss=2.6548e-01\n",
      "Epoch 34500: training loss=2.3760e-01, validation loss=2.6534e-01\n",
      "Epoch 34600: training loss=2.3743e-01, validation loss=2.6519e-01\n",
      "Epoch 34700: training loss=2.3726e-01, validation loss=2.6505e-01\n",
      "Epoch 34800: training loss=2.3710e-01, validation loss=2.6491e-01\n",
      "Epoch 34900: training loss=2.3693e-01, validation loss=2.6476e-01\n",
      "Epoch 35000: training loss=2.3676e-01, validation loss=2.6462e-01\n",
      "Epoch 35100: training loss=2.3660e-01, validation loss=2.6448e-01\n",
      "Epoch 35200: training loss=2.3643e-01, validation loss=2.6433e-01\n",
      "Epoch 35300: training loss=2.3627e-01, validation loss=2.6419e-01\n",
      "Epoch 35400: training loss=2.3610e-01, validation loss=2.6404e-01\n",
      "Epoch 35500: training loss=2.3594e-01, validation loss=2.6389e-01\n",
      "Epoch 35600: training loss=2.3578e-01, validation loss=2.6375e-01\n",
      "Epoch 35700: training loss=2.3561e-01, validation loss=2.6360e-01\n",
      "Epoch 35800: training loss=2.3545e-01, validation loss=2.6345e-01\n",
      "Epoch 35900: training loss=2.3529e-01, validation loss=2.6330e-01\n",
      "Epoch 36000: training loss=2.3513e-01, validation loss=2.6316e-01\n",
      "Epoch 36100: training loss=2.3497e-01, validation loss=2.6301e-01\n",
      "Epoch 36200: training loss=2.3481e-01, validation loss=2.6286e-01\n",
      "Epoch 36300: training loss=2.3465e-01, validation loss=2.6271e-01\n",
      "Epoch 36400: training loss=2.3449e-01, validation loss=2.6256e-01\n",
      "Epoch 36500: training loss=2.3434e-01, validation loss=2.6241e-01\n",
      "Epoch 36600: training loss=2.3418e-01, validation loss=2.6226e-01\n",
      "Epoch 36700: training loss=2.3402e-01, validation loss=2.6212e-01\n",
      "Epoch 36800: training loss=2.3387e-01, validation loss=2.6197e-01\n",
      "Epoch 36900: training loss=2.3372e-01, validation loss=2.6182e-01\n",
      "Epoch 37000: training loss=2.3356e-01, validation loss=2.6167e-01\n",
      "Epoch 37100: training loss=2.3341e-01, validation loss=2.6152e-01\n",
      "Epoch 37200: training loss=2.3326e-01, validation loss=2.6137e-01\n",
      "Epoch 37300: training loss=2.3311e-01, validation loss=2.6122e-01\n",
      "Epoch 37400: training loss=2.3296e-01, validation loss=2.6107e-01\n",
      "Epoch 37500: training loss=2.3282e-01, validation loss=2.6093e-01\n",
      "Epoch 37600: training loss=2.3267e-01, validation loss=2.6078e-01\n",
      "Epoch 37700: training loss=2.3253e-01, validation loss=2.6063e-01\n",
      "Epoch 37800: training loss=2.3238e-01, validation loss=2.6048e-01\n",
      "Epoch 37900: training loss=2.3224e-01, validation loss=2.6034e-01\n",
      "Epoch 38000: training loss=2.3210e-01, validation loss=2.6019e-01\n",
      "Epoch 38100: training loss=2.3196e-01, validation loss=2.6005e-01\n",
      "Epoch 38200: training loss=2.3182e-01, validation loss=2.5990e-01\n",
      "Epoch 38300: training loss=2.3168e-01, validation loss=2.5976e-01\n",
      "Epoch 38400: training loss=2.3155e-01, validation loss=2.5961e-01\n",
      "Epoch 38500: training loss=2.3141e-01, validation loss=2.5947e-01\n",
      "Epoch 38600: training loss=2.3128e-01, validation loss=2.5932e-01\n",
      "Epoch 38700: training loss=2.3114e-01, validation loss=2.5918e-01\n",
      "Epoch 38800: training loss=2.3101e-01, validation loss=2.5904e-01\n",
      "Epoch 38900: training loss=2.3088e-01, validation loss=2.5890e-01\n",
      "Epoch 39000: training loss=2.3076e-01, validation loss=2.5876e-01\n",
      "Epoch 39100: training loss=2.3063e-01, validation loss=2.5862e-01\n",
      "Epoch 39200: training loss=2.3050e-01, validation loss=2.5848e-01\n",
      "Epoch 39300: training loss=2.3038e-01, validation loss=2.5834e-01\n",
      "Epoch 39400: training loss=2.3026e-01, validation loss=2.5821e-01\n",
      "Epoch 39500: training loss=2.3013e-01, validation loss=2.5807e-01\n",
      "Epoch 39600: training loss=2.3001e-01, validation loss=2.5793e-01\n",
      "Epoch 39700: training loss=2.2990e-01, validation loss=2.5780e-01\n",
      "Epoch 39800: training loss=2.2978e-01, validation loss=2.5766e-01\n",
      "Epoch 39900: training loss=2.2966e-01, validation loss=2.5753e-01\n",
      "Epoch 40000: training loss=2.2955e-01, validation loss=2.5740e-01\n",
      "Epoch 40100: training loss=2.2943e-01, validation loss=2.5727e-01\n",
      "Epoch 40200: training loss=2.2932e-01, validation loss=2.5714e-01\n",
      "Epoch 40300: training loss=2.2921e-01, validation loss=2.5701e-01\n",
      "Epoch 40400: training loss=2.2910e-01, validation loss=2.5688e-01\n",
      "Epoch 40500: training loss=2.2899e-01, validation loss=2.5675e-01\n",
      "Epoch 40600: training loss=2.2889e-01, validation loss=2.5662e-01\n",
      "Epoch 40700: training loss=2.2878e-01, validation loss=2.5650e-01\n",
      "Epoch 40800: training loss=2.2868e-01, validation loss=2.5637e-01\n",
      "Epoch 40900: training loss=2.2858e-01, validation loss=2.5625e-01\n",
      "Epoch 41000: training loss=2.2847e-01, validation loss=2.5613e-01\n",
      "Epoch 41100: training loss=2.2837e-01, validation loss=2.5600e-01\n",
      "Epoch 41200: training loss=2.2827e-01, validation loss=2.5588e-01\n",
      "Epoch 41300: training loss=2.2818e-01, validation loss=2.5576e-01\n",
      "Epoch 41400: training loss=2.2808e-01, validation loss=2.5564e-01\n",
      "Epoch 41500: training loss=2.2798e-01, validation loss=2.5553e-01\n",
      "Epoch 41600: training loss=2.2789e-01, validation loss=2.5541e-01\n",
      "Epoch 41700: training loss=2.2780e-01, validation loss=2.5529e-01\n",
      "Epoch 41800: training loss=2.2770e-01, validation loss=2.5518e-01\n",
      "Epoch 41900: training loss=2.2761e-01, validation loss=2.5506e-01\n",
      "Epoch 42000: training loss=2.2752e-01, validation loss=2.5495e-01\n",
      "Epoch 42100: training loss=2.2744e-01, validation loss=2.5483e-01\n",
      "Epoch 42200: training loss=2.2735e-01, validation loss=2.5472e-01\n",
      "Epoch 42300: training loss=2.2726e-01, validation loss=2.5461e-01\n",
      "Epoch 42400: training loss=2.2718e-01, validation loss=2.5450e-01\n",
      "Epoch 42500: training loss=2.2709e-01, validation loss=2.5439e-01\n",
      "Epoch 42600: training loss=2.2701e-01, validation loss=2.5429e-01\n",
      "Epoch 42700: training loss=2.2693e-01, validation loss=2.5418e-01\n",
      "Epoch 42800: training loss=2.2684e-01, validation loss=2.5407e-01\n",
      "Epoch 42900: training loss=2.2676e-01, validation loss=2.5397e-01\n",
      "Epoch 43000: training loss=2.2668e-01, validation loss=2.5386e-01\n",
      "Epoch 43100: training loss=2.2661e-01, validation loss=2.5376e-01\n",
      "Epoch 43200: training loss=2.2653e-01, validation loss=2.5365e-01\n",
      "Epoch 43300: training loss=2.2645e-01, validation loss=2.5355e-01\n",
      "Epoch 43400: training loss=2.2638e-01, validation loss=2.5345e-01\n",
      "Epoch 43500: training loss=2.2630e-01, validation loss=2.5335e-01\n",
      "Epoch 43600: training loss=2.2623e-01, validation loss=2.5325e-01\n",
      "Epoch 43700: training loss=2.2615e-01, validation loss=2.5315e-01\n",
      "Epoch 43800: training loss=2.2608e-01, validation loss=2.5305e-01\n",
      "Epoch 43900: training loss=2.2601e-01, validation loss=2.5296e-01\n",
      "Epoch 44000: training loss=2.2594e-01, validation loss=2.5286e-01\n",
      "Epoch 44100: training loss=2.2587e-01, validation loss=2.5277e-01\n",
      "Epoch 44200: training loss=2.2580e-01, validation loss=2.5267e-01\n",
      "Epoch 44300: training loss=2.2573e-01, validation loss=2.5258e-01\n",
      "Epoch 44400: training loss=2.2567e-01, validation loss=2.5248e-01\n",
      "Epoch 44500: training loss=2.2560e-01, validation loss=2.5239e-01\n",
      "Epoch 44600: training loss=2.2553e-01, validation loss=2.5230e-01\n",
      "Epoch 44700: training loss=2.2547e-01, validation loss=2.5221e-01\n",
      "Epoch 44800: training loss=2.2540e-01, validation loss=2.5212e-01\n",
      "Epoch 44900: training loss=2.2534e-01, validation loss=2.5203e-01\n",
      "Epoch 45000: training loss=2.2528e-01, validation loss=2.5194e-01\n",
      "Epoch 45100: training loss=2.2521e-01, validation loss=2.5185e-01\n",
      "Epoch 45200: training loss=2.2515e-01, validation loss=2.5176e-01\n",
      "Epoch 45300: training loss=2.2509e-01, validation loss=2.5168e-01\n",
      "Epoch 45400: training loss=2.2503e-01, validation loss=2.5159e-01\n",
      "Epoch 45500: training loss=2.2497e-01, validation loss=2.5151e-01\n",
      "Epoch 45600: training loss=2.2491e-01, validation loss=2.5142e-01\n",
      "Epoch 45700: training loss=2.2485e-01, validation loss=2.5134e-01\n",
      "Epoch 45800: training loss=2.2479e-01, validation loss=2.5125e-01\n",
      "Epoch 45900: training loss=2.2474e-01, validation loss=2.5117e-01\n",
      "Epoch 46000: training loss=2.2468e-01, validation loss=2.5109e-01\n",
      "Epoch 46100: training loss=2.2462e-01, validation loss=2.5101e-01\n",
      "Epoch 46200: training loss=2.2457e-01, validation loss=2.5093e-01\n",
      "Epoch 46300: training loss=2.2451e-01, validation loss=2.5085e-01\n",
      "Epoch 46400: training loss=2.2446e-01, validation loss=2.5077e-01\n",
      "Epoch 46500: training loss=2.2440e-01, validation loss=2.5069e-01\n",
      "Epoch 46600: training loss=2.2435e-01, validation loss=2.5061e-01\n",
      "Epoch 46700: training loss=2.2430e-01, validation loss=2.5053e-01\n",
      "Epoch 46800: training loss=2.2425e-01, validation loss=2.5045e-01\n",
      "Epoch 46900: training loss=2.2419e-01, validation loss=2.5038e-01\n",
      "Epoch 47000: training loss=2.2414e-01, validation loss=2.5030e-01\n",
      "Epoch 47100: training loss=2.2409e-01, validation loss=2.5022e-01\n",
      "Epoch 47200: training loss=2.2404e-01, validation loss=2.5015e-01\n",
      "Epoch 47300: training loss=2.2399e-01, validation loss=2.5007e-01\n",
      "Epoch 47400: training loss=2.2394e-01, validation loss=2.5000e-01\n",
      "Epoch 47500: training loss=2.2389e-01, validation loss=2.4993e-01\n",
      "Epoch 47600: training loss=2.2384e-01, validation loss=2.4985e-01\n",
      "Epoch 47700: training loss=2.2379e-01, validation loss=2.4978e-01\n",
      "Epoch 47800: training loss=2.2375e-01, validation loss=2.4971e-01\n",
      "Epoch 47900: training loss=2.2370e-01, validation loss=2.4964e-01\n",
      "Epoch 48000: training loss=2.2365e-01, validation loss=2.4957e-01\n",
      "Epoch 48100: training loss=2.2361e-01, validation loss=2.4949e-01\n",
      "Epoch 48200: training loss=2.2356e-01, validation loss=2.4942e-01\n",
      "Epoch 48300: training loss=2.2351e-01, validation loss=2.4935e-01\n",
      "Epoch 48400: training loss=2.2347e-01, validation loss=2.4928e-01\n",
      "Epoch 48500: training loss=2.2342e-01, validation loss=2.4922e-01\n",
      "Epoch 48600: training loss=2.2338e-01, validation loss=2.4915e-01\n",
      "Epoch 48700: training loss=2.2333e-01, validation loss=2.4908e-01\n",
      "Epoch 48800: training loss=2.2329e-01, validation loss=2.4901e-01\n",
      "Epoch 48900: training loss=2.2325e-01, validation loss=2.4894e-01\n",
      "Epoch 49000: training loss=2.2320e-01, validation loss=2.4888e-01\n",
      "Epoch 49100: training loss=2.2316e-01, validation loss=2.4881e-01\n",
      "Epoch 49200: training loss=2.2312e-01, validation loss=2.4875e-01\n",
      "Epoch 49300: training loss=2.2308e-01, validation loss=2.4868e-01\n",
      "Epoch 49400: training loss=2.2303e-01, validation loss=2.4861e-01\n",
      "Epoch 49500: training loss=2.2299e-01, validation loss=2.4855e-01\n",
      "Epoch 49600: training loss=2.2295e-01, validation loss=2.4849e-01\n",
      "Epoch 49700: training loss=2.2291e-01, validation loss=2.4842e-01\n",
      "Epoch 49800: training loss=2.2287e-01, validation loss=2.4836e-01\n",
      "Epoch 49900: training loss=2.2283e-01, validation loss=2.4829e-01\n",
      "Epoch 50000: training loss=2.2279e-01, validation loss=2.4823e-01\n",
      "Epoch 50100: training loss=2.2275e-01, validation loss=2.4817e-01\n",
      "Epoch 50200: training loss=2.2271e-01, validation loss=2.4811e-01\n",
      "Epoch 50300: training loss=2.2267e-01, validation loss=2.4805e-01\n",
      "Epoch 50400: training loss=2.2263e-01, validation loss=2.4798e-01\n",
      "Epoch 50500: training loss=2.2260e-01, validation loss=2.4792e-01\n",
      "Epoch 50600: training loss=2.2256e-01, validation loss=2.4786e-01\n",
      "Epoch 50700: training loss=2.2252e-01, validation loss=2.4780e-01\n",
      "Epoch 50800: training loss=2.2248e-01, validation loss=2.4774e-01\n",
      "Epoch 50900: training loss=2.2244e-01, validation loss=2.4768e-01\n",
      "Epoch 51000: training loss=2.2241e-01, validation loss=2.4762e-01\n",
      "Epoch 51100: training loss=2.2237e-01, validation loss=2.4756e-01\n",
      "Epoch 51200: training loss=2.2233e-01, validation loss=2.4750e-01\n",
      "Epoch 51300: training loss=2.2230e-01, validation loss=2.4745e-01\n",
      "Epoch 51400: training loss=2.2226e-01, validation loss=2.4739e-01\n",
      "Epoch 51500: training loss=2.2222e-01, validation loss=2.4733e-01\n",
      "Epoch 51600: training loss=2.2219e-01, validation loss=2.4727e-01\n",
      "Epoch 51700: training loss=2.2215e-01, validation loss=2.4721e-01\n",
      "Epoch 51800: training loss=2.2212e-01, validation loss=2.4716e-01\n",
      "Epoch 51900: training loss=2.2208e-01, validation loss=2.4710e-01\n",
      "Epoch 52000: training loss=2.2205e-01, validation loss=2.4704e-01\n",
      "Epoch 52100: training loss=2.2201e-01, validation loss=2.4699e-01\n",
      "Epoch 52200: training loss=2.2198e-01, validation loss=2.4693e-01\n",
      "Epoch 52300: training loss=2.2195e-01, validation loss=2.4688e-01\n",
      "Epoch 52400: training loss=2.2191e-01, validation loss=2.4682e-01\n",
      "Epoch 52500: training loss=2.2188e-01, validation loss=2.4677e-01\n",
      "Epoch 52600: training loss=2.2184e-01, validation loss=2.4671e-01\n",
      "Epoch 52700: training loss=2.2181e-01, validation loss=2.4666e-01\n",
      "Epoch 52800: training loss=2.2178e-01, validation loss=2.4660e-01\n",
      "Epoch 52900: training loss=2.2175e-01, validation loss=2.4655e-01\n",
      "Epoch 53000: training loss=2.2171e-01, validation loss=2.4649e-01\n",
      "Epoch 53100: training loss=2.2168e-01, validation loss=2.4644e-01\n",
      "Epoch 53200: training loss=2.2165e-01, validation loss=2.4639e-01\n",
      "Epoch 53300: training loss=2.2162e-01, validation loss=2.4633e-01\n",
      "Epoch 53400: training loss=2.2158e-01, validation loss=2.4628e-01\n",
      "Epoch 53500: training loss=2.2155e-01, validation loss=2.4623e-01\n",
      "Epoch 53600: training loss=2.2152e-01, validation loss=2.4618e-01\n",
      "Epoch 53700: training loss=2.2149e-01, validation loss=2.4612e-01\n",
      "Epoch 53800: training loss=2.2146e-01, validation loss=2.4607e-01\n",
      "Epoch 53900: training loss=2.2143e-01, validation loss=2.4602e-01\n",
      "Epoch 54000: training loss=2.2140e-01, validation loss=2.4597e-01\n",
      "Epoch 54100: training loss=2.2136e-01, validation loss=2.4592e-01\n",
      "Epoch 54200: training loss=2.2133e-01, validation loss=2.4587e-01\n",
      "Epoch 54300: training loss=2.2130e-01, validation loss=2.4582e-01\n",
      "Epoch 54400: training loss=2.2127e-01, validation loss=2.4576e-01\n",
      "Epoch 54500: training loss=2.2124e-01, validation loss=2.4571e-01\n",
      "Epoch 54600: training loss=2.2121e-01, validation loss=2.4566e-01\n",
      "Epoch 54700: training loss=2.2118e-01, validation loss=2.4561e-01\n",
      "Epoch 54800: training loss=2.2115e-01, validation loss=2.4556e-01\n",
      "Epoch 54900: training loss=2.2112e-01, validation loss=2.4551e-01\n",
      "Epoch 55000: training loss=2.2109e-01, validation loss=2.4546e-01\n",
      "Epoch 55100: training loss=2.2107e-01, validation loss=2.4542e-01\n",
      "Epoch 55200: training loss=2.2104e-01, validation loss=2.4537e-01\n",
      "Epoch 55300: training loss=2.2101e-01, validation loss=2.4532e-01\n",
      "Epoch 55400: training loss=2.2098e-01, validation loss=2.4527e-01\n",
      "Epoch 55500: training loss=2.2095e-01, validation loss=2.4522e-01\n",
      "Epoch 55600: training loss=2.2092e-01, validation loss=2.4517e-01\n",
      "Epoch 55700: training loss=2.2089e-01, validation loss=2.4512e-01\n",
      "Epoch 55800: training loss=2.2086e-01, validation loss=2.4508e-01\n",
      "Epoch 55900: training loss=2.2084e-01, validation loss=2.4503e-01\n",
      "Epoch 56000: training loss=2.2081e-01, validation loss=2.4498e-01\n",
      "Epoch 56100: training loss=2.2078e-01, validation loss=2.4493e-01\n",
      "Epoch 56200: training loss=2.2075e-01, validation loss=2.4489e-01\n",
      "Epoch 56300: training loss=2.2072e-01, validation loss=2.4484e-01\n",
      "Epoch 56400: training loss=2.2070e-01, validation loss=2.4479e-01\n",
      "Epoch 56500: training loss=2.2067e-01, validation loss=2.4474e-01\n",
      "Epoch 56600: training loss=2.2064e-01, validation loss=2.4470e-01\n",
      "Epoch 56700: training loss=2.2061e-01, validation loss=2.4465e-01\n",
      "Epoch 56800: training loss=2.2059e-01, validation loss=2.4461e-01\n",
      "Epoch 56900: training loss=2.2056e-01, validation loss=2.4456e-01\n",
      "Epoch 57000: training loss=2.2053e-01, validation loss=2.4451e-01\n",
      "Epoch 57100: training loss=2.2051e-01, validation loss=2.4447e-01\n",
      "Epoch 57200: training loss=2.2048e-01, validation loss=2.4442e-01\n",
      "Epoch 57300: training loss=2.2045e-01, validation loss=2.4438e-01\n",
      "Epoch 57400: training loss=2.2043e-01, validation loss=2.4433e-01\n",
      "Epoch 57500: training loss=2.2040e-01, validation loss=2.4429e-01\n",
      "Epoch 57600: training loss=2.2037e-01, validation loss=2.4424e-01\n",
      "Epoch 57700: training loss=2.2035e-01, validation loss=2.4420e-01\n",
      "Epoch 57800: training loss=2.2032e-01, validation loss=2.4415e-01\n",
      "Epoch 57900: training loss=2.2029e-01, validation loss=2.4411e-01\n",
      "Epoch 58000: training loss=2.2027e-01, validation loss=2.4406e-01\n",
      "Epoch 58100: training loss=2.2024e-01, validation loss=2.4402e-01\n",
      "Epoch 58200: training loss=2.2022e-01, validation loss=2.4397e-01\n",
      "Epoch 58300: training loss=2.2019e-01, validation loss=2.4393e-01\n",
      "Epoch 58400: training loss=2.2017e-01, validation loss=2.4388e-01\n",
      "Epoch 58500: training loss=2.2014e-01, validation loss=2.4384e-01\n",
      "Epoch 58600: training loss=2.2011e-01, validation loss=2.4380e-01\n",
      "Epoch 58700: training loss=2.2009e-01, validation loss=2.4375e-01\n",
      "Epoch 58800: training loss=2.2006e-01, validation loss=2.4371e-01\n",
      "Epoch 58900: training loss=2.2004e-01, validation loss=2.4367e-01\n",
      "Epoch 59000: training loss=2.2001e-01, validation loss=2.4362e-01\n",
      "Epoch 59100: training loss=2.1999e-01, validation loss=2.4358e-01\n",
      "Epoch 59200: training loss=2.1996e-01, validation loss=2.4354e-01\n",
      "Epoch 59300: training loss=2.1994e-01, validation loss=2.4349e-01\n",
      "Epoch 59400: training loss=2.1991e-01, validation loss=2.4345e-01\n",
      "Epoch 59500: training loss=2.1989e-01, validation loss=2.4341e-01\n",
      "Epoch 59600: training loss=2.1986e-01, validation loss=2.4337e-01\n",
      "Epoch 59700: training loss=2.1984e-01, validation loss=2.4332e-01\n",
      "Epoch 59800: training loss=2.1982e-01, validation loss=2.4328e-01\n",
      "Epoch 59900: training loss=2.1979e-01, validation loss=2.4324e-01\n",
      "Epoch 60000: training loss=2.1977e-01, validation loss=2.4320e-01\n",
      "Epoch 60100: training loss=2.1974e-01, validation loss=2.4316e-01\n",
      "Epoch 60200: training loss=2.1972e-01, validation loss=2.4311e-01\n",
      "Epoch 60300: training loss=2.1969e-01, validation loss=2.4307e-01\n",
      "Epoch 60400: training loss=2.1967e-01, validation loss=2.4303e-01\n",
      "Epoch 60500: training loss=2.1965e-01, validation loss=2.4299e-01\n",
      "Epoch 60600: training loss=2.1962e-01, validation loss=2.4295e-01\n",
      "Epoch 60700: training loss=2.1960e-01, validation loss=2.4291e-01\n",
      "Epoch 60800: training loss=2.1957e-01, validation loss=2.4286e-01\n",
      "Epoch 60900: training loss=2.1955e-01, validation loss=2.4282e-01\n",
      "Epoch 61000: training loss=2.1953e-01, validation loss=2.4278e-01\n",
      "Epoch 61100: training loss=2.1950e-01, validation loss=2.4274e-01\n",
      "Epoch 61200: training loss=2.1948e-01, validation loss=2.4270e-01\n",
      "Epoch 61300: training loss=2.1946e-01, validation loss=2.4266e-01\n",
      "Epoch 61400: training loss=2.1943e-01, validation loss=2.4262e-01\n",
      "Epoch 61500: training loss=2.1941e-01, validation loss=2.4258e-01\n",
      "Epoch 61600: training loss=2.1939e-01, validation loss=2.4254e-01\n",
      "Epoch 61700: training loss=2.1936e-01, validation loss=2.4250e-01\n",
      "Epoch 61800: training loss=2.1934e-01, validation loss=2.4246e-01\n",
      "Epoch 61900: training loss=2.1932e-01, validation loss=2.4242e-01\n",
      "Epoch 62000: training loss=2.1929e-01, validation loss=2.4238e-01\n",
      "Epoch 62100: training loss=2.1927e-01, validation loss=2.4234e-01\n",
      "Epoch 62200: training loss=2.1925e-01, validation loss=2.4230e-01\n",
      "Epoch 62300: training loss=2.1922e-01, validation loss=2.4226e-01\n",
      "Epoch 62400: training loss=2.1920e-01, validation loss=2.4222e-01\n",
      "Epoch 62500: training loss=2.1918e-01, validation loss=2.4218e-01\n",
      "Epoch 62600: training loss=2.1915e-01, validation loss=2.4214e-01\n",
      "Epoch 62700: training loss=2.1913e-01, validation loss=2.4210e-01\n",
      "Epoch 62800: training loss=2.1911e-01, validation loss=2.4206e-01\n",
      "Epoch 62900: training loss=2.1909e-01, validation loss=2.4202e-01\n",
      "Epoch 63000: training loss=2.1906e-01, validation loss=2.4198e-01\n",
      "Epoch 63100: training loss=2.1904e-01, validation loss=2.4194e-01\n",
      "Epoch 63200: training loss=2.1902e-01, validation loss=2.4190e-01\n",
      "Epoch 63300: training loss=2.1900e-01, validation loss=2.4186e-01\n",
      "Epoch 63400: training loss=2.1897e-01, validation loss=2.4183e-01\n",
      "Epoch 63500: training loss=2.1895e-01, validation loss=2.4179e-01\n",
      "Epoch 63600: training loss=2.1893e-01, validation loss=2.4175e-01\n",
      "Epoch 63700: training loss=2.1891e-01, validation loss=2.4171e-01\n",
      "Epoch 63800: training loss=2.1888e-01, validation loss=2.4167e-01\n",
      "Epoch 63900: training loss=2.1886e-01, validation loss=2.4163e-01\n",
      "Epoch 64000: training loss=2.1884e-01, validation loss=2.4159e-01\n",
      "Epoch 64100: training loss=2.1882e-01, validation loss=2.4156e-01\n",
      "Epoch 64200: training loss=2.1879e-01, validation loss=2.4152e-01\n",
      "Epoch 64300: training loss=2.1877e-01, validation loss=2.4148e-01\n",
      "Epoch 64400: training loss=2.1875e-01, validation loss=2.4144e-01\n",
      "Epoch 64500: training loss=2.1873e-01, validation loss=2.4140e-01\n",
      "Epoch 64600: training loss=2.1871e-01, validation loss=2.4137e-01\n",
      "Epoch 64700: training loss=2.1868e-01, validation loss=2.4133e-01\n",
      "Epoch 64800: training loss=2.1866e-01, validation loss=2.4129e-01\n",
      "Epoch 64900: training loss=2.1864e-01, validation loss=2.4125e-01\n",
      "Epoch 65000: training loss=2.1862e-01, validation loss=2.4121e-01\n",
      "Epoch 65100: training loss=2.1860e-01, validation loss=2.4118e-01\n",
      "Epoch 65200: training loss=2.1858e-01, validation loss=2.4114e-01\n",
      "Epoch 65300: training loss=2.1855e-01, validation loss=2.4110e-01\n",
      "Epoch 65400: training loss=2.1853e-01, validation loss=2.4106e-01\n",
      "Epoch 65500: training loss=2.1851e-01, validation loss=2.4103e-01\n",
      "Epoch 65600: training loss=2.1849e-01, validation loss=2.4099e-01\n",
      "Epoch 65700: training loss=2.1847e-01, validation loss=2.4095e-01\n",
      "Epoch 65800: training loss=2.1844e-01, validation loss=2.4092e-01\n",
      "Epoch 65900: training loss=2.1842e-01, validation loss=2.4088e-01\n",
      "Epoch 66000: training loss=2.1840e-01, validation loss=2.4084e-01\n",
      "Epoch 66100: training loss=2.1838e-01, validation loss=2.4080e-01\n",
      "Epoch 66200: training loss=2.1836e-01, validation loss=2.4077e-01\n",
      "Epoch 66300: training loss=2.1834e-01, validation loss=2.4073e-01\n",
      "Epoch 66400: training loss=2.1832e-01, validation loss=2.4069e-01\n",
      "Epoch 66500: training loss=2.1829e-01, validation loss=2.4066e-01\n",
      "Epoch 66600: training loss=2.1827e-01, validation loss=2.4062e-01\n",
      "Epoch 66700: training loss=2.1825e-01, validation loss=2.4058e-01\n",
      "Epoch 66800: training loss=2.1823e-01, validation loss=2.4055e-01\n",
      "Epoch 66900: training loss=2.1821e-01, validation loss=2.4051e-01\n",
      "Epoch 67000: training loss=2.1819e-01, validation loss=2.4047e-01\n",
      "Epoch 67100: training loss=2.1817e-01, validation loss=2.4044e-01\n",
      "Epoch 67200: training loss=2.1814e-01, validation loss=2.4040e-01\n",
      "Epoch 67300: training loss=2.1812e-01, validation loss=2.4037e-01\n",
      "Epoch 67400: training loss=2.1810e-01, validation loss=2.4033e-01\n",
      "Epoch 67500: training loss=2.1808e-01, validation loss=2.4029e-01\n",
      "Epoch 67600: training loss=2.1806e-01, validation loss=2.4026e-01\n",
      "Epoch 67700: training loss=2.1804e-01, validation loss=2.4022e-01\n",
      "Epoch 67800: training loss=2.1802e-01, validation loss=2.4019e-01\n",
      "Epoch 67900: training loss=2.1800e-01, validation loss=2.4015e-01\n",
      "Epoch 68000: training loss=2.1798e-01, validation loss=2.4011e-01\n",
      "Epoch 68100: training loss=2.1795e-01, validation loss=2.4008e-01\n",
      "Epoch 68200: training loss=2.1793e-01, validation loss=2.4004e-01\n",
      "Epoch 68300: training loss=2.1791e-01, validation loss=2.4001e-01\n",
      "Epoch 68400: training loss=2.1789e-01, validation loss=2.3997e-01\n",
      "Epoch 68500: training loss=2.1787e-01, validation loss=2.3994e-01\n",
      "Epoch 68600: training loss=2.1785e-01, validation loss=2.3990e-01\n",
      "Epoch 68700: training loss=2.1783e-01, validation loss=2.3987e-01\n",
      "Epoch 68800: training loss=2.1781e-01, validation loss=2.3983e-01\n",
      "Epoch 68900: training loss=2.1779e-01, validation loss=2.3980e-01\n",
      "Epoch 69000: training loss=2.1777e-01, validation loss=2.3976e-01\n",
      "Epoch 69100: training loss=2.1774e-01, validation loss=2.3972e-01\n",
      "Epoch 69200: training loss=2.1772e-01, validation loss=2.3969e-01\n",
      "Epoch 69300: training loss=2.1770e-01, validation loss=2.3965e-01\n",
      "Epoch 69400: training loss=2.1768e-01, validation loss=2.3962e-01\n",
      "Epoch 69500: training loss=2.1766e-01, validation loss=2.3958e-01\n",
      "Epoch 69600: training loss=2.1764e-01, validation loss=2.3955e-01\n",
      "Epoch 69700: training loss=2.1762e-01, validation loss=2.3951e-01\n",
      "Epoch 69800: training loss=2.1760e-01, validation loss=2.3948e-01\n",
      "Epoch 69900: training loss=2.1758e-01, validation loss=2.3945e-01\n",
      "Epoch 70000: training loss=2.1756e-01, validation loss=2.3941e-01\n",
      "Epoch 70100: training loss=2.1754e-01, validation loss=2.3938e-01\n",
      "Epoch 70200: training loss=2.1752e-01, validation loss=2.3934e-01\n",
      "Epoch 70300: training loss=2.1750e-01, validation loss=2.3931e-01\n",
      "Epoch 70400: training loss=2.1748e-01, validation loss=2.3927e-01\n",
      "Epoch 70500: training loss=2.1745e-01, validation loss=2.3924e-01\n",
      "Epoch 70600: training loss=2.1743e-01, validation loss=2.3920e-01\n",
      "Epoch 70700: training loss=2.1741e-01, validation loss=2.3917e-01\n",
      "Epoch 70800: training loss=2.1739e-01, validation loss=2.3913e-01\n",
      "Epoch 70900: training loss=2.1737e-01, validation loss=2.3910e-01\n",
      "Epoch 71000: training loss=2.1735e-01, validation loss=2.3907e-01\n",
      "Epoch 71100: training loss=2.1733e-01, validation loss=2.3903e-01\n",
      "Epoch 71200: training loss=2.1731e-01, validation loss=2.3900e-01\n",
      "Epoch 71300: training loss=2.1729e-01, validation loss=2.3896e-01\n",
      "Epoch 71400: training loss=2.1727e-01, validation loss=2.3893e-01\n",
      "Epoch 71500: training loss=2.1725e-01, validation loss=2.3889e-01\n",
      "Epoch 71600: training loss=2.1723e-01, validation loss=2.3886e-01\n",
      "Epoch 71700: training loss=2.1721e-01, validation loss=2.3883e-01\n",
      "Epoch 71800: training loss=2.1719e-01, validation loss=2.3879e-01\n",
      "Epoch 71900: training loss=2.1717e-01, validation loss=2.3876e-01\n",
      "Epoch 72000: training loss=2.1715e-01, validation loss=2.3872e-01\n",
      "Epoch 72100: training loss=2.1713e-01, validation loss=2.3869e-01\n",
      "Epoch 72200: training loss=2.1711e-01, validation loss=2.3866e-01\n",
      "Epoch 72300: training loss=2.1708e-01, validation loss=2.3862e-01\n",
      "Epoch 72400: training loss=2.1706e-01, validation loss=2.3859e-01\n",
      "Epoch 72500: training loss=2.1704e-01, validation loss=2.3856e-01\n",
      "Epoch 72600: training loss=2.1702e-01, validation loss=2.3852e-01\n",
      "Epoch 72700: training loss=2.1700e-01, validation loss=2.3849e-01\n",
      "Epoch 72800: training loss=2.1698e-01, validation loss=2.3846e-01\n",
      "Epoch 72900: training loss=2.1696e-01, validation loss=2.3842e-01\n",
      "Epoch 73000: training loss=2.1694e-01, validation loss=2.3839e-01\n",
      "Epoch 73100: training loss=2.1692e-01, validation loss=2.3836e-01\n",
      "Epoch 73200: training loss=2.1690e-01, validation loss=2.3832e-01\n",
      "Epoch 73300: training loss=2.1688e-01, validation loss=2.3829e-01\n",
      "Epoch 73400: training loss=2.1686e-01, validation loss=2.3826e-01\n",
      "Epoch 73500: training loss=2.1684e-01, validation loss=2.3822e-01\n",
      "Epoch 73600: training loss=2.1682e-01, validation loss=2.3819e-01\n",
      "Epoch 73700: training loss=2.1680e-01, validation loss=2.3816e-01\n",
      "Epoch 73800: training loss=2.1678e-01, validation loss=2.3812e-01\n",
      "Epoch 73900: training loss=2.1676e-01, validation loss=2.3809e-01\n",
      "Epoch 74000: training loss=2.1674e-01, validation loss=2.3806e-01\n",
      "Epoch 74100: training loss=2.1672e-01, validation loss=2.3803e-01\n",
      "Epoch 74200: training loss=2.1670e-01, validation loss=2.3799e-01\n",
      "Epoch 74300: training loss=2.1668e-01, validation loss=2.3796e-01\n",
      "Epoch 74400: training loss=2.1666e-01, validation loss=2.3793e-01\n",
      "Epoch 74500: training loss=2.1664e-01, validation loss=2.3789e-01\n",
      "Epoch 74600: training loss=2.1662e-01, validation loss=2.3786e-01\n",
      "Epoch 74700: training loss=2.1660e-01, validation loss=2.3783e-01\n",
      "Epoch 74800: training loss=2.1658e-01, validation loss=2.3780e-01\n",
      "Epoch 74900: training loss=2.1656e-01, validation loss=2.3776e-01\n",
      "Epoch 75000: training loss=2.1654e-01, validation loss=2.3773e-01\n",
      "Epoch 75100: training loss=2.1652e-01, validation loss=2.3770e-01\n",
      "Epoch 75200: training loss=2.1650e-01, validation loss=2.3767e-01\n",
      "Epoch 75300: training loss=2.1648e-01, validation loss=2.3763e-01\n",
      "Epoch 75400: training loss=2.1646e-01, validation loss=2.3760e-01\n",
      "Epoch 75500: training loss=2.1644e-01, validation loss=2.3757e-01\n",
      "Epoch 75600: training loss=2.1642e-01, validation loss=2.3754e-01\n",
      "Epoch 75700: training loss=2.1640e-01, validation loss=2.3751e-01\n",
      "Epoch 75800: training loss=2.1638e-01, validation loss=2.3747e-01\n",
      "Epoch 75900: training loss=2.1636e-01, validation loss=2.3744e-01\n",
      "Epoch 76000: training loss=2.1634e-01, validation loss=2.3741e-01\n",
      "Epoch 76100: training loss=2.1632e-01, validation loss=2.3738e-01\n",
      "Epoch 76200: training loss=2.1630e-01, validation loss=2.3735e-01\n",
      "Epoch 76300: training loss=2.1628e-01, validation loss=2.3731e-01\n",
      "Epoch 76400: training loss=2.1626e-01, validation loss=2.3728e-01\n",
      "Epoch 76500: training loss=2.1624e-01, validation loss=2.3725e-01\n",
      "Epoch 76600: training loss=2.1622e-01, validation loss=2.3722e-01\n",
      "Epoch 76700: training loss=2.1620e-01, validation loss=2.3719e-01\n",
      "Epoch 76800: training loss=2.1618e-01, validation loss=2.3715e-01\n",
      "Epoch 76900: training loss=2.1616e-01, validation loss=2.3712e-01\n",
      "Epoch 77000: training loss=2.1614e-01, validation loss=2.3709e-01\n",
      "Epoch 77100: training loss=2.1612e-01, validation loss=2.3706e-01\n",
      "Epoch 77200: training loss=2.1610e-01, validation loss=2.3703e-01\n",
      "Epoch 77300: training loss=2.1608e-01, validation loss=2.3700e-01\n",
      "Epoch 77400: training loss=2.1606e-01, validation loss=2.3696e-01\n",
      "Epoch 77500: training loss=2.1604e-01, validation loss=2.3693e-01\n",
      "Epoch 77600: training loss=2.1602e-01, validation loss=2.3690e-01\n",
      "Epoch 77700: training loss=2.1600e-01, validation loss=2.3687e-01\n",
      "Epoch 77800: training loss=2.1598e-01, validation loss=2.3684e-01\n",
      "Epoch 77900: training loss=2.1596e-01, validation loss=2.3681e-01\n",
      "Epoch 78000: training loss=2.1594e-01, validation loss=2.3678e-01\n",
      "Epoch 78100: training loss=2.1592e-01, validation loss=2.3674e-01\n",
      "Epoch 78200: training loss=2.1590e-01, validation loss=2.3671e-01\n",
      "Epoch 78300: training loss=2.1588e-01, validation loss=2.3668e-01\n",
      "Epoch 78400: training loss=2.1586e-01, validation loss=2.3665e-01\n",
      "Epoch 78500: training loss=2.1584e-01, validation loss=2.3662e-01\n",
      "Epoch 78600: training loss=2.1582e-01, validation loss=2.3659e-01\n",
      "Epoch 78700: training loss=2.1580e-01, validation loss=2.3656e-01\n",
      "Epoch 78800: training loss=2.1578e-01, validation loss=2.3653e-01\n",
      "Epoch 78900: training loss=2.1576e-01, validation loss=2.3650e-01\n",
      "Epoch 79000: training loss=2.1574e-01, validation loss=2.3647e-01\n",
      "Epoch 79100: training loss=2.1572e-01, validation loss=2.3644e-01\n",
      "Epoch 79200: training loss=2.1570e-01, validation loss=2.3640e-01\n",
      "Epoch 79300: training loss=2.1568e-01, validation loss=2.3637e-01\n",
      "Epoch 79400: training loss=2.1566e-01, validation loss=2.3634e-01\n",
      "Epoch 79500: training loss=2.1564e-01, validation loss=2.3631e-01\n",
      "Epoch 79600: training loss=2.1562e-01, validation loss=2.3628e-01\n",
      "Epoch 79700: training loss=2.1560e-01, validation loss=2.3625e-01\n",
      "Epoch 79800: training loss=2.1558e-01, validation loss=2.3622e-01\n",
      "Epoch 79900: training loss=2.1556e-01, validation loss=2.3619e-01\n",
      "Epoch 80000: training loss=2.1554e-01, validation loss=2.3616e-01\n",
      "Epoch 80100: training loss=2.1552e-01, validation loss=2.3613e-01\n",
      "Epoch 80200: training loss=2.1550e-01, validation loss=2.3610e-01\n",
      "Epoch 80300: training loss=2.1549e-01, validation loss=2.3607e-01\n",
      "Epoch 80400: training loss=2.1547e-01, validation loss=2.3604e-01\n",
      "Epoch 80500: training loss=2.1545e-01, validation loss=2.3601e-01\n",
      "Epoch 80600: training loss=2.1543e-01, validation loss=2.3598e-01\n",
      "Epoch 80700: training loss=2.1541e-01, validation loss=2.3595e-01\n",
      "Epoch 80800: training loss=2.1539e-01, validation loss=2.3592e-01\n",
      "Epoch 80900: training loss=2.1537e-01, validation loss=2.3589e-01\n",
      "Epoch 81000: training loss=2.1535e-01, validation loss=2.3586e-01\n",
      "Epoch 81100: training loss=2.1533e-01, validation loss=2.3583e-01\n",
      "Epoch 81200: training loss=2.1531e-01, validation loss=2.3580e-01\n",
      "Epoch 81300: training loss=2.1529e-01, validation loss=2.3577e-01\n",
      "Epoch 81400: training loss=2.1527e-01, validation loss=2.3574e-01\n",
      "Epoch 81500: training loss=2.1525e-01, validation loss=2.3571e-01\n",
      "Epoch 81600: training loss=2.1523e-01, validation loss=2.3568e-01\n",
      "Epoch 81700: training loss=2.1521e-01, validation loss=2.3565e-01\n",
      "Epoch 81800: training loss=2.1519e-01, validation loss=2.3562e-01\n",
      "Epoch 81900: training loss=2.1517e-01, validation loss=2.3559e-01\n",
      "Epoch 82000: training loss=2.1515e-01, validation loss=2.3556e-01\n",
      "Epoch 82100: training loss=2.1513e-01, validation loss=2.3553e-01\n",
      "Epoch 82200: training loss=2.1511e-01, validation loss=2.3550e-01\n",
      "Epoch 82300: training loss=2.1510e-01, validation loss=2.3547e-01\n",
      "Epoch 82400: training loss=2.1508e-01, validation loss=2.3544e-01\n",
      "Epoch 82500: training loss=2.1506e-01, validation loss=2.3542e-01\n",
      "Epoch 82600: training loss=2.1504e-01, validation loss=2.3539e-01\n",
      "Epoch 82700: training loss=2.1502e-01, validation loss=2.3536e-01\n",
      "Epoch 82800: training loss=2.1500e-01, validation loss=2.3533e-01\n",
      "Epoch 82900: training loss=2.1498e-01, validation loss=2.3530e-01\n",
      "Epoch 83000: training loss=2.1496e-01, validation loss=2.3527e-01\n",
      "Epoch 83100: training loss=2.1494e-01, validation loss=2.3524e-01\n",
      "Epoch 83200: training loss=2.1492e-01, validation loss=2.3521e-01\n",
      "Epoch 83300: training loss=2.1490e-01, validation loss=2.3518e-01\n",
      "Epoch 83400: training loss=2.1488e-01, validation loss=2.3515e-01\n",
      "Epoch 83500: training loss=2.1486e-01, validation loss=2.3513e-01\n",
      "Epoch 83600: training loss=2.1484e-01, validation loss=2.3510e-01\n",
      "Epoch 83700: training loss=2.1482e-01, validation loss=2.3507e-01\n",
      "Epoch 83800: training loss=2.1481e-01, validation loss=2.3504e-01\n",
      "Epoch 83900: training loss=2.1479e-01, validation loss=2.3501e-01\n",
      "Epoch 84000: training loss=2.1477e-01, validation loss=2.3498e-01\n",
      "Epoch 84100: training loss=2.1475e-01, validation loss=2.3495e-01\n",
      "Epoch 84200: training loss=2.1473e-01, validation loss=2.3492e-01\n",
      "Epoch 84300: training loss=2.1471e-01, validation loss=2.3490e-01\n",
      "Epoch 84400: training loss=2.1469e-01, validation loss=2.3487e-01\n",
      "Epoch 84500: training loss=2.1467e-01, validation loss=2.3484e-01\n",
      "Epoch 84600: training loss=2.1465e-01, validation loss=2.3481e-01\n",
      "Epoch 84700: training loss=2.1463e-01, validation loss=2.3478e-01\n",
      "Epoch 84800: training loss=2.1461e-01, validation loss=2.3475e-01\n",
      "Epoch 84900: training loss=2.1460e-01, validation loss=2.3473e-01\n",
      "Epoch 85000: training loss=2.1458e-01, validation loss=2.3470e-01\n",
      "Epoch 85100: training loss=2.1456e-01, validation loss=2.3467e-01\n",
      "Epoch 85200: training loss=2.1454e-01, validation loss=2.3464e-01\n",
      "Epoch 85300: training loss=2.1452e-01, validation loss=2.3461e-01\n",
      "Epoch 85400: training loss=2.1450e-01, validation loss=2.3459e-01\n",
      "Epoch 85500: training loss=2.1448e-01, validation loss=2.3456e-01\n",
      "Epoch 85600: training loss=2.1446e-01, validation loss=2.3453e-01\n",
      "Epoch 85700: training loss=2.1444e-01, validation loss=2.3450e-01\n",
      "Epoch 85800: training loss=2.1442e-01, validation loss=2.3448e-01\n",
      "Epoch 85900: training loss=2.1441e-01, validation loss=2.3445e-01\n",
      "Epoch 86000: training loss=2.1439e-01, validation loss=2.3442e-01\n",
      "Epoch 86100: training loss=2.1437e-01, validation loss=2.3439e-01\n",
      "Epoch 86200: training loss=2.1435e-01, validation loss=2.3437e-01\n",
      "Epoch 86300: training loss=2.1433e-01, validation loss=2.3434e-01\n",
      "Epoch 86400: training loss=2.1431e-01, validation loss=2.3431e-01\n",
      "Epoch 86500: training loss=2.1429e-01, validation loss=2.3428e-01\n",
      "Epoch 86600: training loss=2.1427e-01, validation loss=2.3426e-01\n",
      "Epoch 86700: training loss=2.1425e-01, validation loss=2.3423e-01\n",
      "Epoch 86800: training loss=2.1424e-01, validation loss=2.3420e-01\n",
      "Epoch 86900: training loss=2.1422e-01, validation loss=2.3417e-01\n",
      "Epoch 87000: training loss=2.1420e-01, validation loss=2.3415e-01\n",
      "Epoch 87100: training loss=2.1418e-01, validation loss=2.3412e-01\n",
      "Epoch 87200: training loss=2.1416e-01, validation loss=2.3409e-01\n",
      "Epoch 87300: training loss=2.1414e-01, validation loss=2.3407e-01\n",
      "Epoch 87400: training loss=2.1412e-01, validation loss=2.3404e-01\n",
      "Epoch 87500: training loss=2.1411e-01, validation loss=2.3401e-01\n",
      "Epoch 87600: training loss=2.1409e-01, validation loss=2.3398e-01\n",
      "Epoch 87700: training loss=2.1407e-01, validation loss=2.3396e-01\n",
      "Epoch 87800: training loss=2.1405e-01, validation loss=2.3393e-01\n",
      "Epoch 87900: training loss=2.1403e-01, validation loss=2.3391e-01\n",
      "Epoch 88000: training loss=2.1401e-01, validation loss=2.3388e-01\n",
      "Epoch 88100: training loss=2.1399e-01, validation loss=2.3385e-01\n",
      "Epoch 88200: training loss=2.1398e-01, validation loss=2.3383e-01\n",
      "Epoch 88300: training loss=2.1396e-01, validation loss=2.3380e-01\n",
      "Epoch 88400: training loss=2.1394e-01, validation loss=2.3377e-01\n",
      "Epoch 88500: training loss=2.1392e-01, validation loss=2.3375e-01\n",
      "Epoch 88600: training loss=2.1390e-01, validation loss=2.3372e-01\n",
      "Epoch 88700: training loss=2.1388e-01, validation loss=2.3369e-01\n",
      "Epoch 88800: training loss=2.1387e-01, validation loss=2.3367e-01\n",
      "Epoch 88900: training loss=2.1385e-01, validation loss=2.3364e-01\n",
      "Epoch 89000: training loss=2.1383e-01, validation loss=2.3361e-01\n",
      "Epoch 89100: training loss=2.1381e-01, validation loss=2.3359e-01\n",
      "Epoch 89200: training loss=2.1379e-01, validation loss=2.3356e-01\n",
      "Epoch 89300: training loss=2.1377e-01, validation loss=2.3354e-01\n",
      "Epoch 89400: training loss=2.1376e-01, validation loss=2.3351e-01\n",
      "Epoch 89500: training loss=2.1374e-01, validation loss=2.3349e-01\n",
      "Epoch 89600: training loss=2.1372e-01, validation loss=2.3346e-01\n",
      "Epoch 89700: training loss=2.1370e-01, validation loss=2.3343e-01\n",
      "Epoch 89800: training loss=2.1368e-01, validation loss=2.3341e-01\n",
      "Epoch 89900: training loss=2.1366e-01, validation loss=2.3338e-01\n",
      "Epoch 90000: training loss=2.1365e-01, validation loss=2.3336e-01\n",
      "Epoch 90100: training loss=2.1363e-01, validation loss=2.3333e-01\n",
      "Epoch 90200: training loss=2.1361e-01, validation loss=2.3331e-01\n",
      "Epoch 90300: training loss=2.1359e-01, validation loss=2.3328e-01\n",
      "Epoch 90400: training loss=2.1357e-01, validation loss=2.3326e-01\n",
      "Epoch 90500: training loss=2.1356e-01, validation loss=2.3323e-01\n",
      "Epoch 90600: training loss=2.1354e-01, validation loss=2.3320e-01\n",
      "Epoch 90700: training loss=2.1352e-01, validation loss=2.3318e-01\n",
      "Epoch 90800: training loss=2.1350e-01, validation loss=2.3315e-01\n",
      "Epoch 90900: training loss=2.1348e-01, validation loss=2.3313e-01\n",
      "Epoch 91000: training loss=2.1347e-01, validation loss=2.3310e-01\n",
      "Epoch 91100: training loss=2.1345e-01, validation loss=2.3308e-01\n",
      "Epoch 91200: training loss=2.1343e-01, validation loss=2.3305e-01\n",
      "Epoch 91300: training loss=2.1341e-01, validation loss=2.3303e-01\n",
      "Epoch 91400: training loss=2.1339e-01, validation loss=2.3300e-01\n",
      "Epoch 91500: training loss=2.1338e-01, validation loss=2.3298e-01\n",
      "Epoch 91600: training loss=2.1336e-01, validation loss=2.3295e-01\n",
      "Epoch 91700: training loss=2.1334e-01, validation loss=2.3293e-01\n",
      "Epoch 91800: training loss=2.1332e-01, validation loss=2.3290e-01\n",
      "Epoch 91900: training loss=2.1331e-01, validation loss=2.3288e-01\n",
      "Epoch 92000: training loss=2.1329e-01, validation loss=2.3286e-01\n",
      "Epoch 92100: training loss=2.1327e-01, validation loss=2.3283e-01\n",
      "Epoch 92200: training loss=2.1325e-01, validation loss=2.3281e-01\n",
      "Epoch 92300: training loss=2.1323e-01, validation loss=2.3278e-01\n",
      "Epoch 92400: training loss=2.1322e-01, validation loss=2.3276e-01\n",
      "Epoch 92500: training loss=2.1320e-01, validation loss=2.3273e-01\n",
      "Epoch 92600: training loss=2.1318e-01, validation loss=2.3271e-01\n",
      "Epoch 92700: training loss=2.1316e-01, validation loss=2.3269e-01\n",
      "Epoch 92800: training loss=2.1315e-01, validation loss=2.3266e-01\n",
      "Epoch 92900: training loss=2.1313e-01, validation loss=2.3264e-01\n",
      "Epoch 93000: training loss=2.1311e-01, validation loss=2.3261e-01\n",
      "Epoch 93100: training loss=2.1310e-01, validation loss=2.3259e-01\n",
      "Epoch 93200: training loss=2.1308e-01, validation loss=2.3257e-01\n",
      "Epoch 93300: training loss=2.1306e-01, validation loss=2.3254e-01\n",
      "Epoch 93400: training loss=2.1304e-01, validation loss=2.3252e-01\n",
      "Epoch 93500: training loss=2.1303e-01, validation loss=2.3249e-01\n",
      "Epoch 93600: training loss=2.1301e-01, validation loss=2.3247e-01\n",
      "Epoch 93700: training loss=2.1299e-01, validation loss=2.3245e-01\n",
      "Epoch 93800: training loss=2.1297e-01, validation loss=2.3242e-01\n",
      "Epoch 93900: training loss=2.1296e-01, validation loss=2.3240e-01\n",
      "Epoch 94000: training loss=2.1294e-01, validation loss=2.3238e-01\n",
      "Epoch 94100: training loss=2.1292e-01, validation loss=2.3235e-01\n",
      "Epoch 94200: training loss=2.1291e-01, validation loss=2.3233e-01\n",
      "Epoch 94300: training loss=2.1289e-01, validation loss=2.3231e-01\n",
      "Epoch 94400: training loss=2.1287e-01, validation loss=2.3228e-01\n",
      "Epoch 94500: training loss=2.1285e-01, validation loss=2.3226e-01\n",
      "Epoch 94600: training loss=2.1284e-01, validation loss=2.3224e-01\n",
      "Epoch 94700: training loss=2.1282e-01, validation loss=2.3221e-01\n",
      "Epoch 94800: training loss=2.1280e-01, validation loss=2.3219e-01\n",
      "Epoch 94900: training loss=2.1279e-01, validation loss=2.3217e-01\n",
      "Epoch 95000: training loss=2.1277e-01, validation loss=2.3215e-01\n",
      "Epoch 95100: training loss=2.1275e-01, validation loss=2.3212e-01\n",
      "Epoch 95200: training loss=2.1273e-01, validation loss=2.3210e-01\n",
      "Epoch 95300: training loss=2.1272e-01, validation loss=2.3208e-01\n",
      "Epoch 95400: training loss=2.1270e-01, validation loss=2.3205e-01\n",
      "Epoch 95500: training loss=2.1268e-01, validation loss=2.3203e-01\n",
      "Epoch 95600: training loss=2.1267e-01, validation loss=2.3201e-01\n",
      "Epoch 95700: training loss=2.1265e-01, validation loss=2.3199e-01\n",
      "Epoch 95800: training loss=2.1263e-01, validation loss=2.3197e-01\n",
      "Epoch 95900: training loss=2.1262e-01, validation loss=2.3194e-01\n",
      "Epoch 96000: training loss=2.1260e-01, validation loss=2.3192e-01\n",
      "Epoch 96100: training loss=2.1258e-01, validation loss=2.3190e-01\n",
      "Epoch 96200: training loss=2.1257e-01, validation loss=2.3188e-01\n",
      "Epoch 96300: training loss=2.1255e-01, validation loss=2.3185e-01\n",
      "Epoch 96400: training loss=2.1253e-01, validation loss=2.3183e-01\n",
      "Epoch 96500: training loss=2.1252e-01, validation loss=2.3181e-01\n",
      "Epoch 96600: training loss=2.1250e-01, validation loss=2.3179e-01\n",
      "Epoch 96700: training loss=2.1249e-01, validation loss=2.3177e-01\n",
      "Epoch 96800: training loss=2.1247e-01, validation loss=2.3174e-01\n",
      "Epoch 96900: training loss=2.1245e-01, validation loss=2.3172e-01\n",
      "Epoch 97000: training loss=2.1244e-01, validation loss=2.3170e-01\n",
      "Epoch 97100: training loss=2.1242e-01, validation loss=2.3168e-01\n",
      "Epoch 97200: training loss=2.1240e-01, validation loss=2.3166e-01\n",
      "Epoch 97300: training loss=2.1239e-01, validation loss=2.3164e-01\n",
      "Epoch 97400: training loss=2.1237e-01, validation loss=2.3161e-01\n",
      "Epoch 97500: training loss=2.1235e-01, validation loss=2.3159e-01\n",
      "Epoch 97600: training loss=2.1234e-01, validation loss=2.3157e-01\n",
      "Epoch 97700: training loss=2.1232e-01, validation loss=2.3155e-01\n",
      "Epoch 97800: training loss=2.1231e-01, validation loss=2.3153e-01\n",
      "Epoch 97900: training loss=2.1229e-01, validation loss=2.3151e-01\n",
      "Epoch 98000: training loss=2.1227e-01, validation loss=2.3149e-01\n",
      "Epoch 98100: training loss=2.1226e-01, validation loss=2.3146e-01\n",
      "Epoch 98200: training loss=2.1224e-01, validation loss=2.3144e-01\n",
      "Epoch 98300: training loss=2.1223e-01, validation loss=2.3142e-01\n",
      "Epoch 98400: training loss=2.1221e-01, validation loss=2.3140e-01\n",
      "Epoch 98500: training loss=2.1219e-01, validation loss=2.3138e-01\n",
      "Epoch 98600: training loss=2.1218e-01, validation loss=2.3136e-01\n",
      "Epoch 98700: training loss=2.1216e-01, validation loss=2.3134e-01\n",
      "Epoch 98800: training loss=2.1215e-01, validation loss=2.3132e-01\n",
      "Epoch 98900: training loss=2.1213e-01, validation loss=2.3130e-01\n",
      "Epoch 99000: training loss=2.1211e-01, validation loss=2.3128e-01\n",
      "Epoch 99100: training loss=2.1210e-01, validation loss=2.3126e-01\n",
      "Epoch 99200: training loss=2.1208e-01, validation loss=2.3124e-01\n",
      "Epoch 99300: training loss=2.1207e-01, validation loss=2.3122e-01\n",
      "Epoch 99400: training loss=2.1205e-01, validation loss=2.3119e-01\n",
      "Epoch 99500: training loss=2.1204e-01, validation loss=2.3117e-01\n",
      "Epoch 99600: training loss=2.1202e-01, validation loss=2.3115e-01\n",
      "Epoch 99700: training loss=2.1201e-01, validation loss=2.3113e-01\n",
      "Epoch 99800: training loss=2.1199e-01, validation loss=2.3111e-01\n",
      "Epoch 99900: training loss=2.1197e-01, validation loss=2.3109e-01\n",
      "Epoch 99999: training loss=2.1196e-01, validation loss=2.3107e-01\n"
     ]
    }
   ],
   "source": [
    "# set the random seed, as SGD uses stochastic elements\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# instantiate the neural network from the class we defined above\n",
    "two_layer_neural_network = TwoLayerNeuralNetworkRegressor()\n",
    "\n",
    "# the learning_rate is the 'step' that the gradient descent will take to move in the\n",
    "# search space of the parameters\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# the number of 'epochs' is the number of iterations of gradient descent after\n",
    "# which the algorithm will stop and return the best solution found so far\n",
    "max_epochs = 100000\n",
    "\n",
    "# we select the type of loss we are going to use; in this case, it's going to be\n",
    "# Mean Squared Error (MSE), appropriate for most regression tasks\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# and now we can start the iterative optimization loop; we instantiate the optimizer\n",
    "# Stochastic Gradient Descent (SGD) to optimize the parameters of the network\n",
    "optimizer = torch.optim.SGD(params=two_layer_neural_network.parameters(), lr=learning_rate)\n",
    "\n",
    "# some data structures here to store the training and validation loss\n",
    "train_losses = np.zeros((max_epochs,))\n",
    "val_losses = np.zeros((max_epochs,))\n",
    "\n",
    "# and now we start the optimization process!\n",
    "print(\"Starting optimization...\")\n",
    "for epoch in range(0, max_epochs) :\n",
    "  # get the tensor containing the network predictions for the training set,\n",
    "  # using the current parameters (initially, they will all be random)\n",
    "  y_train_pred = two_layer_neural_network(X_train_tensor)\n",
    "\n",
    "  # compute loss\n",
    "  loss_train = mse_loss(y_train_pred, y_train_tensor.view(-1,1))\n",
    "  train_losses[epoch] = loss_train.item() # .item() here access the value stored inside the tensor that mse_loss returns\n",
    "\n",
    "  # now, here we need to also compute the loss on the validation set; we use the\n",
    "  # context torch.no_grad() to skip all computations on the tensor during the\n",
    "  # forward pass, we do not need to store that information for the validation set\n",
    "  # TODO: you have to code this\n",
    "  with torch.no_grad() :\n",
    "    # TODO: change the part here to actually compute and store the validation loss\n",
    "    y_val_pred = two_layer_neural_network(X_val_tensor)\n",
    "    loss_val = mse_loss(y_val_pred, y_val_tensor.view(-1,1)) \n",
    "    val_losses[epoch] = loss_val.item()\n",
    "\n",
    "  # printout on the first and last epoch, plus all epochs exactly divisible by 100\n",
    "  if epoch == 0 or epoch % 100 == 0 or epoch == max_epochs-1 :\n",
    "    print(\"Epoch %d: training loss=%.4e, validation loss=%.4e\" % (epoch, loss_train, loss_val)) # this printout is run only each 100 epochs\n",
    "\n",
    "  # set the cumulated gradients back to zero (to avoid cumulating from one epoch to the next)\n",
    "  optimizer.zero_grad()\n",
    "  # perform the backward operation to retropropagate the error and get the gradient\n",
    "  loss_train.backward()\n",
    "  # perform one step of the gradient descent, modifying the network parameters\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IbooNRmc8Hu"
   },
   "source": [
    "Now we can compute the values of some metrics, and check the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "90sZIW48c_ON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on training: 0.7880\n",
      "R2 on validation: 0.7790\n",
      "R2 on test: 0.7731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Performance on: training R2=0.7880; validation R2=0.7790; test R2=0.7731')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# we do not need to store information about the derivatives or compute gradients\n",
    "# to evaluate the network, so let's use the context no_grad()\n",
    "with torch.no_grad() :\n",
    "\n",
    "  # the r2_score function expects lists or numpy arrays as inputs, so we need\n",
    "  # to convert the tensors to the appropriate data type before using it\n",
    "  y_train_pred = two_layer_neural_network(X_train_tensor)\n",
    "  r2_train = r2_score(y_train_tensor.numpy(), y_train_pred.numpy())\n",
    "\n",
    "  y_val_pred = two_layer_neural_network(X_val_tensor)\n",
    "  r2_val = r2_score(y_val_tensor.numpy(), y_val_pred.numpy())\n",
    "\n",
    "  y_test_pred = two_layer_neural_network(X_test_tensor)\n",
    "  r2_test = r2_score(y_test_tensor.numpy(), y_test_pred.numpy())\n",
    "\n",
    "  print(\"R2 on training: %.4f\" % r2_train)\n",
    "  print(\"R2 on validation: %.4f\" % r2_val)\n",
    "  print(\"R2 on test: %.4f\" % r2_test)\n",
    "\n",
    "# this is used for the x-axis of the plot\n",
    "x_epochs = [i for i in range(0, max_epochs)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(x_epochs, train_losses, color='orange', label=\"Training loss\")\n",
    "ax.plot(x_epochs, val_losses, color='green', label=\"Validation loss\")\n",
    "ax.legend(loc='best')\n",
    "ax.set_title(\"Performance on: training R2=%.4f; validation R2=%.4f; test R2=%.4f\" % (r2_train, r2_val, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qucoCu8Ygme1"
   },
   "source": [
    "Try to play a little with the hyperparameters of the network to obtain better results. You should be able to obtain a good performance just by altering the hyperparameters of the optimizer. As a reference, here below are the basic results that you can obtain with Random Forest and XGBoost, without any hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jS0ScWLspmpJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest, R2_train=0.9597; R2_test=0.6924\n",
      "XGBoost, R2_train=0.9614; R2_test=0.7298\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "print(\"Random Forest, R2_train=%.4f; R2_test=%.4f\" % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n",
    "\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "print(\"XGBoost, R2_train=%.4f; R2_test=%.4f\" % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90OprmXvybWs"
   },
   "source": [
    "## Advanced monitoring and logging: Tensorboard\n",
    "\n",
    "Tensorboard is a utility visualization tool created by TensorFlow. It is so convenient and successful that even other libraries (like pytorch) added ways to use it. At its core, Tensorboard reads logs (text files) in a specific format, resulting from the training of a neural network, and provides a graphical output. Let's try to run Tensorboard in Google Colaboratory, using a pytorch utility to write the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YpQoJ8any_UV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb6I78eA0BLr"
   },
   "source": [
    "We are now going to create another instance of the `TwoLayerNeuralNetworkRegressor`, with a training loop similar to the one above; the only difference is that we now can call the `SummaryWriter` instance to write the log files that will be later read by Tensorboard to create the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6mTLaNo208kj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss=1.0510e+00, validation loss=1.0510e+00\n",
      "Epoch 100: training loss=5.8522e-01, validation loss=5.8522e-01\n",
      "Epoch 200: training loss=5.7408e-01, validation loss=5.7408e-01\n",
      "Epoch 300: training loss=5.6990e-01, validation loss=5.6990e-01\n",
      "Epoch 400: training loss=5.6167e-01, validation loss=5.6167e-01\n",
      "Epoch 500: training loss=5.4515e-01, validation loss=5.4515e-01\n",
      "Epoch 600: training loss=5.1688e-01, validation loss=5.1688e-01\n",
      "Epoch 700: training loss=4.8037e-01, validation loss=4.8037e-01\n",
      "Epoch 800: training loss=4.4459e-01, validation loss=4.4459e-01\n",
      "Epoch 900: training loss=4.1571e-01, validation loss=4.1571e-01\n",
      "Epoch 999: training loss=3.9494e-01, validation loss=3.9494e-01\n"
     ]
    }
   ],
   "source": [
    "# set the random seed\n",
    "random_seed = 424242\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# SummaryWriter is a class able to create logs in a format that Tensorboard can read\n",
    "# we can specify a directory where the logs will be stored\n",
    "folder_name = \"my_great_runs/run_%d\" % random_seed\n",
    "writer = SummaryWriter(folder_name)\n",
    "\n",
    "# create another instance of the class\n",
    "two_layer_neural_network_2 = TwoLayerNeuralNetworkRegressor()\n",
    "\n",
    "# set hyperparameters of the optimizer, and create another instance of the optimizer\n",
    "learning_rate = 1e-1\n",
    "max_epochs = 1000\n",
    "optimizer_2 = torch.optim.SGD(params=two_layer_neural_network_2.parameters(), lr=learning_rate)\n",
    "\n",
    "# another instance of the loss function, classic MSE\n",
    "mse_loss_2 = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(0, max_epochs) :\n",
    "  # get network predictions\n",
    "  y_train_pred = two_layer_neural_network_2(X_train_tensor)\n",
    "\n",
    "  # compute loss\n",
    "  loss_train = mse_loss_2(y_train_pred, y_train_tensor.view(-1,1))\n",
    "\n",
    "  # now, we write the information on the loss in the log files\n",
    "  writer.add_scalar(\"Loss/train\", loss_train, epoch)\n",
    "\n",
    "  with torch.no_grad() :\n",
    "    loss_val = loss_train # TODO: change this, add the real validation loss\n",
    "\n",
    "    # also store the information about the validation loss\n",
    "    writer.add_scalar(\"Loss/validation\", loss_val, epoch)\n",
    "\n",
    "  # printout on the first and last epoch, plus all epochs exactly divisible by 100\n",
    "  if epoch == 0 or epoch % 100 == 0 or epoch == max_epochs-1 :\n",
    "    print(\"Epoch %d: training loss=%.4e, validation loss=%.4e\" % (epoch, loss_train, loss_val)) # this printout is run only each 100 epochs\n",
    "\n",
    "  # set the cumulated gradients back to zero (to avoid cumulating from one epoch to the next)\n",
    "  optimizer_2.zero_grad()\n",
    "  # perform the backward operation to retropropagate the error and get the gradient\n",
    "  loss_train.backward()\n",
    "  # perform one step of the gradient descent, modifying the network parameters\n",
    "  optimizer_2.step()\n",
    "\n",
    "# 'flush' here is invoked just in case that the writer did not finish writing\n",
    "# everything to disk\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1uOUCHx2jhk"
   },
   "source": [
    "Now that the log files of the run have been written (you can check in the 'files' part here to the side <-, you should see the folder \"my_great_runs\"), we can read them using Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "z67Z6Xm-4kYR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 22484), started 0:05:15 ago. (Use '!kill 22484' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-58beb7b2e57f4302\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-58beb7b2e57f4302\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is necessary to run Tensorboard in Google Colaboratory, I am not sure whether\n",
    "# it would work on a notbook running on a local machine, we will discover it as we go ^_^;\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir my_great_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uudAN3si5Xhg"
   },
   "source": [
    "Going back to the cell with the training loop, you can try launching another training run **changing the random seed** (maybe also with different hyperparameters, and/or adding the proper code for the validation loss). The Tensorboard should update with the separate results of the second run, so that you can compare performance.\n",
    "\n",
    "Don't rerun the cell above, just click on the \"reload\" button in the top right corner of the Tensorflow graphical user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk3HIpZNkD9n"
   },
   "source": [
    "## (Really) Stochastic optimization\n",
    "\n",
    "We will now step into modern optimization of neural network parameters using (really) Stochastic Gradient Descent and its descendants. Now, as we have seen during class, the stochasticity comes from only evaluating the loss function on a subset of samples from the training set (called **batch**) before updating the weights with the information coming from the corresponding gradient.\n",
    "\n",
    "Now, luckily for us, we won't have to hand-code the functions and classes to generate and load batches given a data set. However, pytorch philosophy is to create new classes that inherit from others, so unsurprisingly we will have to create a new class for the Dataset. The `torch.data.utils.Dataset` is the class we will inherit from. Just as when we were inheriting from `torch.nn.Module` we had to specify the builder `__init__()` and the `forward()` method, for this inheritance we will have to specify `__len__()` that returns that total number of samples in the data set, and `__getitem__(index)` that returns a single sample in the dataset, in position `index`. You might notice that the names of both functions start with `__`, which in Python marks them as special methods. While they can both called directly, `object.__len__()` is also invoked when a `len(object)` is called, while `object.__getitem__(index)` is invoked when `object[index]` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ps3m8LRMlOko"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in the training set: 5897\n",
      "Samples in the validation set: 1475\n",
      "Samples in the test set: 820\n"
     ]
    }
   ],
   "source": [
    "# here is our new class, inheriting from Dataset; it is supposed to be a generic\n",
    "# class to load data sets obtained from openml\n",
    "class OpenMLDataset(torch.utils.data.Dataset) :\n",
    "\n",
    "  # we create our own builder, that has two compulsory arguments, X (tensor with feature values)\n",
    "  # and y (tensor with values of the target)\n",
    "  def __init__(self, X, y) :\n",
    "    # call __init__ of the parent class (in this case, the parent class has no __init__, but this is an implementation detail)\n",
    "    super(OpenMLDataset, self).__init__()\n",
    "    # we store the information internally in the object, as two attributes self.X and self.y\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  # the function that returns the total number of samples is easy\n",
    "  def __len__(self) :\n",
    "    return self.y.shape[0]\n",
    "\n",
    "  # the function that gets a single sample is also pretty straightforward\n",
    "  def __getitem__(self, index) :\n",
    "    return self.X[index], self.y[index]\n",
    "\n",
    "# fantastic! now we can instantiate our OpenMLDataset class, and use it to store\n",
    "# the training, validation and test set we created above\n",
    "train_data = OpenMLDataset(X_train_tensor, y_train_tensor)\n",
    "val_data = OpenMLDataset(X_val_tensor, y_val_tensor)\n",
    "test_data = OpenMLDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# let's give it a trial run\n",
    "print(\"Samples in the training set:\", len(train_data)) # here len() calls the __len__() method of the object\n",
    "print(\"Samples in the validation set:\", len(val_data))\n",
    "print(\"Samples in the test set:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftwbDkhBp0BO"
   },
   "source": [
    "Now, we are going to use `DataLoader` objects to manage the batches. `DataLoader` accepts several arguments in the builder, that are pretty self-explanatory.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4tGY8ottpxdN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 64 samples\n",
      "The load_train DataLoader gave me a batch of 9 samples\n"
     ]
    }
   ],
   "source": [
    "# the 'shuffle' option here is whether the samples should be returned in a random order;\n",
    "# it only makes sense for the training data, from which we will obtain the loss information\n",
    "load_train = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "# we don't really need a data loader for the validation or test set, as we will just use them in one go\n",
    "\n",
    "# but in practice, what happens when we ask the DataLoaders to give us some samples?\n",
    "# let's check!\n",
    "for X_batch, y_batch in load_train :\n",
    "  print(\"The load_train DataLoader gave me a batch of %d samples\" % X_batch.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuJpY6mir1tG"
   },
   "source": [
    "Calling a DataLoader like we did in the code cell above will return a series of batches of the specified size, until all the data set has been seen. As you might have noticed, the last batch in the set is smaller than the rest, as the amount of data we got was not exactly divisible by the batch size. Now, let's rewrite the optimization loop, this time using the DataLoaders we just created to update the gradient after each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gy0mylN8u84D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss (mean over batch)=8.2978e-01, validation loss=8.2978e-01\n",
      "Epoch 10: training loss (mean over batch)=3.7739e-01, validation loss=3.7739e-01\n",
      "Epoch 20: training loss (mean over batch)=2.5044e-01, validation loss=2.5044e-01\n",
      "Epoch 30: training loss (mean over batch)=2.3461e-01, validation loss=2.3461e-01\n",
      "Epoch 40: training loss (mean over batch)=2.2939e-01, validation loss=2.2939e-01\n",
      "Epoch 50: training loss (mean over batch)=2.2414e-01, validation loss=2.2414e-01\n",
      "Epoch 60: training loss (mean over batch)=2.2620e-01, validation loss=2.2620e-01\n",
      "Epoch 70: training loss (mean over batch)=2.2196e-01, validation loss=2.2196e-01\n",
      "Epoch 80: training loss (mean over batch)=2.1918e-01, validation loss=2.1918e-01\n",
      "Epoch 90: training loss (mean over batch)=2.1905e-01, validation loss=2.1905e-01\n",
      "Epoch 100: training loss (mean over batch)=2.1692e-01, validation loss=2.1692e-01\n",
      "Epoch 110: training loss (mean over batch)=2.1831e-01, validation loss=2.1831e-01\n",
      "Epoch 120: training loss (mean over batch)=2.1605e-01, validation loss=2.1605e-01\n",
      "Epoch 130: training loss (mean over batch)=2.1521e-01, validation loss=2.1521e-01\n",
      "Epoch 140: training loss (mean over batch)=2.1785e-01, validation loss=2.1785e-01\n",
      "Epoch 150: training loss (mean over batch)=2.1641e-01, validation loss=2.1641e-01\n",
      "Epoch 160: training loss (mean over batch)=2.1474e-01, validation loss=2.1474e-01\n",
      "Epoch 170: training loss (mean over batch)=2.1262e-01, validation loss=2.1262e-01\n",
      "Epoch 180: training loss (mean over batch)=2.1271e-01, validation loss=2.1271e-01\n",
      "Epoch 190: training loss (mean over batch)=2.1201e-01, validation loss=2.1201e-01\n",
      "Epoch 200: training loss (mean over batch)=2.1273e-01, validation loss=2.1273e-01\n",
      "Epoch 210: training loss (mean over batch)=2.1238e-01, validation loss=2.1238e-01\n",
      "Epoch 220: training loss (mean over batch)=2.1134e-01, validation loss=2.1134e-01\n",
      "Epoch 230: training loss (mean over batch)=2.1192e-01, validation loss=2.1192e-01\n",
      "Epoch 240: training loss (mean over batch)=2.1262e-01, validation loss=2.1262e-01\n",
      "Epoch 250: training loss (mean over batch)=2.1140e-01, validation loss=2.1140e-01\n",
      "Epoch 260: training loss (mean over batch)=2.1208e-01, validation loss=2.1208e-01\n",
      "Epoch 270: training loss (mean over batch)=2.1075e-01, validation loss=2.1075e-01\n",
      "Epoch 280: training loss (mean over batch)=2.1077e-01, validation loss=2.1077e-01\n",
      "Epoch 290: training loss (mean over batch)=2.1130e-01, validation loss=2.1130e-01\n",
      "Epoch 300: training loss (mean over batch)=2.0884e-01, validation loss=2.0884e-01\n",
      "Epoch 310: training loss (mean over batch)=2.0824e-01, validation loss=2.0824e-01\n",
      "Epoch 320: training loss (mean over batch)=2.0972e-01, validation loss=2.0972e-01\n",
      "Epoch 330: training loss (mean over batch)=2.0732e-01, validation loss=2.0732e-01\n",
      "Epoch 340: training loss (mean over batch)=2.0799e-01, validation loss=2.0799e-01\n",
      "Epoch 350: training loss (mean over batch)=2.0992e-01, validation loss=2.0992e-01\n",
      "Epoch 360: training loss (mean over batch)=2.0561e-01, validation loss=2.0561e-01\n",
      "Epoch 370: training loss (mean over batch)=2.0560e-01, validation loss=2.0560e-01\n",
      "Epoch 380: training loss (mean over batch)=2.0540e-01, validation loss=2.0540e-01\n",
      "Epoch 390: training loss (mean over batch)=2.0437e-01, validation loss=2.0437e-01\n",
      "Epoch 400: training loss (mean over batch)=2.0335e-01, validation loss=2.0335e-01\n",
      "Epoch 410: training loss (mean over batch)=2.0446e-01, validation loss=2.0446e-01\n",
      "Epoch 420: training loss (mean over batch)=2.0243e-01, validation loss=2.0243e-01\n",
      "Epoch 430: training loss (mean over batch)=2.0125e-01, validation loss=2.0125e-01\n",
      "Epoch 440: training loss (mean over batch)=2.0212e-01, validation loss=2.0212e-01\n",
      "Epoch 450: training loss (mean over batch)=2.0112e-01, validation loss=2.0112e-01\n",
      "Epoch 460: training loss (mean over batch)=2.0104e-01, validation loss=2.0104e-01\n",
      "Epoch 470: training loss (mean over batch)=2.0046e-01, validation loss=2.0046e-01\n",
      "Epoch 480: training loss (mean over batch)=2.0055e-01, validation loss=2.0055e-01\n",
      "Epoch 490: training loss (mean over batch)=2.0124e-01, validation loss=2.0124e-01\n",
      "Epoch 499: training loss (mean over batch)=2.0033e-01, validation loss=2.0033e-01\n"
     ]
    }
   ],
   "source": [
    "# set the random seed\n",
    "random_seed = 42424242\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# SummaryWriter is a class able to create logs in a format that Tensorboard can read\n",
    "# we can specify a directory where the logs will be stored\n",
    "folder_name = \"my_great_runs/run_%d\" % random_seed\n",
    "writer = SummaryWriter(folder_name)\n",
    "\n",
    "# create another instance of the class\n",
    "two_layer_neural_network_3 = TwoLayerNeuralNetworkRegressor()\n",
    "\n",
    "# set hyperparameters of the optimizer, and create another instance of the optimizer\n",
    "learning_rate = 5e-3 # let's use a lower learning rate, for smaller steps\n",
    "max_epochs = 500\n",
    "#optimizer_3 = torch.optim.SGD(params=two_layer_neural_network_3.parameters(), lr=learning_rate)\n",
    "optimizer_3 = torch.optim.Adam(params=two_layer_neural_network_3.parameters(), lr=learning_rate)\n",
    "\n",
    "# another instance of the loss function, classic MSE\n",
    "mse_loss_3 = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(0, max_epochs) :\n",
    "\n",
    "  # now, instead of just using the whole training set, we will work through batches\n",
    "  all_loss_batches = [] # let's keep track of loss of each batch\n",
    "  for X_batch, y_batch in load_train :\n",
    "\n",
    "    # get network predictions and compute loss\n",
    "    y_batch_pred = two_layer_neural_network_3(X_batch)\n",
    "    loss_batch = mse_loss_3(y_batch.view(-1,1), y_batch_pred)\n",
    "\n",
    "    # store information on the loss\n",
    "    all_loss_batches.append(loss_batch.item())\n",
    "\n",
    "    # update gradients and perform a step of the optimizer\n",
    "    optimizer_3.zero_grad() # set all information on gradients for parameters in the network to zero\n",
    "    loss_batch.backward() # use information from the loss_batch to backpropagate gradients on parameters\n",
    "    optimizer_3.step() # update weights in the opposite direction of the gradient\n",
    "\n",
    "  # now, we write the information on the average loss for all the batches\n",
    "  loss_train = np.mean(all_loss_batches)\n",
    "  writer.add_scalar(\"Loss/train\", loss_train, epoch)\n",
    "\n",
    "  with torch.no_grad() :\n",
    "    loss_val = loss_train # TODO: change this, add the real validation loss\n",
    "\n",
    "    # also store the information about the validation loss\n",
    "    writer.add_scalar(\"Loss/validation\", loss_val, epoch)\n",
    "\n",
    "  # printout on the first and last epoch, plus all epochs exactly divisible by 100\n",
    "  if epoch == 0 or epoch % 10 == 0 or epoch == max_epochs-1 :\n",
    "    print(\"Epoch %d: training loss (mean over batch)=%.4e, validation loss=%.4e\" % (epoch, loss_train, loss_val)) # this printout is run only each 100 epochs\n",
    "\n",
    "\n",
    "# 'flush' here is invoked just in case that the writer did not finish writing\n",
    "# everything to disk\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U6RfqLZ3Ase"
   },
   "source": [
    "Now, try replacing the optimizer with one of the most recent ones, for example `Adam`, cutting and pasting the code below into the appropriate place of the cell above. How does it look like? More or less effective than SGD?\n",
    "\n",
    "```\n",
    "optimizer_3 = torch.optim.Adam(params=two_layer_neural_network_3.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "As a final exercise, try using a scheduler to dynamically adjust the learning rate at each iteration. In pytorch, this is relatively easy: we first create an instance of the scheduler, and assign it to the instance of the optimizer:\n",
    "\n",
    "```\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_3, gamma=0.9)\n",
    "```\n",
    "\n",
    "And then, at the end of each iteration, we call a method from the scheduler instance to update the hyperparameters of the optimizer:\n",
    "\n",
    "```\n",
    "for epoch in range(0, max_epochs) :\n",
    "  ...\n",
    "  # end of the loop\n",
    "  scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACwwTnppqu8R"
   },
   "source": [
    "## Checkpointing\n",
    "Here below we have the same code for the network above, just rearranged in a different way. First, an initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q5_JKd-A4OHA"
   },
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "random_seed = 123456\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# create another instance of the class (we are going to overwrite the first networks and other variables we used in the first cells)\n",
    "two_layer_neural_network = TwoLayerNeuralNetworkRegressor()\n",
    "\n",
    "# set hyperparameters of the optimizer, and create another instance of the optimizer\n",
    "learning_rate = 1e-2\n",
    "max_epochs = 50\n",
    "optimizer = torch.optim.SGD(params=two_layer_neural_network.parameters(), lr=learning_rate)\n",
    "\n",
    "# instance of the loss function\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# writer\n",
    "folder_name = \"my_great_runs/run_%d\" % random_seed\n",
    "writer = SummaryWriter(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIwUQtvX5WJr"
   },
   "source": [
    "Then, here below, the code for the optimization. For the moment, do not modifiy the hyperparameters, try just running the cell with the optimization process below **multiple times**. What do you notice? What is happening here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "M3b7419H4xiX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss (mean over batches)=1.0024e+00, validation loss=1.0024e+00\n",
      "Epoch 10: training loss (mean over batches)=5.8744e-01, validation loss=5.8744e-01\n",
      "Epoch 20: training loss (mean over batches)=5.7450e-01, validation loss=5.7450e-01\n",
      "Epoch 30: training loss (mean over batches)=5.6773e-01, validation loss=5.6773e-01\n",
      "Epoch 40: training loss (mean over batches)=5.5584e-01, validation loss=5.5584e-01\n",
      "Epoch 49: training loss (mean over batches)=5.3749e-01, validation loss=5.3749e-01\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, max_epochs) :\n",
    "\n",
    "  # now, instead of just using the whole training set, we will work through batches\n",
    "  all_loss_batches = [] # let's keep track of loss of each batch\n",
    "  for X_batch, y_batch in load_train :\n",
    "\n",
    "    # get network predictions and compute loss\n",
    "    y_batch_pred = two_layer_neural_network(X_batch)\n",
    "    loss_batch = mse_loss(y_batch.view(-1,1), y_batch_pred)\n",
    "\n",
    "    # store information on the loss\n",
    "    all_loss_batches.append(loss_batch.item())\n",
    "\n",
    "    # update gradients and perform a step of the optimizer\n",
    "    optimizer.zero_grad() # set all information on gradients for parameters in the network to zero\n",
    "    loss_batch.backward() # use information from the loss_batch to backpropagate gradients on parameters\n",
    "    optimizer.step() # update weights in the opposite direction of the gradient\n",
    "\n",
    "  # now, we write the information on the average loss for all the batches\n",
    "  loss_train = np.mean(all_loss_batches)\n",
    "  writer.add_scalar(\"Loss/train\", loss_train, epoch)\n",
    "\n",
    "  with torch.no_grad() :\n",
    "    loss_val = loss_train # TODO: change this, add the real validation loss\n",
    "\n",
    "    # also store the information about the validation loss\n",
    "    writer.add_scalar(\"Loss/validation\", loss_val, epoch)\n",
    "\n",
    "  # printout on the first and last epoch, plus all epochs exactly divisible by 100\n",
    "  if epoch == 0 or epoch % 10 == 0 or epoch == max_epochs-1 :\n",
    "    print(\"Epoch %d: training loss (mean over batches)=%.4e, validation loss=%.4e\" % (epoch, loss_train, loss_val)) # this printout is run only each 100 epochs\n",
    "\n",
    "\n",
    "# 'flush' here is invoked just in case that the writer did not finish writing\n",
    "# everything to disk\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0T1ZPWX51AG"
   },
   "source": [
    "A neural network can **restart the training** from the values it obtained at the end of a previous optimization run. This is very useful, as it makes it possible to train a network through successive optimization runs, and even **saving intermediate results**, commonly called \"checkpoints\".\n",
    "\n",
    "Of course, pytorch offers some ways of saving and loading the weights of a network. You can even load the weights of a network inside a new network, but the two have to have the same architecture, same name of the modules, etc. In other words: they have to be two instances of the same torch.nn.Module class. Try the cell code below. It will save the weights of the neural network model we trained in the cell above, create a new instance, and then load the saved weights in the new instance. On Google Colaboratory, the file will be saved on your temporary storage (that you can access from the folder icon in the left-hand menu, 4th icon from the top).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wRc9FAnAcj9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: OrderedDict([('linear_1.weight', tensor([[ 0.0920,  0.2353,  0.2146,  0.5057, -0.5477,  0.8637,  0.7757, -0.4318],\n",
      "        [ 0.0485,  0.0430,  0.0479,  0.3075, -0.1971, -0.2305, -0.2235, -0.1599],\n",
      "        [-0.0275, -0.1768, -0.4325, -0.0444,  0.0832,  0.1365,  0.1698,  0.2010],\n",
      "        [-0.3471, -0.1663, -1.0871, -0.0277,  0.0863, -0.1230, -0.2972, -0.0583],\n",
      "        [ 0.1687, -0.0776,  0.8765,  0.0517, -0.5978, -0.0572, -0.1697, -0.0186]])), ('linear_1.bias', tensor([-0.4968,  0.0379, -0.1541,  0.1727,  0.1682])), ('linear_2.weight', tensor([[-1.0517, -0.0700,  0.2810,  1.2463, -0.9923]])), ('linear_2.bias', tensor([0.1999]))])\n",
      "Untrained weights: OrderedDict([('linear_1.weight', tensor([[ 0.1386, -0.0908,  0.1702, -0.0786,  0.0636, -0.3361, -0.1538, -0.0023],\n",
      "        [ 0.0043,  0.2562,  0.2843,  0.0222,  0.2067, -0.2200, -0.2733, -0.0412],\n",
      "        [-0.1633, -0.1426,  0.0449, -0.0559,  0.1476, -0.2340, -0.3050,  0.1927],\n",
      "        [ 0.3072,  0.3124,  0.1129, -0.2537, -0.0324,  0.3353,  0.0379, -0.1646],\n",
      "        [-0.0331, -0.0431,  0.2951, -0.3190, -0.1473,  0.1086,  0.1830, -0.0151]])), ('linear_1.bias', tensor([ 0.1256, -0.2687,  0.1708, -0.1493,  0.3331])), ('linear_2.weight', tensor([[ 0.1867, -0.1155, -0.0759,  0.2504,  0.4437]])), ('linear_2.bias', tensor([-0.3235]))])\n",
      "Weights loaded from file: OrderedDict([('linear_1.weight', tensor([[ 0.0920,  0.2353,  0.2146,  0.5057, -0.5477,  0.8637,  0.7757, -0.4318],\n",
      "        [ 0.0485,  0.0430,  0.0479,  0.3075, -0.1971, -0.2305, -0.2235, -0.1599],\n",
      "        [-0.0275, -0.1768, -0.4325, -0.0444,  0.0832,  0.1365,  0.1698,  0.2010],\n",
      "        [-0.3471, -0.1663, -1.0871, -0.0277,  0.0863, -0.1230, -0.2972, -0.0583],\n",
      "        [ 0.1687, -0.0776,  0.8765,  0.0517, -0.5978, -0.0572, -0.1697, -0.0186]])), ('linear_1.bias', tensor([-0.4968,  0.0379, -0.1541,  0.1727,  0.1682])), ('linear_2.weight', tensor([[-1.0517, -0.0700,  0.2810,  1.2463, -0.9923]])), ('linear_2.bias', tensor([0.1999]))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alberto\\AppData\\Local\\Temp\\ipykernel_4852\\1164597682.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_from_file = torch.load(\"super_awesome_regressor\")\n"
     ]
    }
   ],
   "source": [
    "state_to_be_saved = {'model_state' : two_layer_neural_network.state_dict()} # .state_dict() is a dictionary with all the values of the parameters\n",
    "print(\"Trained weights:\", two_layer_neural_network.state_dict())\n",
    "\n",
    "# we can save it to a file using a pytorch function\n",
    "torch.save(state_to_be_saved, \"super_awesome_regressor\")\n",
    "\n",
    "# now, we can instantiate a new network, from the same class\n",
    "totally_new_network = TwoLayerNeuralNetworkRegressor()\n",
    "# and we can check that it does have different parameters, at this point randomly initialized\n",
    "print(\"Untrained weights:\", totally_new_network.state_dict())\n",
    "# but by loading the old parameter into the new network, we will find the initial values again\n",
    "state_from_file = torch.load(\"super_awesome_regressor\")\n",
    "totally_new_network.load_state_dict(state_from_file[\"model_state\"])\n",
    "print(\"Weights loaded from file:\", totally_new_network.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bUx3HNHdAvm"
   },
   "source": [
    "If we are using one of the optimizers with internal parameters (for example, keeping track of the history of gradient values and so on), we can also save the current state of the optimizer, to restart the optimization exactly from where we left it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NMucDmr0fr60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer parameters: {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3]}]}\n",
      "Scheduler parameters: {'gamma': 0.9, 'base_lrs': [0.01], 'last_epoch': 0, 'verbose': False, '_step_count': 1, '_get_lr_called_within_step': False, '_last_lr': [0.01]}\n"
     ]
    }
   ],
   "source": [
    "# in general, we can add further information as other keys in the dictionary\n",
    "state_to_be_saved[\"optimizer_state\"] = optimizer.state_dict()\n",
    "print(\"Optimizer parameters:\", optimizer.state_dict())\n",
    "\n",
    "# the same is true for schedulers, and probably other utilities\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "print(\"Scheduler parameters:\", scheduler.state_dict())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
