{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intermediate Deep Learning concepts\n",
        "\n",
        "In this notebook, we are going to see some slightly more advanced concepts of Deep Learning, and how they are practically implemented in pytorch.\n",
        "\n",
        "Let's start again by importing all libraries we will need in the following."
      ],
      "metadata": {
        "id": "2eFWpa3nS9zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openml --quiet\n",
        "\n",
        "import openml"
      ],
      "metadata": {
        "id": "48wMZK0zXjCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEwtBrewS886"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"pytorch version:\",torch.__version__)\n",
        "print(\"Is GPU computation available?\", torch.cuda.is_available())\n",
        "\n",
        "#Â this is only relevant for people with a MacBook M1 and pytorch >= 2.0\n",
        "print(\"(Mac M1 only) Is GPU computation available?\", torch.backends.mps.is_available())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitoring the training process with training loss and validation loss\n",
        "\n",
        "As we have seen in the previous example, it can be difficult to exactly assess when to stop the training process for a DL model; or, in other words, how to set the proper number of iterations in the gradient descent algorithm. In practice, we would like to know exactly when our model starts overfitting the training data, and stop right before overfitting becomes a problem. However, it is impossible to assess overfitting just checking the performance on the training data: two different models might show the same performance on training data; but one could be able to generalize perfectly to unseen data, and the other could be completely overfitted and performing poorly on unseen data.\n",
        "\n",
        "One way of tackling this problem is to further divide our training data into two parts: one training set proper and one *validation set*, that will not be used for training, but just to check performance during the training process, and identify possible signs of overfitting.\n",
        "\n",
        "Let's load and preprocess the data, as usual."
      ],
      "metadata": {
        "id": "Ovnn5qwdUNpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = openml.datasets.get_dataset(189)\n",
        "\n",
        "df, *_ = dataset.get_data()\n",
        "print(df)\n",
        "\n",
        "# as you noticed, some of the columns contain strings instead of numbers; we call these\n",
        "# \"categorical\" columns or features. We need to change that, as most ML algorithms\n",
        "# only process numbers. Don't worry too much about this part, it's just replacing strings\n",
        "# with numbers\n",
        "categorical_columns = df.select_dtypes(include=['category', 'object', 'string']).columns\n",
        "print(\"I found a total of %d categorical features.\" % len(categorical_columns))\n",
        "for c in categorical_columns :\n",
        "  df[c].replace({category : index for index, category in enumerate(df[c].astype('category').cat.categories)}, inplace=True)\n",
        "\n",
        "# also, remove all rows that contain invalid numerical values (for example, missing values)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# name of the target column\n",
        "target_feature = dataset.default_target_attribute\n",
        "other_features = [c for c in df.columns if c != target_feature]\n",
        "\n",
        "# get just the data without the column headers, as numerical matrices\n",
        "X = df[other_features].values\n",
        "y = df[target_feature].values\n",
        "\n",
        "print(\"X=\", X)\n",
        "print(\"y=\", y)"
      ],
      "metadata": {
        "id": "HCNuih3EVssJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, however, we will split the data into three parts: a training set, a validation set, and a test set."
      ],
      "metadata": {
        "id": "vyuolakxXy-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# we use 10% of the data for the test, the rest for training and validation\n",
        "X_training, X_test, y_training, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=42)\n",
        "# we use 20% of the remaining data for validation, the rest for the training\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Whole dataset: %d samples; training set: %d samples; validation set: %d samples; test set: %d samples\"\n",
        "% (X.shape[0], X_train.shape[0], X_val.shape[0], X_test.shape[0]))"
      ],
      "metadata": {
        "id": "1c93YEsrZBwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the data, as usual by learning the normalization on the training set and applying it to the other two sets."
      ],
      "metadata": {
        "id": "oqCTeBh9aNUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# StandardScaler is an object that is able to learn and apply a normalization\n",
        "# that reduces the values of a feature to zero mean and unitary variance\n",
        "# so that most of the data values of a feature will fall into the interval (-1,1)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_X = StandardScaler() # we need a separate instance of the StandardScaler object for X and y\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "scaler_X.fit(X_train)\n",
        "scaler_y.fit(y_train.reshape(-1,1))\n",
        "\n",
        "X_train_scaled = scaler_X.transform(X_train)\n",
        "X_val_scaled = scaler_X.transform(X_val)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "# don't worry too much about all this reshaping going on here, these function like\n",
        "# to have their inputs in a particular way, but the functions later like another\n",
        "# type of input, so we are forced to reshape and reshape again\n",
        "y_train_scaled = scaler_y.transform(y_train.reshape(-1,1)).reshape(-1,)\n",
        "y_val_scaled = scaler_y.transform(y_val.reshape(-1,1)).reshape(-1,)\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1,1)).reshape(-1,)\n",
        "\n",
        "# convert all the arrays to pytorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float)\n",
        "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
        "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float)\n",
        "\n",
        "print(\"y_train_tensor has shape:\", y_train_tensor.shape)\n",
        "print(\"y_val_tensor has shape:\", y_val_tensor.shape)\n",
        "print(\"y_test_tensor has shape:\", y_test_tensor.shape)"
      ],
      "metadata": {
        "id": "Uk9XvF4HaWpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's go back to our first neural network for regression."
      ],
      "metadata": {
        "id": "_kqj25aSaobQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we create a new class, that inherits from torch.nn.Module, the basic pytorch module\n",
        "class TwoLayerNeuralNetworkRegressor(torch.nn.Module) :\n",
        "\n",
        "  # first, the builder __init__ that is called every time the class is instantiated\n",
        "  # note that we added an additional argument, input_features_size, that we can modify\n",
        "  # to adapt this to problems with a different number of features\n",
        "  def __init__(self, input_features_size=8) :\n",
        "    super(TwoLayerNeuralNetworkRegressor, self).__init__() # call the __init__ of the parent class Module\n",
        "    self.linear_1 = torch.nn.Linear(input_features_size, 5) # linear layer, input_features_size inputs and 5 outputs\n",
        "    self.activation_function_1 = torch.nn.Sigmoid() # activation function\n",
        "    self.linear_2 = torch.nn.Linear(5, 1) # another linear layer, 5 inputs, 1 output (that will be intepreted as the prediction)\n",
        "\n",
        "  # the method 'forward' describes what happens during a forward pass\n",
        "  def forward(self, x) :\n",
        "    z_1 = self.linear_1(x) # pass inputs through first linear module\n",
        "    z_2 = self.activation_function_1(z_1) # pass output of linear layer through activation function\n",
        "    y_hat = self.linear_2(z_2) # pass output of activation function through last linear TwoLayerNeuralNetwork\n",
        "\n",
        "    # return the tensor in output of the last module as a prediction\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "_bcwT8DQazG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, your task will be to record the MSE on the validation set during the training process, and plot it at the end. What can you observe?"
      ],
      "metadata": {
        "id": "m3KLiE3ja9M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed, as SGD uses stochastic elements\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# instantiate the neural network from the class we defined above\n",
        "two_layer_neural_network = TwoLayerNeuralNetworkRegressor()\n",
        "\n",
        "# the learning_rate is the 'step' that the gradient descent will take to move in the\n",
        "# search space of the parameters\n",
        "learning_rate = 1e-1\n",
        "\n",
        "# the number of 'epochs' is the number of iterations of gradient descent after\n",
        "# which the algorithm will stop and return the best solution found so far\n",
        "max_epochs = 1000\n",
        "\n",
        "# we select the type of loss we are going to use; in this case, it's going to be\n",
        "# Mean Squared Error (MSE), appropriate for most regression tasks\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "# and now we can start the iterative optimization loop; we instantiate the optimizer\n",
        "# Stochastic Gradient Descent (SGD) to optimize the parameters of the network\n",
        "optimizer = torch.optim.SGD(params=two_layer_neural_network.parameters(), lr=learning_rate)\n",
        "\n",
        "# some data structures here to store the training and validation loss\n",
        "train_losses = np.zeros((max_epochs,))\n",
        "val_losses = np.zeros((max_epochs,))\n",
        "\n",
        "# and now we start the optimization process!\n",
        "print(\"Starting optimization...\")\n",
        "for epoch in range(0, max_epochs) :\n",
        "  # get the tensor containing the network predictions for the training set,\n",
        "  # using the current parameters (initially, they will all be random)\n",
        "  y_train_pred = two_layer_neural_network(X_train_tensor)\n",
        "\n",
        "  # compute loss\n",
        "  loss_train = mse_loss(y_train_pred, y_train_tensor.view(-1,1))\n",
        "  train_losses[epoch] = loss_train.item() # .item() here access the value stored inside the tensor that mse_loss returns\n",
        "\n",
        "  # now, here we need to also compute the loss on the validation set; we use the\n",
        "  # context torch.no_grad() to skip all computations on the tensor during the\n",
        "  # forward pass, we do not need to store that information for the validation set\n",
        "  # TODO: you have to code this\n",
        "  with torch.no_grad() :\n",
        "    # TODO: change the part here to actually compute and store the validation loss\n",
        "    loss_val = loss_train # TODO: change this\n",
        "    val_losses[epoch] = loss_val.item()\n",
        "\n",
        "  # printout on the first and last epoch, plus all epochs exactly divisible by 100\n",
        "  if epoch == 0 or epoch % 100 == 0 or epoch == max_epochs-1 :\n",
        "    print(\"Epoch %d: training loss=%.4e, validation loss=%.4e\" % (epoch, loss_train, loss_val)) # this printout is run only each 100 epochs\n",
        "\n",
        "  # set the cumulated gradients back to zero (to avoid cumulating from one epoch to the next)\n",
        "  optimizer.zero_grad()\n",
        "  # perform the backward operation to retropropagate the error and get the gradient\n",
        "  loss_train.backward()\n",
        "  # perform one step of the gradient descent, modifying the network parameters\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "HHdCgklFbNw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compute the values of some metrics, and check the performance of the network."
      ],
      "metadata": {
        "id": "6IbooNRmc8Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# we do not need to store information about the derivatives or compute gradients\n",
        "# to evaluate the network, so let's use the context no_grad()\n",
        "with torch.no_grad() :\n",
        "\n",
        "  # the r2_score function expects lists or numpy arrays as inputs, so we need\n",
        "  # to convert the tensors to the appropriate data type before using it\n",
        "  y_train_pred = two_layer_neural_network(X_train_tensor)\n",
        "  r2_train = r2_score(y_train_tensor.numpy(), y_train_pred.numpy())\n",
        "\n",
        "  y_val_pred = two_layer_neural_network(X_val_tensor)\n",
        "  r2_val = r2_score(y_val_tensor.numpy(), y_val_pred.numpy())\n",
        "\n",
        "  y_test_pred = two_layer_neural_network(X_test_tensor)\n",
        "  r2_test = r2_score(y_test_tensor.numpy(), y_test_pred.numpy())\n",
        "\n",
        "  print(\"R2 on training: %.4f\" % r2_train)\n",
        "  print(\"R2 on validation: %.4f\" % r2_val)\n",
        "  print(\"R2 on test: %.4f\" % r2_test)\n",
        "\n",
        "# this is used for the x-axis of the plot\n",
        "x_epochs = [i for i in range(0, max_epochs)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "ax.plot(x_epochs, train_losses, color='orange', label=\"Training loss\")\n",
        "ax.plot(x_epochs, val_losses, color='green', label=\"Validation loss\")\n",
        "ax.legend(loc='best')\n",
        "ax.set_title(\"Performance on: training R2=%.4f; validation R2=%.4f; test R2=%.4f\" % (r2_train, r2_val, r2_test))"
      ],
      "metadata": {
        "id": "90sZIW48c_ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to play a little with the hyperparameters of the network to obtain better results. You should be able to obtain a good performance just by altering the hyperparameters of the optimizer. As a reference, here below are the basic results that you can obtain with Random Forest and XGBoost, without any hyperparameter optimization."
      ],
      "metadata": {
        "id": "qucoCu8Ygme1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_train_pred = rf.predict(X_train)\n",
        "y_test_pred = rf.predict(X_test)\n",
        "print(\"Random Forest, R2_train=%.4f; R2_test=%.4f\" % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n",
        "\n",
        "xgb = XGBRegressor(random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_train_pred = xgb.predict(X_train)\n",
        "y_test_pred = xgb.predict(X_test)\n",
        "print(\"XGBoost, R2_train=%.4f; R2_test=%.4f\" % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))"
      ],
      "metadata": {
        "id": "jS0ScWLspmpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced monitoring and logging: Tensorboard\n",
        "\n",
        "Tensorboard is a utility visualization tool created by TensorFlow. It is so convenient and successful that even other libraries (like pytorch) added ways to use it. At its core, Tensorboard reads logs (text files) in a specific format, resulting from the training of a neural network, and provides a graphical output. Let's try to run Tensorboard in Google Colaboratory, using a pytorch utility to write the logs."
      ],
      "metadata": {
        "id": "90OprmXvybWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "YpQoJ8any_UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to create another instance of the `TwoLayerNeuralNetworkRegressor`, with a training loop similar to the one above; the only difference is that we now can call the `SummaryWriter` instance to write the log files that will be later read by Tensorboard to create the visualizations."
      ],
      "metadata": {
        "id": "Kb6I78eA0BLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# SummaryWriter is a class able to create logs in a format that Tensorboard can read\n",
        "# we can specify a directory where the logs will be stored\n",
        "folder_name = \"my_great_runs/run_%d\" % random_seed\n",
        "writer = SummaryWriter(folder_name)\n",
        "\n",
        "# create another instance of the class\n",
        "two_layer_neural_network_2 = TwoLayerNeuralNetworkRegressor()\n",
        "\n",
        "# set hyperparameters of the optimizer, and create another instance of the optimizer\n",
        "learning_rate = 1e-1\n",
        "max_epochs = 1000\n",
        "optimizer_2 = torch.optim.SGD(params=two_layer_neural_network_2.parameters(), lr=learning_rate)\n",
        "\n",
        "# another instance of the loss function, classic MSE\n",
        "mse_loss_2 = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(0, max_epochs) :\n",
        "  # get network predictions\n",
        "  y_train_pred = two_layer_neural_network_2(X_train_tensor)\n",
        "\n",
        "  # compute loss\n",
        "  loss_train = mse_loss_2(y_train_pred, y_train_tensor.view(-1,1))\n",
        "\n",
        "  # now, we write the information on the loss in the log files\n",
        "  writer.add_scalar(\"Loss/train\", loss_train, epoch)\n",
        "\n",
        "  with torch.no_grad() :\n",
        "    loss_val = loss_train # TODO: change this, add the real validation loss\n",
        "\n",
        "    # also store the information about the validation loss\n",
        "    writer.add_scalar(\"Loss/validation\", loss_val, epoch)\n",
        "\n",
        "  # printout on the first and last epoch, plus all epochs exactly divisible by 100\n",
        "  if epoch == 0 or epoch % 100 == 0 or epoch == max_epochs-1 :\n",
        "    print(\"Epoch %d: training loss=%.4e, validation loss=%.4e\" % (epoch, loss_train, loss_val)) # this printout is run only each 100 epochs\n",
        "\n",
        "  # set the cumulated gradients back to zero (to avoid cumulating from one epoch to the next)\n",
        "  optimizer_2.zero_grad()\n",
        "  # perform the backward operation to retropropagate the error and get the gradient\n",
        "  loss_train.backward()\n",
        "  # perform one step of the gradient descent, modifying the network parameters\n",
        "  optimizer_2.step()\n",
        "\n",
        "# 'flush' here is invoked just in case that the writer did not finish writing\n",
        "# everything to disk\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "6mTLaNo208kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the log files of the run have been written (you can check in the 'files' part here to the side <-, you should see the folder \"my_great_runs\"), we can read them using Tensorboard"
      ],
      "metadata": {
        "id": "s1uOUCHx2jhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is necessary to run Tensorboard in Google Colaboratory, I am not sure whether\n",
        "# it would work on a notbook running on a local machine, we will discover it as we go ^_^;\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir my_great_runs"
      ],
      "metadata": {
        "id": "z67Z6Xm-4kYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going back to the cell with the training loop, you can try launching another training run **changing the random seed** (maybe also with different hyperparameters, and/or adding the proper code for the validation loss). The Tensorboard should update with the separate results of the second run, so that you can compare performance.\n",
        "\n",
        "Don't rerun the cell above, just click on the \"reload\" button in the top right corner of the Tensorflow graphical user interface."
      ],
      "metadata": {
        "id": "uudAN3si5Xhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Really) Stochastic optimization\n",
        "\n",
        "We will now step into modern optimization of neural network parameters using (really) Stochastic Gradient Descent and its descendants. Now, as we have seen during class, the stochasticity comes from only evaluating the loss function on a subset of samples from the training set (called **batch**) before updating the weights with the information coming from the corresponding gradient.\n",
        "\n",
        "Now, luckily for us, we won't have to hand-code the functions and classes to generate and load batches given a data set. However, pytorch philosophy is to create new classes that inherit from others, so unsurprisingly we will have to create a new class for the Dataset. The `torch.data.utils.Dataset` is the class we will inherit from. Just as when we were inheriting from `torch.nn.Module` we had to specify the builder `__init__()` and the `forward()` method, for this inheritance we will have to specify `__len__()` that returns that total number of samples in the data set, and `__getitem__(index)` that returns a single sample in the dataset, in position `index`. You might notice that the names of both functions start with `__`, which in Python marks them as special methods. While they can both called directly, `object.__len__()` is also invoked when a `len(object)` is called, while `object.__getitem__(index)` is invoked when `object[index]` is called."
      ],
      "metadata": {
        "id": "Wk3HIpZNkD9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here is our new class, inheriting from Dataset; it is supposed to be a generic\n",
        "# class to load data sets obtained from openml\n",
        "class OpenMLDataset(torch.utils.data.Dataset) :\n",
        "\n",
        "  # we create our own builder, that has two compulsory arguments, X (tensor with feature values)\n",
        "  # and y (tensor with values of the target)\n",
        "  def __init__(self, X, y) :\n",
        "    # call __init__ of the parent class (in this case, the parent class has no __init__, but this is an implementation detail)\n",
        "    super(OpenMLDataset, self).__init__()\n",
        "    # we store the information internally in the object, as two attributes self.X and self.y\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  # the function that returns the total number of samples is easy\n",
        "  def __len__(self) :\n",
        "    return self.y.shape[0]\n",
        "\n",
        "  # the function that gets a single sample is also pretty straightforward\n",
        "  def __getitem__(self, index) :\n",
        "    return self.X[index], self.y[index]\n",
        "\n",
        "# fantastic! now we can instantiate our OpenMLDataset class, and use it to store\n",
        "# the training, validation and test set we created above\n",
        "train_data = OpenMLDataset(X_train_tensor, y_train_tensor)\n",
        "val_data = OpenMLDataset(X_val_tensor, y_val_tensor)\n",
        "test_data = OpenMLDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# let's give it a trial run\n",
        "print(\"Samples in the training set:\", len(train_data)) # here len() calls the __len__() method of the object\n",
        "print(\"Samples in the validation set:\", len(val_data))\n",
        "print(\"Samples in the test set:\", len(test_data))"
      ],
      "metadata": {
        "id": "ps3m8LRMlOko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are going to use `DataLoader` objects to manage the batches. `DataLoader` accepts several arguments in the builder, that are pretty self-explanatory.   "
      ],
      "metadata": {
        "id": "ftwbDkhBp0BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the 'shuffle' option here is whether the samples should be returned in a random order;\n",
        "# it only makes sense for the training data, from which we will obtain the loss information\n",
        "load_train = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "# we don't really need a data loader for the validation or test set, as we will just use them in one go\n",
        "\n",
        "# but in practice, what happens when we ask the DataLoaders to give us some samples?\n",
        "# let's check!\n",
        "for X_batch, y_batch in load_train :\n",
        "  print(\"The load_train DataLoader gave me a batch of %d samples\" % X_batch.shape[0])"
      ],
      "metadata": {
        "id": "4tGY8ottpxdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling a DataLoader like we did in the code cell above will return a series of batches of the specified size, until all the data set has been seen. As you might have noticed, the last batch in the set is smaller than the rest, as the amount of data we got was not exactly divisible by the batch size. Now, let's rewrite the optimization loop, this time using the DataLoaders we just created to update the gradient after each batch."
      ],
      "metadata": {
        "id": "iuJpY6mir1tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed\n",
        "random_seed = 42424242\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# SummaryWriter is a class able to create logs in a format that Tensorboard can read\n",
        "# we can specify a directory where the logs will be stored\n",
        "folder_name = \"my_great_runs/run_%d\" % random_seed\n",
        "writer = SummaryWriter(folder_name)\n",
        "\n",
        "# create another instance of the class\n",
        "two_layer_neural_network_3 = TwoLayerNeuralNetworkRegressor()\n",
        "\n",
        "# set hyperparameters of the optimizer, and create another instance of the optimizer\n",
        "learning_rate = 5e-3 # let's use a lower learning rate, for smaller steps\n",
        "max_epochs = 500\n",
        "optimizer_3 = torch.optim.SGD(params=two_layer_neural_network_3.parameters(), lr=learning_rate)\n",
        "\n",
        "# another instance of the loss function, classic MSE\n",
        "mse_loss_3 = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(0, max_epochs) :\n",
        "\n",
        "  # now, instead of just using the whole training set, we will work through batches\n",
        "  all_loss_batches = [] # let's keep track of loss of each batch\n",
        "  for X_batch, y_batch in load_train :\n",
        "\n",
        "    # get network predictions and compute loss\n",
        "    y_batch_pred = two_layer_neural_network_3(X_batch)\n",
        "    loss_batch = mse_loss_3(y_batch.view(-1,1), y_batch_pred)\n",
        "\n",
        "    # store information on the loss\n",
        "    all_loss_batches.append(loss_batch.item())\n",
        "\n",
        "    # update gradients and perform a step of the optimizer\n",
        "    optimizer_3.zero_grad() # set all information on gradients for parameters in the network to zero\n",
        "    loss_batch.backward() # use information from the loss_batch to backpropagate gradients on parameters\n",
        "    optimizer_3.step() # update weights in the opposite direction of the gradient\n",
        "\n",
        "  # now, we write the information on the average loss for all the batches\n",
        "  loss_train = np.mean(all_loss_batches)\n",
        "  writer.add_scalar(\"Loss/train\", loss_train, epoch)\n",
        "\n",
        "  with torch.no_grad() :\n",
        "    loss_val = loss_train # TODO: change this, add the real validation loss\n",
        "\n",
        "    # also store the information about the validation loss\n",
        "    writer.add_scalar(\"Loss/validation\", loss_val, epoch)\n",
        "\n",
        "  # printout on the first and last epoch, plus all epochs exactly divisible by 100\n",
        "  if epoch == 0 or epoch % 10 == 0 or epoch == max_epochs-1 :\n",
        "    print(\"Epoch %d: training loss (mean over batch)=%.4e, validation loss=%.4e\" % (epoch, loss_train, loss_val)) # this printout is run only each 100 epochs\n",
        "\n",
        "\n",
        "# 'flush' here is invoked just in case that the writer did not finish writing\n",
        "# everything to disk\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "gy0mylN8u84D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, try replacing the optimizer with one of the most recent ones, for example `Adam`, cutting and pasting the code below into the appropriate place of the cell above. How does it look like? More or less effective than SGD?\n",
        "\n",
        "```\n",
        "optimizer_3 = torch.optim.Adam(params=two_layer_neural_network_3.parameters(), lr=learning_rate)\n",
        "```\n",
        "\n",
        "As a final exercise, try using a scheduler to dynamically adjust the learning rate at each iteration. In pytorch, this is relatively easy: we first create an instance of the scheduler, and assign it to the instance of the optimizer:\n",
        "\n",
        "```\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_3, gamma=0.9)\n",
        "```\n",
        "\n",
        "And then, at the end of each iteration, we call a method from the scheduler instance to update the hyperparameters of the optimizer:\n",
        "\n",
        "```\n",
        "for epoch in range(0, max_epochs) :\n",
        "  ...\n",
        "  # end of the loop\n",
        "  scheduler.step()\n",
        "```"
      ],
      "metadata": {
        "id": "7U6RfqLZ3Ase"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpointing\n",
        "Here below we have the same code for the network above, just rearranged in a different way. First, an initialization."
      ],
      "metadata": {
        "id": "ACwwTnppqu8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed\n",
        "random_seed = 123456\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# create another instance of the class (we are going to overwrite the first networks and other variables we used in the first cells)\n",
        "two_layer_neural_network = TwoLayerNeuralNetworkRegressor()\n",
        "\n",
        "# set hyperparameters of the optimizer, and create another instance of the optimizer\n",
        "learning_rate = 1e-2\n",
        "max_epochs = 50\n",
        "optimizer = torch.optim.SGD(params=two_layer_neural_network.parameters(), lr=learning_rate)\n",
        "\n",
        "# instance of the loss function\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "# writer\n",
        "folder_name = \"my_great_runs/run_%d\" % random_seed\n",
        "writer = SummaryWriter(folder_name)"
      ],
      "metadata": {
        "id": "q5_JKd-A4OHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, here below, the code for the optimization. For the moment, do not modifiy the hyperparameters, try just running the cell with the optimization process below **multiple times**. What do you notice? What is happening here?"
      ],
      "metadata": {
        "id": "zIwUQtvX5WJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, max_epochs) :\n",
        "\n",
        "  # now, instead of just using the whole training set, we will work through batches\n",
        "  all_loss_batches = [] # let's keep track of loss of each batch\n",
        "  for X_batch, y_batch in load_train :\n",
        "\n",
        "    # get network predictions and compute loss\n",
        "    y_batch_pred = two_layer_neural_network(X_batch)\n",
        "    loss_batch = mse_loss(y_batch.view(-1,1), y_batch_pred)\n",
        "\n",
        "    # store information on the loss\n",
        "    all_loss_batches.append(loss_batch.item())\n",
        "\n",
        "    # update gradients and perform a step of the optimizer\n",
        "    optimizer.zero_grad() # set all information on gradients for parameters in the network to zero\n",
        "    loss_batch.backward() # use information from the loss_batch to backpropagate gradients on parameters\n",
        "    optimizer.step() # update weights in the opposite direction of the gradient\n",
        "\n",
        "  # now, we write the information on the average loss for all the batches\n",
        "  loss_train = np.mean(all_loss_batches)\n",
        "  writer.add_scalar(\"Loss/train\", loss_train, epoch)\n",
        "\n",
        "  with torch.no_grad() :\n",
        "    loss_val = loss_train # TODO: change this, add the real validation loss\n",
        "\n",
        "    # also store the information about the validation loss\n",
        "    writer.add_scalar(\"Loss/validation\", loss_val, epoch)\n",
        "\n",
        "  # printout on the first and last epoch, plus all epochs exactly divisible by 100\n",
        "  if epoch == 0 or epoch % 10 == 0 or epoch == max_epochs-1 :\n",
        "    print(\"Epoch %d: training loss (mean over batches)=%.4e, validation loss=%.4e\" % (epoch, loss_train, loss_val)) # this printout is run only each 100 epochs\n",
        "\n",
        "\n",
        "# 'flush' here is invoked just in case that the writer did not finish writing\n",
        "# everything to disk\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "M3b7419H4xiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network can **restart the training** from the values it obtained at the end of a previous optimization run. This is very useful, as it makes it possible to train a network through successive optimization runs, and even **saving intermediate results**, commonly called \"checkpoints\".\n",
        "\n",
        "Of course, pytorch offers some ways of saving and loading the weights of a network. You can even load the weights of a network inside a new network, but the two have to have the same architecture, same name of the modules, etc. In other words: they have to be two instances of the same torch.nn.Module class. Try the cell code below. It will save the weights of the neural network model we trained in the cell above, create a new instance, and then load the saved weights in the new instance. On Google Colaboratory, the file will be saved on your temporary storage (that you can access from the folder icon in the left-hand menu, 4th icon from the top).\n",
        "\n"
      ],
      "metadata": {
        "id": "I0T1ZPWX51AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_to_be_saved = {'model_state' : two_layer_neural_network.state_dict()} # .state_dict() is a dictionary with all the values of the parameters\n",
        "print(\"Trained weights:\", two_layer_neural_network.state_dict())\n",
        "\n",
        "# we can save it to a file using a pytorch function\n",
        "torch.save(state_to_be_saved, \"super_awesome_regressor\")\n",
        "\n",
        "# now, we can instantiate a new network, from the same class\n",
        "totally_new_network = TwoLayerNeuralNetworkRegressor()\n",
        "# and we can check that it does have different parameters, at this point randomly initialized\n",
        "print(\"Untrained weights:\", totally_new_network.state_dict())\n",
        "# but by loading the old parameter into the new network, we will find the initial values again\n",
        "state_from_file = torch.load(\"super_awesome_regressor\")\n",
        "totally_new_network.load_state_dict(state_from_file[\"model_state\"])\n",
        "print(\"Weights loaded from file:\", totally_new_network.state_dict())"
      ],
      "metadata": {
        "id": "wRc9FAnAcj9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are using one of the optimizers with internal parameters (for example, keeping track of the history of gradient values and so on), we can also save the current state of the optimizer, to restart the optimization exactly from where we left it."
      ],
      "metadata": {
        "id": "4bUx3HNHdAvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in general, we can add further information as other keys in the dictionary\n",
        "state_to_be_saved[\"optimizer_state\"] = optimizer.state_dict()\n",
        "print(\"Optimizer parameters:\", optimizer.state_dict())\n",
        "\n",
        "# the same is true for schedulers, and probably other utilities\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "print(\"Scheduler parameters:\", scheduler.state_dict())"
      ],
      "metadata": {
        "id": "NMucDmr0fr60"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}