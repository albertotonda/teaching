{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Jk5Yg-9ot0"
      },
      "source": [
        "# Visualization methods for Convolutional Neural Networks\n",
        "\n",
        "CNNs have been extremely successful, but since they perform _feature construction_, they are even more black-box than other models. For many practical applications, it is important to try and make sense of what the network learned. Here below are a few methods that can be used and easily implemented in pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NySUyeUBLcHa"
      },
      "source": [
        "## _Almost_ a real model: VGG-16\n",
        "Since so far we worked with toy models, let's try something more complex. VGG-16 is a CNN that was considered the state of the art for image recognition...in 2014. See the paper [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556).\n",
        "\n",
        "The architecture of VGG-16 is depicted in the image below. Luckily, pytorch (and in particular the torchvision library) gives us access to the whole network with pre-trained parameters. It might take a while to download (it's around 500MB), but it's easy.\n",
        "\n",
        "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvf_E6CB9ot3"
      },
      "outputs": [],
      "source": [
        "# classic imports\n",
        "import copy\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.optim import Adam\n",
        "from torchvision import models\n",
        "\n",
        "from PIL import Image, ImageFilter\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JAkUEJLa9ot3"
      },
      "outputs": [],
      "source": [
        "# let's get our model!\n",
        "vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "print(vgg16)\n",
        "total_parameters = sum(p.numel() for p in vgg16.parameters())\n",
        "print(\"This chonker of a model has %s parameters!\" % \"{:,}\".format(total_parameters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-MVylRD9ot3"
      },
      "source": [
        "Several times during the class, we repeated that using pre-trained models with pytorch is easy. Let's put this statement to the test! VGG-16 was originally trained for a classification competition, featuring image samples from 1,000 classes, so one dimension of the output tensor will be 1,000, with one value for each class, the highest being the class that the network predicted for the sample. The images have to be exactly 224x224 pixels, as that was the format for the competition. Let's find something from the internet and try to classify it. It would also be nice to have the _description_ of the class labels, to verify if the results make sense. Let's get all the necessary components and give VGG-16 a trial run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfJIMHXW9ot3"
      },
      "outputs": [],
      "source": [
        "# get an image from the internet using the PIL library; I intentionally picked an image that IS NOT\n",
        "# part of the images used to train VGG-16, it's a high-resolution cat on the internet\n",
        "# you can actually change the URL and use another image, but it will probably require modifying the\n",
        "# transformations below\n",
        "image_url = \"https://api.time.com/wp-content/uploads/2014/07/492290913.jpg\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "\n",
        "# torchvision has a utility function to transform the image in a tensor\n",
        "image_tensor = torchvision.transforms.functional.pil_to_tensor(image)\n",
        "print(\"Image tensor has shape:\", image_tensor.shape)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# .permute() here below offers a view of the tensor with the dimensions switched; this is because\n",
        "# in neural networks (and pytorch), traditionally the dimension defining the channels (R,G,B) comes\n",
        "# first, while in matplotlib.pyplot.imshow() it has to come in last; so the call .permute(1,2,0)\n",
        "# just means \"return a view that puts dimension 1 first, dimension 2 second, and dimension 0 last\"\n",
        "ax.imshow(image_tensor.permute(1,2,0).numpy())\n",
        "ax.set_title(\"Look at this adorable but GIANT cat, measuring %s x %s pixels\" %\n",
        "             (\"{:,}\".format(image_tensor.shape[1]), \"{:,}\".format(image_tensor.shape[2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvsnacRd9ot3"
      },
      "outputs": [],
      "source": [
        "# luckily, we can run some transformations to crop the image to a square, and then resize it, using\n",
        "# other torchvision transformations\n",
        "image_width = image_tensor.shape[2]\n",
        "image_height = image_tensor.shape[1]\n",
        "image_tensor = torchvision.transforms.functional.crop(image_tensor, top=0,\n",
        "                                        left=image_width-image_height,\n",
        "                                        height=image_height,\n",
        "                                        width=image_height,\n",
        "                                            )\n",
        "print(\"Image tensor after cropping has now shape:\", image_tensor.shape)\n",
        "\n",
        "image_tensor = torchvision.transforms.functional.resize(image_tensor, (224, 224))\n",
        "print(\"Image tensor after resizing has now shape:\", image_tensor.shape)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(image_tensor.permute(1,2,0).numpy())\n",
        "ax.set_title(\"Look at this adorable cat, now measuring a more reasonable %s x %s pixels\" %\n",
        "             (\"{:,}\".format(image_tensor.shape[1]), \"{:,}\".format(image_tensor.shape[2])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDjG38bm9ot3"
      },
      "source": [
        "And now, after pre-processing the image, we can finally send it through VGG-16 and look at the result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMq7EjYn9ot4"
      },
      "outputs": [],
      "source": [
        "# we still need a few more transformation, casting the data type of the tensor to float32,\n",
        "# and normalizing everything using means and standard deviations,\n",
        "# taken from the ImageNet literature; when performing multiple chained transformations, we can apply\n",
        "# the Compose functionality. Here we use the most recent version of the transformations, namespace \"v2\"\n",
        "# print(torchvision.__version__) # this was just a debug print, we need at least torchvision 0.17\n",
        "from torchvision.transforms import v2\n",
        "composed_transformation = torchvision.transforms.Compose(\n",
        "                        [\n",
        "                            v2.ToDtype(torch.float32, scale=True),\n",
        "                            v2.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                        std=(0.229, 0.224, 0.225))\n",
        "                        ]\n",
        "                        )\n",
        "image_tensor = composed_transformation(image_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG5JuFrC9ot4"
      },
      "outputs": [],
      "source": [
        "# we can finally send the image through VGG-16!\n",
        "# first, we need to unsqueeze the image tensor, currently of shape [n_channels, height, width]\n",
        "# to add another dimension, as the tensor that VGG-16 expects in input has shape\n",
        "# [n_samples, n_channels, height, width], so we add another dimension in the first position (dim=0)\n",
        "print(\"Performing forward pass with the image...\")\n",
        "vgg16.eval()\n",
        "y_hat = vgg16(image_tensor.unsqueeze(dim=0))\n",
        "print(\"Done! Shape of the output tensor:\", y_hat.shape)\n",
        "\n",
        "# the output is clearly a tensor of shape [1,1000], where 1,000 are the classes used for the ImageNet\n",
        "# competition; the class prediction will be the highest value in the resulting tensor\n",
        "class_prediction = torch.argmax(y_hat, dim=1).item()\n",
        "\n",
        "# unfortunately, we don't have the class labels readily somewhere in pytorch; but we can download them from\n",
        "# the best available source, a random GitHub repository\n",
        "imagenet_index_to_label_url = \"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\"\n",
        "r = requests.get(imagenet_index_to_label_url)\n",
        "# using a library called 'ast' to quickly parse the text file into a dictionary\n",
        "import ast\n",
        "imagenet_index_to_label = ast.literal_eval(r.text)\n",
        "\n",
        "print(\"The class label for the predicted class is (drum roll): index #%d, \\\"%s\\\"\" %\n",
        "      (class_prediction, imagenet_index_to_label[class_prediction]))\n",
        "\n",
        "# we can even sort the predictions in the y_hat tensor from highest to lowest, to have an idea of\n",
        "# what could be other likely classes; we convert the tensor to a numpy array, and use a numpy function\n",
        "# to obtain an array of indexes\n",
        "indexes_by_likelihood = np.argsort(-y_hat.detach().squeeze().numpy())\n",
        "top_classes = [imagenet_index_to_label[indexes_by_likelihood[i]] for i in range(0, 5)]\n",
        "print(\"Top 5 most likely classes:\", top_classes)\n",
        "\n",
        "# this worked so well that I am keeping a copy of the input image tensor, so that I can use it later\n",
        "image_tensor_cat = image_tensor.clone()\n",
        "class_index_for_image_tensor_cat = class_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg2fXWqF9ot4"
      },
      "source": [
        "Easy, right? It took me only **four whole hours** to make this part of the code work, but to be fair it was mostly because of stupid issues with image preprocessing and having an older version of torchvision. Obtaining and using the pre-trained models is easy.\n",
        "\n",
        "Now, try to download, preprocess, and send through the network another image of your choice, checking if the classification label is correct. You can take a look at the imagenet_index_to_label dictionary to have an idea of the type of objects that the VGG-16 network has been trained to recognize. You can use the preprocessing pipeline below to easily apply a \"center crop\" (cropping so that it looks like a square, centered on the middle point of the image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dL5zMx19ot4"
      },
      "outputs": [],
      "source": [
        "# download image and visualize it; TODO: change the url (note: some websites forbid direct access to images,\n",
        "# if you get errors with the request, just pick another image, or download it and find a way to access\n",
        "# and load the file from your notebook)\n",
        "image_url = \"https://api.time.com/wp-content/uploads/2014/07/492290913.jpg\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "image_width, image_height = image.size\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(image)\n",
        "ax.set_title(\"Image: %d x %d pixels\" % (image_width, image_height))\n",
        "\n",
        "# apply preprocessing to: convert image to a tensor, center crop it, resize it, and normalize it\n",
        "# using torchvision.transforms.Compose is pretty convenient here, as we can define our pre-processing\n",
        "# pipeline once, and then re-use it with just one call\n",
        "imagenet_composed_transformation = torchvision.transforms.Compose(\n",
        "                        [\n",
        "                            v2.PILToTensor(),\n",
        "                            # here below, it is cropping using a square with a side of\n",
        "                            #  the smallest dimension, centered on the middle point\n",
        "                            v2.CenterCrop(min(image_height, image_width)),\n",
        "                            v2.Resize((224, 224)),\n",
        "                            v2.ToDtype(torch.float32, scale=True),\n",
        "                            v2.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                        std=(0.229, 0.224, 0.225))\n",
        "                        ]\n",
        "                        )\n",
        "image_tensor = imagenet_composed_transformation(image)\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(image_tensor.permute(1, 2, 0).numpy())\n",
        "ax.set_title(\"Image after cropping, resizing, normalizing: %d x %d pixels\" %\n",
        "             (image_tensor.shape[1], image_tensor.shape[2]))\n",
        "\n",
        "# TODO: now pass the image_tensor through the network, collect the output tensor,\n",
        "# and use the dictionary imagenet_index_to_label to obtain the class label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jempcb39ot4"
      },
      "source": [
        "## Visualizing features constructed/extracted by the CNN\n",
        "\n",
        "Now that everything is running, we can start by checking the features extracted by VGG-16. In order to do so, we will have to check the intermediate outputs of each module. There are lots of modules and lots of outputs, but luckily we have complete access to the network: we know its architecture, and we even have access to each of its individual layers. A first, straightforward way to visualize what is happening is to run the input image tensor through the layers, one by one, and interpret the outputs as images. Let's see an example for the first layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJIbWAz99ot4"
      },
      "outputs": [],
      "source": [
        "# let's first print out the names of the modules\n",
        "for name, module in vgg16.named_modules() :\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Sr0pFk9ot4"
      },
      "source": [
        "Now, you might have noticed that VGG-16 has several modules named `features.NUMBER` and `classifier.NUMBER`. VGG-16 is using a specific type of nested architecture, where the modules have been grouped into three parts, called: `features`, `avgpool`, and `classifier`. Just calling `vgg16.features` will return a reference to the whole set of modules that are part of that group. We can access a single module in a group (called `torch.nn.Sequential`) by treating it as a list and specifying for example `vgg16.features[4]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNkkJtZT9ot4"
      },
      "outputs": [],
      "source": [
        "print(\"This is the whole set of torch.nn.Sequential modules called \\\"features\\\":\", vgg16.features)\n",
        "print(\"This is the first module:\", vgg16.features[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hAErW819ot4"
      },
      "source": [
        "Now that we know how to access single modules group together in a `torch.nn.Sequential` object, let's visualize the output of the first module. You will have to write a part of the code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aZwEhc09ot4"
      },
      "outputs": [],
      "source": [
        "vgg16_module = vgg16.features[0] # reference to the module we are interested in # INTERESTINGLY, YOU CAN ALSO SLICE IT AS A LIST, INSTEAD OF LOOPING\n",
        "print(\"The target module has %d parameters\" %\n",
        "      sum(p.numel() for p in vgg16_module.parameters() if p.requires_grad))\n",
        "\n",
        "# let's pass the image through the network\n",
        "output_tensor_module = vgg16_module(image_tensor)\n",
        "print(\"The original input had shape: %s; the output of the module has shape: %s\"\n",
        "     % (str(image_tensor.shape), str(output_tensor_module.shape)))\n",
        "\n",
        "# the first module is a Conv2D with 64 \"filters\", let's visualize the output\n",
        "fig, ax = plt.subplots(nrows=8, ncols=8, figsize=(20, 20))\n",
        "for row in range(0, 8) :\n",
        "    for column in range(0, 8) :\n",
        "        filter_index = row + column * 8\n",
        "        # ax[row, column].imshow(...) # TODO you need to write this part\n",
        "        ax[row, column].set_title(\"Output of filter %d\" % (filter_index))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExePHCVz9ot5"
      },
      "source": [
        "Now, modify the cell above to do the same for another convolutional module, this time close to the end of the neural network. How do the features look like?\n",
        "\n",
        "It might also be interesting to check how features change as the image tensor goes through more and more modules. Let's plot just one dimension of the intermediate output tensor per module, and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9m_d--o9ot5"
      },
      "outputs": [],
      "source": [
        "# fix a particular value of the output dimension that we are going to check\n",
        "output_dimension = 0\n",
        "\n",
        "# let's create a copy of the original image tensor, so that we can work on it without messing up\n",
        "z_tensor = image_tensor.clone()\n",
        "\n",
        "# there are a total of 31 modules, so we can create a plot with 32 subplots and put the image in ax[0, 0]\n",
        "fig, ax = plt.subplots(ncols=4, nrows=8, figsize=(16, 32))\n",
        "\n",
        "for row in range(0, 8) :\n",
        "    for column in range(0, 4) :\n",
        "        if row == 0 and column == 0 :\n",
        "            ax[row, column].imshow(image_tensor.permute(1, 2, 0).numpy())\n",
        "            ax[row, column].set_title(\"Original image\")\n",
        "        else :\n",
        "            module_index = row * 4 + column - 1\n",
        "            z_tensor = vgg16.features[module_index](z_tensor)\n",
        "            # let's work again on a copy, to avoid issues\n",
        "            module_output_tensor = z_tensor.clone()\n",
        "            # TODO you have to do this part; pick a slice of the output tensor\n",
        "            # and visualize it as an image with .imshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdXuV4fq9ot5"
      },
      "source": [
        "While this visualization is very cool (albeit a bit psychedelic), ultimately it is not very informative. There are better visualization methods for CNNs, and we are going to see them in the next exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_SSNGXk9ot5"
      },
      "source": [
        "## Saliency maps\n",
        "\n",
        "The idea at the base of saliency maps is to identify the areas of the image that most impact the final decision for the class. The intuition is to send the image tensor through the network, and evaluate the gradient of the value of the element of the output tensor corresponding to the correct class, with _respect to each pixel_ of the original image.\n",
        "\n",
        "High absolute values of the gradient for a pixel indicate that that pixel might be fundamental for the decision: in other words, slightly changing the value of a pixel for which there is a high gradient could strongly impact the value of the tensor at the output of the network, and thus change the classification.\n",
        "\n",
        "Step by step, the method consists in:\n",
        "1. Sending the image tensor through the whole network (forward pass), with `requires_grad=True`.\n",
        "2. Compute the backward from the output score of the target class.\n",
        "3. Visualize the absolute value of the gradient with respect to the each element (pixel) in the image tensor. If an image tensor has multiple channels (RGB), we pick the maximum value of the gradient for each channel corresponding to the same pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPyHTRU39ot5"
      },
      "outputs": [],
      "source": [
        "def get_saliency(model, img, label):\n",
        "    # reset gradients in the model\n",
        "    model.zero_grad()\n",
        "    # activate gradient computation for image tensor, initialize gradients to None\n",
        "    img.requires_grad = True\n",
        "    img.grad = None\n",
        "    # compute output of the model\n",
        "    model_output = model(img.unsqueeze(0))\n",
        "    # compute a softmax on the output, to get something more similar to class probabilities\n",
        "    softmax_output = torch.nn.Softmax(dim=1)(model_output)\n",
        "    # only consider the output value for the true class label\n",
        "    softmax_output = softmax_output[0, label]\n",
        "    # backward pass\n",
        "    softmax_output.backward()\n",
        "    # get the absolute values of the gradients for each element of the input tensor\n",
        "    sal = img.grad.abs()\n",
        "    # if the input tensor had more than two dimensions (e.g. because of 3 channels)\n",
        "    # resize the saliency map to two dimensions by taking the highest gradient value\n",
        "    # for each channel\n",
        "    if sal.dim() > 2:\n",
        "        sal = torch.max(sal,dim=0)[0]\n",
        "    return sal\n",
        "\n",
        "# this example below is using my beloved cat image; TODO: replace it with the image you selected\n",
        "saliency = get_saliency(vgg16, image_tensor_cat, class_index_for_image_tensor_cat)\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 16))\n",
        "ax[0].imshow(image_tensor_cat.detach().cpu().squeeze(0).permute(1, 2, 0))\n",
        "ax[0].set_title(\"Original image\")\n",
        "ax[1].imshow(saliency.to('cpu'), cmap=\"seismic\") #, interpolation=\"bilinear\")\n",
        "ax[1].set_title(\"Saliency map for image with respect to class index %d (%s)\" %\n",
        "               (class_index_for_image_tensor_cat, imagenet_index_to_label[class_index_for_image_tensor_cat]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJFuTj-f9ot5"
      },
      "source": [
        "## Gradient-weighted Class Activation Maps (Grad-CAM)\n",
        "\n",
        "Saliency maps, however, use only the information from the final output to the original pixels. As the last modules performing convolutions map to large areas of the original image, it would be interesting to know the importance of the output of the last modules on the final classification performance. Unfortunately, the last part of most CNNs is a set of linear modules + activations, creating a high complexity of non-linearities that makes it harder to assess the impact of each output on the final class label. This is also true for VGG-16, where the last modules of the network are grouped together in a `torch.nn.Sequential` object called `classifier`. Look at how many parameters it has!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl-xVAUf9ot5"
      },
      "outputs": [],
      "source": [
        "print(vgg16.classifier)\n",
        "print(\"Total number of parameters:\", \"{:,}\".format(sum(p.numel() for p in vgg16.classifier.parameters())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oGcBTjw9ot5"
      },
      "source": [
        "The idea of Grad-CAM is to replace the last part of a CNN with a simple linear module, accepting a higher classification error in exchange for an easier assessment of the impact of each output of the convolutional modules on the final result. This would normally require replacing the last modules of the network with a linear module, and retraining it on the training set. We ain't got time to do that, so we are going to exploit a visualization library built on top of pytorchvision, called [pytorchgradcam](https://github.com/jacobgil/pytorch-grad-cam/tree/master), where this has been done for us by Jacob Gildenblat. This might take some time to download and install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2xEOuXQ9ot5"
      },
      "outputs": [],
      "source": [
        "!pip install grad-cam --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example here below is still using the image of the cat. Run it to see what happens, then replace it with another image, and check whether the GradCAM still makes sense."
      ],
      "metadata": {
        "id": "yyVbHVBFAck_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1sOPYl19ot5"
      },
      "outputs": [],
      "source": [
        "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# for several reasons, we have to reload the image here, as pytorch_grad_cam likes to work with a specific\n",
        "# format for visualizing the original; replace this with your image URL and class index\n",
        "# TODO: replace the URL and the class index with the image of your choice, and check the results\n",
        "image_url = \"https://api.time.com/wp-content/uploads/2014/07/492290913.jpg\"\n",
        "image_raw = Image.open(requests.get(image_url, stream=True).raw)\n",
        "image_width, image_height = image_raw.size\n",
        "cropped_image = image_raw.crop((image_width-image_height, 0, image_width, image_height))\n",
        "resized_image = cropped_image.resize((224, 224))\n",
        "# now, we have to separately prepare a resized 'visualizable' image as a numpy array\n",
        "# because pytorch_grad_cam does only likes images with a certain format\n",
        "visualizable_image = np.array(resized_image)\n",
        "visualizable_image = np.float32(visualizable_image) / 255\n",
        "\n",
        "# here comes the interesting part; we define a GradCAM object, and all related necessary information\n",
        "# first, the layers we are interested in; it can be a list, here it's just the last layer in 'features'\n",
        "target_layers = [ vgg16.features[-1] ]\n",
        "# instantiate the GradCAM object\n",
        "cam = GradCAM(model=vgg16, target_layers=target_layers)\n",
        "# we also need the classification target, a ClassifierOutputTarget object that needs the class index\n",
        "targets = [ ClassifierOutputTarget(class_index_for_image_tensor_cat) ]\n",
        "\n",
        "# let's run the gradcam! the output is a tensor of greyscale maps that show the most important parts\n",
        "# of the image\n",
        "greyscale_map = cam(input_tensor=image_tensor_cat.unsqueeze(0), targets=targets)\n",
        "# in this example, there was just one image, so let's take the first grayscale map\n",
        "greyscale_map = greyscale_map[0, :]\n",
        "\n",
        "# and now, we can plot everything using a utility function that overlaps the greyscale map to the image\n",
        "cam_image = show_cam_on_image(visualizable_image, greyscale_map, use_rgb=True)\n",
        "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(12, 4))\n",
        "ax[0].imshow(visualizable_image)\n",
        "ax[0].set_title(\"Original image\")\n",
        "ax[1].imshow(greyscale_map, cmap=\"Greys\")\n",
        "ax[1].set_title(\"Greyscale CAM\")\n",
        "ax[2].imshow(cam_image)\n",
        "ax[2].set_title(\"CAM overlapped on the original image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWSNF5g99ot5"
      },
      "source": [
        "The pytorchgradcam library also contains more advanced versions of GradCAM-based visualization methods for CNNs, and metrics that can be used to compare different methods. If you are interested, [the tutorials, including ipython notebooks, can be found here](https://jacobgil.github.io/pytorch-gradcam-book/introduction.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiNEp6Y8L2kp"
      },
      "source": [
        "## Visualization of feature patterns\n",
        "\n",
        "If you remember the short introduction to xAI that we saw during the class, you might also recall that there are two different types of explanations: **local** (why did the ML algorithm answer in this way for this particular sample?) and **global** (from the point of view of the ML algorithm, what are the characteristics of a certain class in a classification problem?). The examples we saw so far are all _local_ explanations, trying to make sense of one sample. We are now going to see an attempt at a more _global_ explanation of CNN behavior, finding out the patterns recognized by the each filter in the Convolutional modules.\n",
        "\n",
        "The approach is to _generate an image_ such that it maximizes one output in a particular module. We can frame this task as an optimization problem, and employ gradients: in particular, we can compute the gradients on the pixels with respect to the target output, and optimize the pixel values with a gradient descent algorithm. In this case, instead of minimizing, we are attempting to _maximize_ the target output value.\n",
        "\n",
        "Let's try to do that, with some code stolen directly from the GitHub repository of [Utku Ozbulak](github.com/utkuozbulak), that also features examples of several other visualization techniques for CNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9fjNJyXLWh5"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torchvision import models\n",
        "\n",
        "from PIL import Image, ImageFilter\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def preprocess_image(pil_im, resize_im=True):\n",
        "    \"\"\"\n",
        "        Processes image for CNNs\n",
        "\n",
        "    Args:\n",
        "        PIL_img (PIL_img): PIL Image or numpy array to process\n",
        "        resize_im (bool): Resize to 224 or not\n",
        "    returns:\n",
        "        im_as_var (torch variable): Variable that contains processed float tensor\n",
        "    \"\"\"\n",
        "    # mean and std list for channels (Imagenet)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    #ensure or transform incoming image to PIL image\n",
        "    if type(pil_im) != Image.Image:\n",
        "        try:\n",
        "            pil_im = Image.fromarray(pil_im)\n",
        "        except Exception as e:\n",
        "            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
        "\n",
        "    im_as_arr = np.float32(pil_im)\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "        im_as_arr[channel] -= mean[channel]\n",
        "        im_as_arr[channel] /= std[channel]\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var\n",
        "\n",
        "\n",
        "def recreate_image(im_as_var):\n",
        "    \"\"\"\n",
        "        Recreates images from a torch variable, sort of reverse preprocessing\n",
        "    Args:\n",
        "        im_as_var (torch variable): Image to recreate\n",
        "    returns:\n",
        "        recreated_im (numpy arr): Recreated image in array\n",
        "    \"\"\"\n",
        "    reverse_mean = [-0.485, -0.456, -0.406]\n",
        "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
        "    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n",
        "    for c in range(3):\n",
        "        recreated_im[c] /= reverse_std[c]\n",
        "        recreated_im[c] -= reverse_mean[c]\n",
        "    recreated_im[recreated_im > 1] = 1\n",
        "    recreated_im[recreated_im < 0] = 0\n",
        "    recreated_im = np.round(recreated_im * 255)\n",
        "\n",
        "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
        "    return recreated_im\n",
        "\n",
        "def save_image(im, path):\n",
        "    \"\"\"\n",
        "        Saves a numpy matrix or PIL image as an image\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
        "        path (str): Path to the image\n",
        "    \"\"\"\n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)\n",
        "\n",
        "def format_np_output(np_arr):\n",
        "    \"\"\"\n",
        "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
        "        It converts all the outputs to the same format which is 3xWxH\n",
        "        with using sucecssive if clauses.\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
        "    \"\"\"\n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    # Result: Add a dimension at the beginning\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "    return np_arr\n",
        "\n",
        "class CNNLayerVisualization():\n",
        "    \"\"\"\n",
        "        Produces an image that minimizes the loss of a convolution\n",
        "        operation for a specific layer and filter\n",
        "    \"\"\"\n",
        "    def __init__(self, model, selected_layer, selected_filter):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.selected_layer = selected_layer\n",
        "        self.selected_filter = selected_filter\n",
        "        self.conv_output = 0\n",
        "        # Create the folder to export images if not exists\n",
        "        if not os.path.exists('../generated'):\n",
        "            os.makedirs('../generated')\n",
        "\n",
        "    def hook_layer(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            # Gets the conv output of the selected filter (from selected layer)\n",
        "            self.conv_output = grad_out[0, self.selected_filter]\n",
        "        # Hook the selected layer\n",
        "        self.model[self.selected_layer].register_forward_hook(hook_function)\n",
        "\n",
        "    def visualise_layer_with_hooks(self, iterations=100):\n",
        "        # Hook the selected layer\n",
        "        self.hook_layer()\n",
        "        # Generate a random image\n",
        "        random_image = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n",
        "        # Process image and return variable\n",
        "        processed_image = preprocess_image(random_image, False)\n",
        "        # Define optimizer for the image\n",
        "        optimizer = Adam([processed_image], lr=0.1, weight_decay=1e-6)\n",
        "        for i in range(1, iterations+1):\n",
        "            optimizer.zero_grad()\n",
        "            # Assign create image to a variable to move forward in the model\n",
        "            x = processed_image\n",
        "            for index, layer in enumerate(self.model):\n",
        "                # Forward pass layer by layer\n",
        "                # x is not used after this point because it is only needed to trigger\n",
        "                # the forward hook function\n",
        "                x = layer(x)\n",
        "                # Only need to forward until the selected layer is reached\n",
        "                if index == self.selected_layer:\n",
        "                    # (forward hook function triggered)\n",
        "                    break\n",
        "            # Loss function is the mean of the output of the selected layer/filter\n",
        "            # We try to minimize the mean of the output of that specific filter\n",
        "            loss = -torch.mean(self.conv_output)\n",
        "            if i % 10 == 0 or i == 1 :\n",
        "                print('Iteration:', str(i), 'Loss:', \"{0:.2e}\".format(loss.data.numpy()))\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            # Update image\n",
        "            optimizer.step()\n",
        "            # Recreate image\n",
        "            self.created_image = recreate_image(processed_image)\n",
        "            # Save image\n",
        "            if i % 10 == 0 or i == 1 :\n",
        "                im_path = './generated/layer_vis_l' + str(self.selected_layer) + \\\n",
        "                    '_f' + str(self.selected_filter) + '_iter' + str(i) + '.jpg'\n",
        "                save_image(self.created_image, im_path)\n",
        "\n",
        "\n",
        "if not os.path.exists(\"generated\") :\n",
        "  os.makedirs(\"generated\")\n",
        "\n",
        "# after defining all these functions, we can set the target layer; for several reasons,\n",
        "# it's easier to just work on the part of VGG-16 that creates the features\n",
        "pretrained_model = vgg16.features\n",
        "module_index = 1 # it's better to check what happens after ReLUs, so index 1 is the first and 29 is the last\n",
        "filter_position = 2 # we also need to select the filter in the module\n",
        "layer_vis = CNNLayerVisualization(pretrained_model, module_index, filter_position)\n",
        "\n",
        "# the results are saved as images in the directory \"./generated\", with file names\n",
        "# in the format \"layer_vis_l<MODULE>_f<FILTER>_iter<ITERATION>.jpg\"\n",
        "layer_vis.visualise_layer_with_hooks(iterations=100)\n",
        "\n",
        "# we can now visualize a few selected images\n",
        "iterations_to_visualize = [1, 20, 50, 100]\n",
        "fig, ax = plt.subplots(nrows=1, ncols=len(iterations_to_visualize),\n",
        "                       figsize=(4 * len(iterations_to_visualize), 4))\n",
        "for index, iteration in enumerate(iterations_to_visualize) :\n",
        "    image_file_name = \"./generated/layer_vis_l%d_f%d_iter%d.jpg\" % (module_index, filter_position, iteration)\n",
        "    ax[index].imshow(Image.open(image_file_name))\n",
        "    ax[index].set_title(\"Module %d, filter %d, iteration %d\" %\n",
        "                        (module_index, filter_position, iteration))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38V38rRC9ot6"
      },
      "source": [
        "Now, the general consensus is that the \"features\" visualized by these images increase in \"complexity\" as the we go deeper into the network. For the first modules, you should see simple repeated patterns all over the place. Higher modules should have more complex patterns, spanning the whole image.\n",
        "\n",
        "Let's try to perform a longitudinal test. We will keep the target filter index fixed, and we will attempt to visualize all outputs in deeper and deeper modules. You will be able to find all the images generated in this way inside the \"generated\" folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S_ZDRJZ9ot6"
      },
      "outputs": [],
      "source": [
        "indexes_of_all_relu_modules = [i for i in range(0, len(pretrained_model))\n",
        "                               if isinstance(pretrained_model[i], torch.nn.ReLU)]\n",
        "print(\"Indexes of all ReLU modules in vgg.features:\", indexes_of_all_relu_modules)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell code below, complete the code to run the visualization algorithm on all the ReLU modules. The images generated in this way will be then read by the code cell after that and visualized. You can get sharper images by increasing the number of iterations, but it will also increase the running time of the algorithm."
      ],
      "metadata": {
        "id": "qbLCIiQvBYlG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6xv1A9-9ot6"
      },
      "outputs": [],
      "source": [
        "filter_position = 4\n",
        "max_iterations = 10\n",
        "for module_index in indexes_of_all_relu_modules :\n",
        "    print(\"Now optimizing image for module %d...\" % module_index)\n",
        "    # TODO you have to complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke6_FTx99ot6"
      },
      "outputs": [],
      "source": [
        "# and now, some plotting\n",
        "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
        "for row in range(0, 4) :\n",
        "    for column in range(0, 4) :\n",
        "        index = row * 4 + column\n",
        "        if index < len(indexes_of_all_relu_modules) :\n",
        "            module_index = indexes_of_all_relu_modules[index]\n",
        "            image_file_name = \"./generated/layer_vis_l%d_f%d_iter%d.jpg\" % (module_index, filter_position, max_iterations)\n",
        "            ax[row, column].imshow(Image.open(image_file_name))\n",
        "            ax[row, column].set_title(\"Module %d, filter %d\" % (module_index, filter_position))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}