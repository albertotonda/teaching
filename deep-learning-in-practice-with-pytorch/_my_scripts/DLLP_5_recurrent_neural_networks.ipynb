{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "In this series of exercises, we are going to see some basic applications of Recurrent Neural Networks (RNNs)."
      ],
      "metadata": {
        "id": "fQ9mZ_pY0nTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# classical imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "A0ijfIOO1LMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First RNN: sinusoidal wave\n",
        "\n",
        "This toy problem is an extremely simple illustration of a univariate (one-feature) time series. Our objective will be to build a model able to correctly predict the next value in the series, given a variable amount of sequential values from previous timesteps. First, let's generate some data and visualize the result."
      ],
      "metadata": {
        "id": "tSC2sCFA079D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja4T7Fm-0f-9"
      },
      "outputs": [],
      "source": [
        "# since we are creating artificial data, we can operate directly on tensors\n",
        "t = torch.linspace(0, 799, 800) # sample uniformely space in (0,799) for 800 points\n",
        "y = torch.sin(t*2*3.1416/40)\n",
        "\n",
        "# let's just visualize the first 100 samples\n",
        "fig, ax = plt.subplots(figsize=(12,4))\n",
        "ax.plot(t[:100], y[:100], color='green', label=\"Sinusoidal wave\")\n",
        "ax.set_xlabel(\"t\")\n",
        "ax.set_ylabel(\"y = sin(t*2*pi/40)\")\n",
        "ax.set_title(\"First 100 points of the time series\")\n",
        "ax.legend(loc='best')\n",
        "ax.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we split the data into training and test (let's forget the validation set, for the moment)."
      ],
      "metadata": {
        "id": "k4kicDmG2JaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 40\n",
        "\n",
        "# negative indices start indexing from the last part of the tensor/array (e.g. last element is y[-1])\n",
        "train_set = y[:-(test_size)]\n",
        "test_set = y[-test_size:]\n",
        "\n",
        "print(\"Total data: %d samples; Training set: %d samples; Test set: %d samples\" %\n",
        "      (y.shape[0], train_set.shape[0], test_set.shape[0]))"
      ],
      "metadata": {
        "id": "qLm7g_q22I4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this were a classical neural network, we would be done; but in this case, we need to actually create training sequences, where we give a few input values to the RNN, and then have the next value in the sequence as output to be predicted, to be used as a ground truth. We can slide a window of given size over the single samples of our training set to create this new training data."
      ],
      "metadata": {
        "id": "cbmpLRlM3uHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use a simple function to automatically create the training data;\n",
        "# it's going to return lists of tuples (x_tensor, y_tensor), where x_tensor\n",
        "# are the values in the previous time steps, and y_tensor is the next value\n",
        "# to be predicted\n",
        "def input_data(sequence, window_size) :\n",
        "    out = []\n",
        "    L = len(sequence)\n",
        "\n",
        "    for i in range(0, L - window_size) :\n",
        "        window = sequence[i:i+window_size]\n",
        "        label = sequence[i+window_size:i+window_size+1]\n",
        "        out.append((window, label))\n",
        "\n",
        "    return out\n",
        "\n",
        "window_size = 40\n",
        "print(\"Creating input sequences of %d samples\" % window_size)\n",
        "train_data = input_data(train_set, window_size)\n",
        "print(\"Created a total of %d input sequences\" % len(train_data))\n",
        "\n",
        "# let's take a look at the first training sequence\n",
        "x_tensor = train_data[0][0] # training samples for this sequence\n",
        "y_tensor = train_data[0][1] # next value to be predicted\n",
        "t = [i for i in range(0, x_tensor.shape[0])]\n",
        "fig, ax = plt.subplots(figsize=(12,4))\n",
        "ax.plot(t, x_tensor, color='green', label=\"Sinusoidal wave, training sequence\")\n",
        "ax.scatter(t[-1] + 1, y_tensor, color='red', label=\"Next value to be predicted\")\n",
        "ax.set_xlabel(\"t\")\n",
        "ax.set_ylabel(\"y = sin(t^2*pi/40)\")\n",
        "ax.set_title(\"First training sequence\")\n",
        "ax.legend(loc='best')\n",
        "ax.grid(True)"
      ],
      "metadata": {
        "id": "TJ9WCJT-4VTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We finally get to our favorite part, where we need to create a new model, inheriting from torch.nn.Module."
      ],
      "metadata": {
        "id": "J0SVfLz-53Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyFirstLSTM(nn.Module):\n",
        "\n",
        "    # we need to specify the size of the hidden state, or in other word, the tensor\n",
        "    # storing the history\n",
        "    def __init__(self,input_size = 1, hidden_size = 1, out_size = 1) :\n",
        "        super().__init__()\n",
        "        # the hidden state of all cells is initialized to zeros; the hidden state in\n",
        "        # this case is a set of h_t and c_t for each LSTM unit in the module\n",
        "        # the number of LSTM units is the parameter 'hidden_size'\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size)) # (h_t0, c_t0)\n",
        "        # and the neural network simply has two modules:\n",
        "        # - a LSTM, that will be unrolled to adjust for sequences of different length\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
        "        # - and a simple linear module, that will output the value for the next element in the sequence\n",
        "        self.linear = nn.Linear(hidden_size, out_size)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self, sequence) :\n",
        "        # the LSTM module is called with the input sequence and the hidden state\n",
        "        # stored inside the attribute self.hidden of this class\n",
        "        lstm_out, self.hidden = self.lstm(sequence.view(len(sequence), 1, -1), self.hidden) # this call performs unrolling inside\n",
        "        # after getting the outputs of all LSTM cells as a sequence, the linear layer\n",
        "        # outputs one prediction per element of the sequence\n",
        "        pred = self.linear(lstm_out.view(len(sequence),-1))\n",
        "        # we return the last element in the sequence as the prediction for t+1,\n",
        "        # but in fact we got one prediction for each unrolled unit of the LSTM module\n",
        "        return pred[-1]\n",
        "\n",
        "    # forward pass with printouts (TODO, see below)\n",
        "\n",
        "# let's instantiate a network here, to check the parameter count\n",
        "my_first_lstm = MyFirstLSTM(hidden_size=40)\n",
        "print(\"My first LSTM network has %d parameters!\" % sum(p.numel() for p in my_first_lstm.parameters() if p.requires_grad))\n",
        "print(\"Of which %d inside the LSTM cells\" % sum(p.numel() for p in my_first_lstm.lstm.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "id": "LYcplSgH5-Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before trying to run the code, try setting the size of the hidden state to 1. This will make the LSTM module use a single unit, so we can check that the number of parameters is what we expect. If they look a bit more than what you would have anticipated, it's because pytorch keeps some of the biases as two separate values instead of one, see the [LSTM module documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) for more details.\n",
        "\n",
        "Now, add a new method to the `MyFirstLSTM` class, `forward_with_printouts` which will basically be a copy of the `forward`, just printing out the shapes of the tensors and the shapes of the hidden state. If you feel particularly inspired, you could try to create [forward hooks for the layers](https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html) instead.\n",
        "\n",
        "Finally, set the hidden state size back to 40 (or another integer of your choice), and we can now proceed with the training."
      ],
      "metadata": {
        "id": "DFdbi4Nk9gNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# re-instantiate the network here (after fixing seed) so we always get same\n",
        "# initial (pseudo-random) values of its internal parameters\n",
        "my_first_lstm = MyFirstLSTM(hidden_size=40)\n",
        "\n",
        "# as usual, we prepare the optimization loop\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(my_first_lstm.parameters(), lr=1e-5)\n",
        "\n",
        "max_epochs = 20\n",
        "future = 40 # number of future points we are going to obtain before evaluation\n",
        "\n",
        "for epoch in range(0, max_epochs):\n",
        "\n",
        "    for seq, y_train in train_data:\n",
        "        optimizer.zero_grad()\n",
        "        my_first_lstm.hidden = (torch.zeros(1, 1, my_first_lstm.hidden_size),\n",
        "                       torch.zeros(1, 1, my_first_lstm.hidden_size))\n",
        "\n",
        "        y_pred = my_first_lstm(seq)\n",
        "        train_loss = loss(y_pred, y_train)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch %d, loss: %.4e\" % (epoch, train_loss.item()))\n",
        "\n",
        "    # this part here is just to offer a visualization of the training process,\n",
        "    # normally we should NEVER use the test set during training\n",
        "    if epoch % 5 == 0 or epoch == max_epochs-1 :\n",
        "      preds = train_set[-window_size:].tolist()\n",
        "      for f in range(0, future) :\n",
        "          seq = torch.FloatTensor(preds[-window_size:])\n",
        "          with torch.no_grad() :\n",
        "              my_first_lstm.hidden = (torch.zeros(1, 1, my_first_lstm.hidden_size),\n",
        "                            torch.zeros(1, 1, my_first_lstm.hidden_size))\n",
        "              preds.append(my_first_lstm(seq).item())\n",
        "\n",
        "      loss_train = loss(torch.tensor(preds[-window_size:]), y[760:])\n",
        "      print(\"Performance on test set: %.4e\" % loss_train.item())\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(12,4))\n",
        "      ax.set_xlim(700, 801) # we visualize only the last part of the data set\n",
        "      ax.grid(True)\n",
        "      ax.plot(y.numpy()[:760], color='green', label=\"Training set\")\n",
        "      ax.scatter(range(760, 800), y.numpy()[760:], color='green', label=\"Test set, ground truth\")\n",
        "      ax.plot(range(760, 800), preds[window_size:], color='orange', label=\"Prediction\")\n",
        "      ax.legend(loc='best')\n",
        "      ax.set_title(\"Epoch %d\" % epoch)\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "6AnljsGX-IGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the network is trained, we can check what happens if we give it an input sequence of different size, at the same time asking it to predict more values than what it typically saw in the training set."
      ],
      "metadata": {
        "id": "cUjg0i08EgwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# since we used what was previously the test set during training,\n",
        "# let's generate completely new data, a proper test set\n",
        "t_new = torch.linspace(1000, 1200, 200)\n",
        "y_new = torch.sin(t_new*2*3.1416/40)\n",
        "\n",
        "input_sequence_length = 80\n",
        "input_sequence = y_new[0:input_sequence_length] # first samples\n",
        "local_input_sequence = input_sequence.clone() # create a separate tensor, to avoid messing up the original\n",
        "pred_sequence_length = 60\n",
        "pred_sequence = torch.zeros(60)\n",
        "\n",
        "# we do not need to perform other backward passes, so we can just perform a forward pass\n",
        "# with the torch.no_grad() context, to avoid recreating the computational graph\n",
        "with torch.no_grad() :\n",
        "\n",
        "  for i in range(0, pred_sequence_length) :\n",
        "    # prepare the LSTM network, setting its initial hidden state to zero\n",
        "    my_first_lstm.hidden = (torch.zeros(1, 1, my_first_lstm.hidden_size),\n",
        "                              torch.zeros(1, 1, my_first_lstm.hidden_size))\n",
        "\n",
        "    # obtain the prediction for the sequence so far\n",
        "    y_pred = my_first_lstm(local_input_sequence)\n",
        "\n",
        "    # store the predicted element\n",
        "    pred_sequence[i] = y_pred.item()\n",
        "\n",
        "    # add the *predicted element* to the next input sequence, using concatenation\n",
        "    local_input_sequence = torch.cat((local_input_sequence, y_pred), dim=0)\n",
        "\n",
        "# check the results\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(t_new[:input_sequence_length], input_sequence, color='green', label=\"Initial training sequence\")\n",
        "ax.scatter(t_new[input_sequence_length:input_sequence_length+pred_sequence_length], y_new[input_sequence_length:input_sequence_length+pred_sequence_length],\n",
        "           color='green', label=\"Ground truth\")\n",
        "ax.plot(t_new[input_sequence_length:input_sequence_length+pred_sequence_length], pred_sequence, color='orange', label=\"Predicted values\")\n",
        "ax.grid(True)\n",
        "ax.legend(loc='best')"
      ],
      "metadata": {
        "id": "nu1EQIWWErFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to use the `forward_with_printouts()` method that you previously designed, instead of just the forward pass. What are the shapes of the tensors and the hidden state? Is this what you were expecting?\n",
        "\n",
        "Just to have a term of comparison, we shall now try an Auto-Regressive Integrated Moving Average (ARIMA) model, from the library statmodels."
      ],
      "metadata": {
        "id": "C69OLi3j3gdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colaboratory has statsmodels natively accessible, if you are running this on\n",
        "# a local notebook, you might have to install it\n",
        "#!pip install statsmodels\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# create a 'history' variable as a list, because we will have to append elements to it\n",
        "history = [x for x in train_set]\n",
        "\n",
        "# the model is autoregressive, so for every sample we want to create, we need\n",
        "# to feed it the whole history up to that point (including its own previous predictions)\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for i in range(0, test_set.shape[0]) :\n",
        "\n",
        "  # create and fit the model to the current history\n",
        "  arima_order = (1,1,0) # this tuple contains hyperparameters for ARIMA, don't worry too much about it\n",
        "  model = ARIMA(history, order=(1,1,0))\n",
        "  model_fitted = model.fit()\n",
        "\n",
        "  # perform a prediction\n",
        "  y_hat = model_fitted.forecast()[0] # get the first element of the forecast\n",
        "\n",
        "  # store information\n",
        "  y_pred.append(y_hat)\n",
        "  y_true.append(test_set[i].item())\n",
        "\n",
        "  # update history\n",
        "  history.append(y_true[-1]) # we are updating the history with an *observed value*\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(range(len(train_set)-100, len(train_set)), train_set[-100:], color='green', label=\"Training data\")\n",
        "ax.scatter(range(len(train_set), len(train_set) + len(test_set)), test_set, color='green', label=\"Test data, ground truth\")\n",
        "ax.plot(range(len(train_set), len(train_set) + len(test_set)), y_pred, color='orange', label=\"ARIMA predictions\")\n",
        "ax.grid(True)\n",
        "ax.legend(loc='best')"
      ],
      "metadata": {
        "id": "gG9sLeBq3r0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite all the warning messages, this one was not that bad! However, it is also true that we always updated the history that ARIMA uses to predict the next value with an _observed_ value. Verify what happens if the history is updated with the value _predicted_ by ARIMA. Does it still work well? Why do you think is that?\n",
        "\n",
        "As a final check on our beloved LSTM network, we could try a forward pass with a sequence and try to visualize the computational graph. We will need to re-install the pytorchviz library, which might take some time, but it could be cool. Let's check!"
      ],
      "metadata": {
        "id": "VejdTb1a7aFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U git+https://github.com/szagoruyko/pytorchviz.git@master --quiet"
      ],
      "metadata": {
        "id": "i2fwqMkXpF7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "# define input sequence\n",
        "input_sequence = y_new[0:input_sequence_length]\n",
        "print(\"The input sequence has %d elements\" % input_sequence_length)\n",
        "# set up hidden state of the LSTM\n",
        "my_first_lstm.hidden = (torch.zeros(1, 1, my_first_lstm.hidden_size),\n",
        "                              torch.zeros(1, 1, my_first_lstm.hidden_size))\n",
        "# get prediction for the next element in the sequence\n",
        "y_hat = my_first_lstm(local_input_sequence)\n",
        "# get the names of the parameters in the network\n",
        "params = dict(my_first_lstm.named_parameters())\n",
        "# add names for the other tensors\n",
        "params[\"y_hat\"] = y_hat\n",
        "# plot computational graph!\n",
        "make_dot(y_hat, params=params)"
      ],
      "metadata": {
        "id": "I4Yxo_whrISP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot looks a bit mysterious at first, but it becomes clearer after reading the documentation: the 'ih' and 'hh' naming convention for weights and biases stands for 'input-to-hidden' and 'hidden-to-hidden' respectively, which means that the parameters are grouped in tensors, based on whether they are between the input at time $t$ and the hidden part $h_{t+1}$ or just between $h_t$ and $h_{t+1}$. They are (probably) stored in this way to improve computational efficiency."
      ],
      "metadata": {
        "id": "9VDY0J5sxl_D"
      }
    }
  ]
}