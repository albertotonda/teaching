{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f330fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408087e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fb9904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize; we split on '\\t' and take only\n",
    "    # the first two parts, because the third one is legal information\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae9a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "# this function returns a True/False\n",
    "def filterPair(p, index_en=0):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[index_en].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116ba830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 232736 sentence pairs\n",
      "['go', 'va !']\n",
      "Trimmed to 18199 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3748\n",
      "fra 5751\n",
      "['we re finally alone', 'nous sommes enfin seuls']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(pairs[0])\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', reverse=False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429d2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6b55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97ebe35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c3b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00382366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) Training loss: %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd89f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74632634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1724ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee45fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 232736 sentence pairs\n",
      "['go', 'va !']\n",
      "Trimmed to 18199 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3748\n",
      "fra 5751\n",
      "0m 15s (- 52m 0s) (5 0%) Training loss: 1.9368\n",
      "0m 30s (- 50m 26s) (10 1%) Training loss: 1.0823\n",
      "0m 45s (- 49m 15s) (15 1%) Training loss: 0.7718\n",
      "0m 59s (- 48m 32s) (20 2%) Training loss: 0.5867\n",
      "1m 13s (- 48m 5s) (25 2%) Training loss: 0.4653\n",
      "1m 28s (- 47m 42s) (30 3%) Training loss: 0.3827\n",
      "1m 43s (- 47m 21s) (35 3%) Training loss: 0.3252\n",
      "1m 57s (- 47m 5s) (40 4%) Training loss: 0.2822\n",
      "2m 12s (- 46m 50s) (45 4%) Training loss: 0.2507\n",
      "2m 27s (- 46m 42s) (50 5%) Training loss: 0.2277\n",
      "2m 42s (- 46m 35s) (55 5%) Training loss: 0.2095\n",
      "2m 57s (- 46m 21s) (60 6%) Training loss: 0.1948\n",
      "3m 12s (- 46m 5s) (65 6%) Training loss: 0.1832\n",
      "3m 27s (- 45m 50s) (70 7%) Training loss: 0.1741\n",
      "3m 41s (- 45m 36s) (75 7%) Training loss: 0.1667\n",
      "3m 56s (- 45m 21s) (80 8%) Training loss: 0.1600\n",
      "4m 11s (- 45m 6s) (85 8%) Training loss: 0.1552\n",
      "4m 26s (- 44m 51s) (90 9%) Training loss: 0.1500\n",
      "4m 40s (- 44m 35s) (95 9%) Training loss: 0.1463\n",
      "4m 55s (- 44m 20s) (100 10%) Training loss: 0.1430\n",
      "5m 10s (- 44m 5s) (105 10%) Training loss: 0.1405\n",
      "5m 24s (- 43m 49s) (110 11%) Training loss: 0.1370\n",
      "5m 39s (- 43m 33s) (115 11%) Training loss: 0.1355\n",
      "5m 54s (- 43m 19s) (120 12%) Training loss: 0.1327\n",
      "6m 9s (- 43m 3s) (125 12%) Training loss: 0.1318\n",
      "6m 23s (- 42m 47s) (130 13%) Training loss: 0.1299\n",
      "6m 38s (- 42m 32s) (135 13%) Training loss: 0.1285\n",
      "6m 53s (- 42m 17s) (140 14%) Training loss: 0.1268\n",
      "7m 7s (- 42m 2s) (145 14%) Training loss: 0.1263\n",
      "7m 22s (- 41m 47s) (150 15%) Training loss: 0.1249\n",
      "7m 37s (- 41m 32s) (155 15%) Training loss: 0.1238\n",
      "7m 51s (- 41m 17s) (160 16%) Training loss: 0.1239\n",
      "8m 6s (- 41m 2s) (165 16%) Training loss: 0.1222\n",
      "8m 21s (- 40m 47s) (170 17%) Training loss: 0.1202\n",
      "8m 35s (- 40m 32s) (175 17%) Training loss: 0.1204\n",
      "8m 50s (- 40m 16s) (180 18%) Training loss: 0.1200\n",
      "9m 5s (- 40m 1s) (185 18%) Training loss: 0.1194\n",
      "9m 19s (- 39m 46s) (190 19%) Training loss: 0.1185\n",
      "9m 34s (- 39m 31s) (195 19%) Training loss: 0.1186\n",
      "9m 49s (- 39m 16s) (200 20%) Training loss: 0.1167\n",
      "10m 3s (- 39m 1s) (205 20%) Training loss: 0.1170\n",
      "10m 18s (- 38m 46s) (210 21%) Training loss: 0.1160\n",
      "10m 33s (- 38m 31s) (215 21%) Training loss: 0.1160\n",
      "10m 47s (- 38m 17s) (220 22%) Training loss: 0.1149\n",
      "11m 2s (- 38m 2s) (225 22%) Training loss: 0.1155\n",
      "11m 17s (- 37m 49s) (230 23%) Training loss: 0.1144\n",
      "11m 33s (- 37m 36s) (235 23%) Training loss: 0.1137\n",
      "11m 48s (- 37m 23s) (240 24%) Training loss: 0.1143\n",
      "12m 3s (- 37m 9s) (245 24%) Training loss: 0.1132\n",
      "12m 18s (- 36m 55s) (250 25%) Training loss: 0.1132\n",
      "12m 33s (- 36m 42s) (255 25%) Training loss: 0.1123\n",
      "12m 49s (- 36m 29s) (260 26%) Training loss: 0.1126\n",
      "13m 4s (- 36m 16s) (265 26%) Training loss: 0.1121\n",
      "13m 19s (- 36m 1s) (270 27%) Training loss: 0.1118\n",
      "13m 34s (- 35m 47s) (275 27%) Training loss: 0.1119\n",
      "13m 49s (- 35m 33s) (280 28%) Training loss: 0.1116\n",
      "14m 4s (- 35m 18s) (285 28%) Training loss: 0.1107\n",
      "14m 19s (- 35m 3s) (290 28%) Training loss: 0.1110\n",
      "14m 34s (- 34m 48s) (295 29%) Training loss: 0.1101\n",
      "14m 48s (- 34m 34s) (300 30%) Training loss: 0.1103\n",
      "15m 3s (- 34m 19s) (305 30%) Training loss: 0.1103\n",
      "15m 18s (- 34m 4s) (310 31%) Training loss: 0.1100\n",
      "15m 33s (- 33m 49s) (315 31%) Training loss: 0.1100\n",
      "15m 48s (- 33m 34s) (320 32%) Training loss: 0.1093\n",
      "16m 2s (- 33m 19s) (325 32%) Training loss: 0.1089\n",
      "16m 17s (- 33m 5s) (330 33%) Training loss: 0.1094\n",
      "16m 32s (- 32m 50s) (335 33%) Training loss: 0.1095\n",
      "16m 47s (- 32m 35s) (340 34%) Training loss: 0.1088\n",
      "17m 2s (- 32m 21s) (345 34%) Training loss: 0.1087\n",
      "17m 17s (- 32m 7s) (350 35%) Training loss: 0.1084\n",
      "17m 32s (- 31m 52s) (355 35%) Training loss: 0.1086\n",
      "17m 48s (- 31m 38s) (360 36%) Training loss: 0.1078\n",
      "18m 3s (- 31m 24s) (365 36%) Training loss: 0.1074\n",
      "18m 18s (- 31m 9s) (370 37%) Training loss: 0.1084\n",
      "18m 33s (- 30m 55s) (375 37%) Training loss: 0.1072\n",
      "18m 48s (- 30m 41s) (380 38%) Training loss: 0.1080\n",
      "19m 3s (- 30m 26s) (385 38%) Training loss: 0.1074\n",
      "19m 18s (- 30m 12s) (390 39%) Training loss: 0.1072\n",
      "19m 33s (- 29m 58s) (395 39%) Training loss: 0.1073\n",
      "19m 49s (- 29m 43s) (400 40%) Training loss: 0.1068\n",
      "20m 4s (- 29m 29s) (405 40%) Training loss: 0.1073\n",
      "20m 19s (- 29m 14s) (410 41%) Training loss: 0.1062\n",
      "20m 34s (- 29m 0s) (415 41%) Training loss: 0.1066\n",
      "20m 49s (- 28m 45s) (420 42%) Training loss: 0.1071\n",
      "21m 4s (- 28m 30s) (425 42%) Training loss: 0.1067\n",
      "21m 19s (- 28m 16s) (430 43%) Training loss: 0.1067\n",
      "21m 34s (- 28m 1s) (435 43%) Training loss: 0.1063\n",
      "21m 49s (- 27m 47s) (440 44%) Training loss: 0.1061\n",
      "22m 5s (- 27m 32s) (445 44%) Training loss: 0.1063\n",
      "22m 20s (- 27m 18s) (450 45%) Training loss: 0.1062\n",
      "22m 35s (- 27m 3s) (455 45%) Training loss: 0.1057\n",
      "22m 50s (- 26m 48s) (460 46%) Training loss: 0.1065\n",
      "23m 5s (- 26m 34s) (465 46%) Training loss: 0.1057\n",
      "23m 20s (- 26m 19s) (470 47%) Training loss: 0.1054\n",
      "23m 36s (- 26m 5s) (475 47%) Training loss: 0.1058\n",
      "23m 51s (- 25m 50s) (480 48%) Training loss: 0.1055\n",
      "24m 6s (- 25m 36s) (485 48%) Training loss: 0.1054\n",
      "24m 21s (- 25m 21s) (490 49%) Training loss: 0.1059\n",
      "24m 36s (- 25m 6s) (495 49%) Training loss: 0.1050\n",
      "24m 51s (- 24m 51s) (500 50%) Training loss: 0.1053\n",
      "25m 7s (- 24m 37s) (505 50%) Training loss: 0.1049\n",
      "25m 22s (- 24m 22s) (510 51%) Training loss: 0.1051\n",
      "25m 37s (- 24m 7s) (515 51%) Training loss: 0.1054\n",
      "25m 53s (- 23m 53s) (520 52%) Training loss: 0.1047\n",
      "26m 8s (- 23m 38s) (525 52%) Training loss: 0.1045\n",
      "26m 23s (- 23m 24s) (530 53%) Training loss: 0.1049\n",
      "26m 38s (- 23m 9s) (535 53%) Training loss: 0.1050\n",
      "26m 53s (- 22m 54s) (540 54%) Training loss: 0.1046\n",
      "27m 8s (- 22m 39s) (545 54%) Training loss: 0.1044\n",
      "27m 24s (- 22m 25s) (550 55%) Training loss: 0.1042\n",
      "27m 39s (- 22m 10s) (555 55%) Training loss: 0.1047\n",
      "27m 54s (- 21m 55s) (560 56%) Training loss: 0.1045\n",
      "28m 8s (- 21m 40s) (565 56%) Training loss: 0.1043\n",
      "28m 23s (- 21m 25s) (570 56%) Training loss: 0.1044\n",
      "28m 38s (- 21m 10s) (575 57%) Training loss: 0.1041\n",
      "28m 53s (- 20m 55s) (580 57%) Training loss: 0.1037\n",
      "29m 8s (- 20m 40s) (585 58%) Training loss: 0.1038\n",
      "29m 22s (- 20m 24s) (590 59%) Training loss: 0.1045\n",
      "29m 37s (- 20m 9s) (595 59%) Training loss: 0.1035\n",
      "29m 52s (- 19m 54s) (600 60%) Training loss: 0.1033\n",
      "30m 6s (- 19m 39s) (605 60%) Training loss: 0.1045\n",
      "30m 21s (- 19m 24s) (610 61%) Training loss: 0.1036\n",
      "30m 36s (- 19m 9s) (615 61%) Training loss: 0.1045\n",
      "30m 50s (- 18m 54s) (620 62%) Training loss: 0.1035\n",
      "31m 5s (- 18m 39s) (625 62%) Training loss: 0.1035\n",
      "31m 20s (- 18m 24s) (630 63%) Training loss: 0.1037\n",
      "31m 34s (- 18m 9s) (635 63%) Training loss: 0.1038\n",
      "31m 49s (- 17m 54s) (640 64%) Training loss: 0.1042\n",
      "32m 4s (- 17m 39s) (645 64%) Training loss: 0.1035\n",
      "32m 19s (- 17m 24s) (650 65%) Training loss: 0.1035\n",
      "32m 34s (- 17m 9s) (655 65%) Training loss: 0.1032\n",
      "32m 48s (- 16m 54s) (660 66%) Training loss: 0.1039\n",
      "33m 3s (- 16m 39s) (665 66%) Training loss: 0.1034\n",
      "33m 17s (- 16m 24s) (670 67%) Training loss: 0.1037\n",
      "33m 32s (- 16m 9s) (675 67%) Training loss: 0.1036\n",
      "33m 47s (- 15m 53s) (680 68%) Training loss: 0.1029\n",
      "34m 1s (- 15m 38s) (685 68%) Training loss: 0.1036\n",
      "34m 16s (- 15m 23s) (690 69%) Training loss: 0.1030\n",
      "34m 31s (- 15m 8s) (695 69%) Training loss: 0.1030\n",
      "34m 45s (- 14m 53s) (700 70%) Training loss: 0.1034\n",
      "35m 0s (- 14m 38s) (705 70%) Training loss: 0.1035\n",
      "35m 14s (- 14m 23s) (710 71%) Training loss: 0.1030\n",
      "35m 29s (- 14m 8s) (715 71%) Training loss: 0.1030\n",
      "35m 44s (- 13m 53s) (720 72%) Training loss: 0.1034\n",
      "35m 58s (- 13m 38s) (725 72%) Training loss: 0.1027\n",
      "36m 13s (- 13m 23s) (730 73%) Training loss: 0.1030\n",
      "36m 28s (- 13m 8s) (735 73%) Training loss: 0.1027\n",
      "36m 42s (- 12m 53s) (740 74%) Training loss: 0.1026\n",
      "36m 58s (- 12m 39s) (745 74%) Training loss: 0.1030\n",
      "37m 15s (- 12m 25s) (750 75%) Training loss: 0.1028\n",
      "37m 31s (- 12m 10s) (755 75%) Training loss: 0.1032\n",
      "37m 47s (- 11m 56s) (760 76%) Training loss: 0.1028\n",
      "38m 3s (- 11m 41s) (765 76%) Training loss: 0.1029\n",
      "38m 18s (- 11m 26s) (770 77%) Training loss: 0.1033\n",
      "38m 32s (- 11m 11s) (775 77%) Training loss: 0.1024\n",
      "38m 47s (- 10m 56s) (780 78%) Training loss: 0.1029\n",
      "39m 4s (- 10m 42s) (785 78%) Training loss: 0.1024\n",
      "39m 21s (- 10m 27s) (790 79%) Training loss: 0.1027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39m 36s (- 10m 12s) (795 79%) Training loss: 0.1023\n",
      "39m 51s (- 9m 57s) (800 80%) Training loss: 0.1024\n",
      "40m 7s (- 9m 43s) (805 80%) Training loss: 0.1024\n",
      "40m 21s (- 9m 28s) (810 81%) Training loss: 0.1022\n",
      "40m 38s (- 9m 13s) (815 81%) Training loss: 0.1030\n",
      "40m 55s (- 8m 59s) (820 82%) Training loss: 0.1023\n",
      "41m 12s (- 8m 44s) (825 82%) Training loss: 0.1027\n",
      "41m 27s (- 8m 29s) (830 83%) Training loss: 0.1027\n",
      "41m 42s (- 8m 14s) (835 83%) Training loss: 0.1024\n",
      "41m 57s (- 7m 59s) (840 84%) Training loss: 0.1025\n",
      "42m 12s (- 7m 44s) (845 84%) Training loss: 0.1018\n",
      "42m 28s (- 7m 29s) (850 85%) Training loss: 0.1029\n",
      "42m 45s (- 7m 15s) (855 85%) Training loss: 0.1023\n",
      "43m 2s (- 7m 0s) (860 86%) Training loss: 0.1023\n",
      "43m 17s (- 6m 45s) (865 86%) Training loss: 0.1027\n",
      "43m 33s (- 6m 30s) (870 87%) Training loss: 0.1024\n",
      "43m 48s (- 6m 15s) (875 87%) Training loss: 0.1023\n",
      "44m 3s (- 6m 0s) (880 88%) Training loss: 0.1019\n",
      "44m 18s (- 5m 45s) (885 88%) Training loss: 0.1025\n",
      "44m 34s (- 5m 30s) (890 89%) Training loss: 0.1021\n",
      "44m 49s (- 5m 15s) (895 89%) Training loss: 0.1021\n",
      "45m 4s (- 5m 0s) (900 90%) Training loss: 0.1016\n",
      "45m 19s (- 4m 45s) (905 90%) Training loss: 0.1024\n",
      "45m 34s (- 4m 30s) (910 91%) Training loss: 0.1018\n",
      "45m 50s (- 4m 15s) (915 91%) Training loss: 0.1023\n",
      "46m 5s (- 4m 0s) (920 92%) Training loss: 0.1019\n",
      "46m 20s (- 3m 45s) (925 92%) Training loss: 0.1023\n",
      "46m 35s (- 3m 30s) (930 93%) Training loss: 0.1019\n",
      "46m 50s (- 3m 15s) (935 93%) Training loss: 0.1019\n",
      "47m 5s (- 3m 0s) (940 94%) Training loss: 0.1023\n",
      "47m 20s (- 2m 45s) (945 94%) Training loss: 0.1020\n",
      "47m 35s (- 2m 30s) (950 95%) Training loss: 0.1020\n",
      "47m 50s (- 2m 15s) (955 95%) Training loss: 0.1024\n",
      "48m 5s (- 2m 0s) (960 96%) Training loss: 0.1018\n",
      "48m 20s (- 1m 45s) (965 96%) Training loss: 0.1015\n",
      "48m 34s (- 1m 30s) (970 97%) Training loss: 0.1020\n",
      "48m 49s (- 1m 15s) (975 97%) Training loss: 0.1016\n",
      "49m 4s (- 1m 0s) (980 98%) Training loss: 0.1012\n",
      "49m 19s (- 0m 45s) (985 98%) Training loss: 0.1015\n",
      "49m 33s (- 0m 30s) (990 99%) Training loss: 0.1022\n",
      "49m 48s (- 0m 15s) (995 99%) Training loss: 0.1020\n",
      "50m 3s (- 0m 0s) (1000 100%) Training loss: 0.1017\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 1000, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3bd86ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> you re stupid to trust him\n",
      "= tu es stupide de lui faire confiance\n",
      "< tu veux sur vous qui avez raison et aide <EOS>\n",
      "\n",
      "> i m in charge here\n",
      "= c est moi le patron ici\n",
      "< c est moi le patron par ici <EOS>\n",
      "\n",
      "> i m impressed you ve done so well\n",
      "= je suis impressionnee que vous ayez si bien reussi\n",
      "< je suis impressionnee que vous ayez si bien reussi <EOS>\n",
      "\n",
      "> you aren t in a hurry are you ?\n",
      "= vous n etes pas pressees si ?\n",
      "< vous n etes pas pressee si ? <EOS>\n",
      "\n",
      "> i m usually at home on mondays\n",
      "= je suis generalement chez moi le lundi\n",
      "< je suis generalement chez moi le lundi <EOS>\n",
      "\n",
      "> i m useless\n",
      "= je ne sers a rien\n",
      "< je n ai pas du tout pour ca <EOS>\n",
      "\n",
      "> he s been waiting here for a long time\n",
      "= ca fait longtemps qu il attend ici\n",
      "< ca fait longtemps qu il attend ici <EOS>\n",
      "\n",
      "> i m going to need more money\n",
      "= je vais avoir besoin de plus d argent\n",
      "< je vais devoir acheter un peu sommeil <EOS>\n",
      "\n",
      "> i m tougher than you think\n",
      "= je suis plus solide que tu ne le penses\n",
      "< je suis plus age que tu ne le penses <EOS>\n",
      "\n",
      "> i m going to a conference next week\n",
      "= je vais a une conference la semaine prochaine\n",
      "< je vais a une conference la semaine prochaine <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbba66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
