# Next iteration
Notes for the next class. Maybe try shorter sets of exercises, with shorter frontal classes?

## Virtual machines
Add packages: torchtext, tqdm, tsfresh

## Deep learning 1
1. Put network instantation and training in the same cell
2. The mushroom dataset seems to be completely separable, so accuracy 100%; maybe find a slightly harder classification problem.
3. Meaning of "fully connected layer".

## CNNs
1. Explain the CNN jargon a bit more: what is a "channel", what is a "filter".

## GPU speed improvement
1. We can use GPUs on Colaboratory
2. Maybe use a docker image?
3. Check also Azure stuff, running on servers

## RNNs
- Apparently, recurrent modules are hard to understand. Go slower, make more examples. Animation on how it works with an example sequence.
- Also, the concept that you can feed back the outputs as inputs for the next iteration
- xLSTMs?

## Embeddings
- T5 could be used to reverse an embedding

## AlphaFold
- Class on AlphaFold: https://www.ebi.ac.uk/training/online/courses/alphafold/
- AlphaFold lecture slides (downloaded somewhere): https://amyxlu.github.io/data/alphafold.pdf
- More AlphaFold 2.0 slides (also should be downloaded somewhere): https://figshare.com/articles/presentation/AlphaFold_2_slides/20822263

## Transformers from scratch in pytorch
https://benjaminwarner.dev/2023/07/01/attention-mechanism
https://benjaminwarner.dev/2023/07/28/rest-of-the-transformer

## Tutorial on diffusion models
https://arxiv.org/abs/2403.18103