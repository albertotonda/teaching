# Next iteration
Notes for the next class. Maybe try shorter sets of exercises, with shorter frontal classes?

## Virtual machines
Add packages: torchtext, tqdm, tsfresh

## Part 1
Refresher on ML
- Also show how you can de-normalize the data (e.g. returning it to its original unscaled values)

## RNNs
- Apparently, recurrent modules are hard to understand. Go slower, make more examples. Animation on how it works with an example sequence.
- Also, the concept that you can feed back the outputs as inputs for the next iteration